<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="rustdoc"><meta name="description" content="Source of the Rust file `juice/src/layer.rs`."><meta name="keywords" content="rust, rustlang, rust-lang"><title>layer.rs - source</title><link rel="preload" as="font" type="font/woff2" crossorigin href="../../SourceSerif4-Regular.ttf.woff2"><link rel="preload" as="font" type="font/woff2" crossorigin href="../../FiraSans-Regular.woff2"><link rel="preload" as="font" type="font/woff2" crossorigin href="../../FiraSans-Medium.woff2"><link rel="preload" as="font" type="font/woff2" crossorigin href="../../SourceCodePro-Regular.ttf.woff2"><link rel="preload" as="font" type="font/woff2" crossorigin href="../../SourceSerif4-Bold.ttf.woff2"><link rel="preload" as="font" type="font/woff2" crossorigin href="../../SourceCodePro-Semibold.ttf.woff2"><link rel="stylesheet" type="text/css" href="../../normalize.css"><link rel="stylesheet" type="text/css" href="../../rustdoc.css" id="mainThemeStyle"><link rel="stylesheet" type="text/css" href="../../ayu.css" disabled><link rel="stylesheet" type="text/css" href="../../dark.css" disabled><link rel="stylesheet" type="text/css" href="../../light.css" id="themeStyle"><script id="default-settings" ></script><script src="../../storage.js"></script><script defer src="../../source-script.js"></script><script defer src="../../source-files.js"></script><script defer src="../../main.js"></script><noscript><link rel="stylesheet" href="../../noscript.css"></noscript><link rel="alternate icon" type="image/png" href="../../favicon-16x16.png"><link rel="alternate icon" type="image/png" href="../../favicon-32x32.png"><link rel="icon" type="image/svg+xml" href="../../favicon.svg"></head><body class="rustdoc source"><!--[if lte IE 11]><div class="warning">This old browser is unsupported and will most likely display funky things.</div><![endif]--><nav class="mobile-topbar"><button class="sidebar-menu-toggle">&#9776;</button><a class="sidebar-logo" href="../../juice/index.html"><div class="logo-container"><img class="rust-logo" src="../../rust-logo.svg" alt="logo"></div>
        </a><h2 class="location"></h2>
    </nav>
    <nav class="sidebar"><a class="sidebar-logo" href="../../juice/index.html"><div class="logo-container"><img class="rust-logo" src="../../rust-logo.svg" alt="logo"></div>
        </a></nav><main><div class="width-limiter"><div class="sub-container"><a class="sub-logo-container" href="../../juice/index.html"><img class="rust-logo" src="../../rust-logo.svg" alt="logo"></a><nav class="sub"><form class="search-form"><div class="search-container"><span></span><input class="search-input" name="search" autocomplete="off" spellcheck="false" placeholder="Click or press ‘S’ to search, ‘?’ for more options…" type="search"><button type="button" id="help-button" title="help">?</button><div id="settings-menu" tabindex="-1">
                                <a href="../../settings.html" title="settings"><img width="22" height="22" alt="Change settings" src="../../wheel.svg"></a></div>
                        </div></form></nav></div><section id="main-content" class="content"><div class="example-wrap"><pre class="line-numbers"><span id="1">1</span>
<span id="2">2</span>
<span id="3">3</span>
<span id="4">4</span>
<span id="5">5</span>
<span id="6">6</span>
<span id="7">7</span>
<span id="8">8</span>
<span id="9">9</span>
<span id="10">10</span>
<span id="11">11</span>
<span id="12">12</span>
<span id="13">13</span>
<span id="14">14</span>
<span id="15">15</span>
<span id="16">16</span>
<span id="17">17</span>
<span id="18">18</span>
<span id="19">19</span>
<span id="20">20</span>
<span id="21">21</span>
<span id="22">22</span>
<span id="23">23</span>
<span id="24">24</span>
<span id="25">25</span>
<span id="26">26</span>
<span id="27">27</span>
<span id="28">28</span>
<span id="29">29</span>
<span id="30">30</span>
<span id="31">31</span>
<span id="32">32</span>
<span id="33">33</span>
<span id="34">34</span>
<span id="35">35</span>
<span id="36">36</span>
<span id="37">37</span>
<span id="38">38</span>
<span id="39">39</span>
<span id="40">40</span>
<span id="41">41</span>
<span id="42">42</span>
<span id="43">43</span>
<span id="44">44</span>
<span id="45">45</span>
<span id="46">46</span>
<span id="47">47</span>
<span id="48">48</span>
<span id="49">49</span>
<span id="50">50</span>
<span id="51">51</span>
<span id="52">52</span>
<span id="53">53</span>
<span id="54">54</span>
<span id="55">55</span>
<span id="56">56</span>
<span id="57">57</span>
<span id="58">58</span>
<span id="59">59</span>
<span id="60">60</span>
<span id="61">61</span>
<span id="62">62</span>
<span id="63">63</span>
<span id="64">64</span>
<span id="65">65</span>
<span id="66">66</span>
<span id="67">67</span>
<span id="68">68</span>
<span id="69">69</span>
<span id="70">70</span>
<span id="71">71</span>
<span id="72">72</span>
<span id="73">73</span>
<span id="74">74</span>
<span id="75">75</span>
<span id="76">76</span>
<span id="77">77</span>
<span id="78">78</span>
<span id="79">79</span>
<span id="80">80</span>
<span id="81">81</span>
<span id="82">82</span>
<span id="83">83</span>
<span id="84">84</span>
<span id="85">85</span>
<span id="86">86</span>
<span id="87">87</span>
<span id="88">88</span>
<span id="89">89</span>
<span id="90">90</span>
<span id="91">91</span>
<span id="92">92</span>
<span id="93">93</span>
<span id="94">94</span>
<span id="95">95</span>
<span id="96">96</span>
<span id="97">97</span>
<span id="98">98</span>
<span id="99">99</span>
<span id="100">100</span>
<span id="101">101</span>
<span id="102">102</span>
<span id="103">103</span>
<span id="104">104</span>
<span id="105">105</span>
<span id="106">106</span>
<span id="107">107</span>
<span id="108">108</span>
<span id="109">109</span>
<span id="110">110</span>
<span id="111">111</span>
<span id="112">112</span>
<span id="113">113</span>
<span id="114">114</span>
<span id="115">115</span>
<span id="116">116</span>
<span id="117">117</span>
<span id="118">118</span>
<span id="119">119</span>
<span id="120">120</span>
<span id="121">121</span>
<span id="122">122</span>
<span id="123">123</span>
<span id="124">124</span>
<span id="125">125</span>
<span id="126">126</span>
<span id="127">127</span>
<span id="128">128</span>
<span id="129">129</span>
<span id="130">130</span>
<span id="131">131</span>
<span id="132">132</span>
<span id="133">133</span>
<span id="134">134</span>
<span id="135">135</span>
<span id="136">136</span>
<span id="137">137</span>
<span id="138">138</span>
<span id="139">139</span>
<span id="140">140</span>
<span id="141">141</span>
<span id="142">142</span>
<span id="143">143</span>
<span id="144">144</span>
<span id="145">145</span>
<span id="146">146</span>
<span id="147">147</span>
<span id="148">148</span>
<span id="149">149</span>
<span id="150">150</span>
<span id="151">151</span>
<span id="152">152</span>
<span id="153">153</span>
<span id="154">154</span>
<span id="155">155</span>
<span id="156">156</span>
<span id="157">157</span>
<span id="158">158</span>
<span id="159">159</span>
<span id="160">160</span>
<span id="161">161</span>
<span id="162">162</span>
<span id="163">163</span>
<span id="164">164</span>
<span id="165">165</span>
<span id="166">166</span>
<span id="167">167</span>
<span id="168">168</span>
<span id="169">169</span>
<span id="170">170</span>
<span id="171">171</span>
<span id="172">172</span>
<span id="173">173</span>
<span id="174">174</span>
<span id="175">175</span>
<span id="176">176</span>
<span id="177">177</span>
<span id="178">178</span>
<span id="179">179</span>
<span id="180">180</span>
<span id="181">181</span>
<span id="182">182</span>
<span id="183">183</span>
<span id="184">184</span>
<span id="185">185</span>
<span id="186">186</span>
<span id="187">187</span>
<span id="188">188</span>
<span id="189">189</span>
<span id="190">190</span>
<span id="191">191</span>
<span id="192">192</span>
<span id="193">193</span>
<span id="194">194</span>
<span id="195">195</span>
<span id="196">196</span>
<span id="197">197</span>
<span id="198">198</span>
<span id="199">199</span>
<span id="200">200</span>
<span id="201">201</span>
<span id="202">202</span>
<span id="203">203</span>
<span id="204">204</span>
<span id="205">205</span>
<span id="206">206</span>
<span id="207">207</span>
<span id="208">208</span>
<span id="209">209</span>
<span id="210">210</span>
<span id="211">211</span>
<span id="212">212</span>
<span id="213">213</span>
<span id="214">214</span>
<span id="215">215</span>
<span id="216">216</span>
<span id="217">217</span>
<span id="218">218</span>
<span id="219">219</span>
<span id="220">220</span>
<span id="221">221</span>
<span id="222">222</span>
<span id="223">223</span>
<span id="224">224</span>
<span id="225">225</span>
<span id="226">226</span>
<span id="227">227</span>
<span id="228">228</span>
<span id="229">229</span>
<span id="230">230</span>
<span id="231">231</span>
<span id="232">232</span>
<span id="233">233</span>
<span id="234">234</span>
<span id="235">235</span>
<span id="236">236</span>
<span id="237">237</span>
<span id="238">238</span>
<span id="239">239</span>
<span id="240">240</span>
<span id="241">241</span>
<span id="242">242</span>
<span id="243">243</span>
<span id="244">244</span>
<span id="245">245</span>
<span id="246">246</span>
<span id="247">247</span>
<span id="248">248</span>
<span id="249">249</span>
<span id="250">250</span>
<span id="251">251</span>
<span id="252">252</span>
<span id="253">253</span>
<span id="254">254</span>
<span id="255">255</span>
<span id="256">256</span>
<span id="257">257</span>
<span id="258">258</span>
<span id="259">259</span>
<span id="260">260</span>
<span id="261">261</span>
<span id="262">262</span>
<span id="263">263</span>
<span id="264">264</span>
<span id="265">265</span>
<span id="266">266</span>
<span id="267">267</span>
<span id="268">268</span>
<span id="269">269</span>
<span id="270">270</span>
<span id="271">271</span>
<span id="272">272</span>
<span id="273">273</span>
<span id="274">274</span>
<span id="275">275</span>
<span id="276">276</span>
<span id="277">277</span>
<span id="278">278</span>
<span id="279">279</span>
<span id="280">280</span>
<span id="281">281</span>
<span id="282">282</span>
<span id="283">283</span>
<span id="284">284</span>
<span id="285">285</span>
<span id="286">286</span>
<span id="287">287</span>
<span id="288">288</span>
<span id="289">289</span>
<span id="290">290</span>
<span id="291">291</span>
<span id="292">292</span>
<span id="293">293</span>
<span id="294">294</span>
<span id="295">295</span>
<span id="296">296</span>
<span id="297">297</span>
<span id="298">298</span>
<span id="299">299</span>
<span id="300">300</span>
<span id="301">301</span>
<span id="302">302</span>
<span id="303">303</span>
<span id="304">304</span>
<span id="305">305</span>
<span id="306">306</span>
<span id="307">307</span>
<span id="308">308</span>
<span id="309">309</span>
<span id="310">310</span>
<span id="311">311</span>
<span id="312">312</span>
<span id="313">313</span>
<span id="314">314</span>
<span id="315">315</span>
<span id="316">316</span>
<span id="317">317</span>
<span id="318">318</span>
<span id="319">319</span>
<span id="320">320</span>
<span id="321">321</span>
<span id="322">322</span>
<span id="323">323</span>
<span id="324">324</span>
<span id="325">325</span>
<span id="326">326</span>
<span id="327">327</span>
<span id="328">328</span>
<span id="329">329</span>
<span id="330">330</span>
<span id="331">331</span>
<span id="332">332</span>
<span id="333">333</span>
<span id="334">334</span>
<span id="335">335</span>
<span id="336">336</span>
<span id="337">337</span>
<span id="338">338</span>
<span id="339">339</span>
<span id="340">340</span>
<span id="341">341</span>
<span id="342">342</span>
<span id="343">343</span>
<span id="344">344</span>
<span id="345">345</span>
<span id="346">346</span>
<span id="347">347</span>
<span id="348">348</span>
<span id="349">349</span>
<span id="350">350</span>
<span id="351">351</span>
<span id="352">352</span>
<span id="353">353</span>
<span id="354">354</span>
<span id="355">355</span>
<span id="356">356</span>
<span id="357">357</span>
<span id="358">358</span>
<span id="359">359</span>
<span id="360">360</span>
<span id="361">361</span>
<span id="362">362</span>
<span id="363">363</span>
<span id="364">364</span>
<span id="365">365</span>
<span id="366">366</span>
<span id="367">367</span>
<span id="368">368</span>
<span id="369">369</span>
<span id="370">370</span>
<span id="371">371</span>
<span id="372">372</span>
<span id="373">373</span>
<span id="374">374</span>
<span id="375">375</span>
<span id="376">376</span>
<span id="377">377</span>
<span id="378">378</span>
<span id="379">379</span>
<span id="380">380</span>
<span id="381">381</span>
<span id="382">382</span>
<span id="383">383</span>
<span id="384">384</span>
<span id="385">385</span>
<span id="386">386</span>
<span id="387">387</span>
<span id="388">388</span>
<span id="389">389</span>
<span id="390">390</span>
<span id="391">391</span>
<span id="392">392</span>
<span id="393">393</span>
<span id="394">394</span>
<span id="395">395</span>
<span id="396">396</span>
<span id="397">397</span>
<span id="398">398</span>
<span id="399">399</span>
<span id="400">400</span>
<span id="401">401</span>
<span id="402">402</span>
<span id="403">403</span>
<span id="404">404</span>
<span id="405">405</span>
<span id="406">406</span>
<span id="407">407</span>
<span id="408">408</span>
<span id="409">409</span>
<span id="410">410</span>
<span id="411">411</span>
<span id="412">412</span>
<span id="413">413</span>
<span id="414">414</span>
<span id="415">415</span>
<span id="416">416</span>
<span id="417">417</span>
<span id="418">418</span>
<span id="419">419</span>
<span id="420">420</span>
<span id="421">421</span>
<span id="422">422</span>
<span id="423">423</span>
<span id="424">424</span>
<span id="425">425</span>
<span id="426">426</span>
<span id="427">427</span>
<span id="428">428</span>
<span id="429">429</span>
<span id="430">430</span>
<span id="431">431</span>
<span id="432">432</span>
<span id="433">433</span>
<span id="434">434</span>
<span id="435">435</span>
<span id="436">436</span>
<span id="437">437</span>
<span id="438">438</span>
<span id="439">439</span>
<span id="440">440</span>
<span id="441">441</span>
<span id="442">442</span>
<span id="443">443</span>
<span id="444">444</span>
<span id="445">445</span>
<span id="446">446</span>
<span id="447">447</span>
<span id="448">448</span>
<span id="449">449</span>
<span id="450">450</span>
<span id="451">451</span>
<span id="452">452</span>
<span id="453">453</span>
<span id="454">454</span>
<span id="455">455</span>
<span id="456">456</span>
<span id="457">457</span>
<span id="458">458</span>
<span id="459">459</span>
<span id="460">460</span>
<span id="461">461</span>
<span id="462">462</span>
<span id="463">463</span>
<span id="464">464</span>
<span id="465">465</span>
<span id="466">466</span>
<span id="467">467</span>
<span id="468">468</span>
<span id="469">469</span>
<span id="470">470</span>
<span id="471">471</span>
<span id="472">472</span>
<span id="473">473</span>
<span id="474">474</span>
<span id="475">475</span>
<span id="476">476</span>
<span id="477">477</span>
<span id="478">478</span>
<span id="479">479</span>
<span id="480">480</span>
<span id="481">481</span>
<span id="482">482</span>
<span id="483">483</span>
<span id="484">484</span>
<span id="485">485</span>
<span id="486">486</span>
<span id="487">487</span>
<span id="488">488</span>
<span id="489">489</span>
<span id="490">490</span>
<span id="491">491</span>
<span id="492">492</span>
<span id="493">493</span>
<span id="494">494</span>
<span id="495">495</span>
<span id="496">496</span>
<span id="497">497</span>
<span id="498">498</span>
<span id="499">499</span>
<span id="500">500</span>
<span id="501">501</span>
<span id="502">502</span>
<span id="503">503</span>
<span id="504">504</span>
<span id="505">505</span>
<span id="506">506</span>
<span id="507">507</span>
<span id="508">508</span>
<span id="509">509</span>
<span id="510">510</span>
<span id="511">511</span>
<span id="512">512</span>
<span id="513">513</span>
<span id="514">514</span>
<span id="515">515</span>
<span id="516">516</span>
<span id="517">517</span>
<span id="518">518</span>
<span id="519">519</span>
<span id="520">520</span>
<span id="521">521</span>
<span id="522">522</span>
<span id="523">523</span>
<span id="524">524</span>
<span id="525">525</span>
<span id="526">526</span>
<span id="527">527</span>
<span id="528">528</span>
<span id="529">529</span>
<span id="530">530</span>
<span id="531">531</span>
<span id="532">532</span>
<span id="533">533</span>
<span id="534">534</span>
<span id="535">535</span>
<span id="536">536</span>
<span id="537">537</span>
<span id="538">538</span>
<span id="539">539</span>
<span id="540">540</span>
<span id="541">541</span>
<span id="542">542</span>
<span id="543">543</span>
<span id="544">544</span>
<span id="545">545</span>
<span id="546">546</span>
<span id="547">547</span>
<span id="548">548</span>
<span id="549">549</span>
<span id="550">550</span>
<span id="551">551</span>
<span id="552">552</span>
<span id="553">553</span>
<span id="554">554</span>
<span id="555">555</span>
<span id="556">556</span>
<span id="557">557</span>
<span id="558">558</span>
<span id="559">559</span>
<span id="560">560</span>
<span id="561">561</span>
<span id="562">562</span>
<span id="563">563</span>
<span id="564">564</span>
<span id="565">565</span>
<span id="566">566</span>
<span id="567">567</span>
<span id="568">568</span>
<span id="569">569</span>
<span id="570">570</span>
<span id="571">571</span>
<span id="572">572</span>
<span id="573">573</span>
<span id="574">574</span>
<span id="575">575</span>
<span id="576">576</span>
<span id="577">577</span>
<span id="578">578</span>
<span id="579">579</span>
<span id="580">580</span>
<span id="581">581</span>
<span id="582">582</span>
<span id="583">583</span>
<span id="584">584</span>
<span id="585">585</span>
<span id="586">586</span>
<span id="587">587</span>
<span id="588">588</span>
<span id="589">589</span>
<span id="590">590</span>
<span id="591">591</span>
<span id="592">592</span>
<span id="593">593</span>
<span id="594">594</span>
<span id="595">595</span>
<span id="596">596</span>
<span id="597">597</span>
<span id="598">598</span>
<span id="599">599</span>
<span id="600">600</span>
<span id="601">601</span>
<span id="602">602</span>
<span id="603">603</span>
<span id="604">604</span>
<span id="605">605</span>
<span id="606">606</span>
<span id="607">607</span>
<span id="608">608</span>
<span id="609">609</span>
<span id="610">610</span>
<span id="611">611</span>
<span id="612">612</span>
<span id="613">613</span>
<span id="614">614</span>
<span id="615">615</span>
<span id="616">616</span>
<span id="617">617</span>
<span id="618">618</span>
<span id="619">619</span>
<span id="620">620</span>
<span id="621">621</span>
<span id="622">622</span>
<span id="623">623</span>
<span id="624">624</span>
<span id="625">625</span>
<span id="626">626</span>
<span id="627">627</span>
<span id="628">628</span>
<span id="629">629</span>
<span id="630">630</span>
<span id="631">631</span>
<span id="632">632</span>
<span id="633">633</span>
<span id="634">634</span>
<span id="635">635</span>
<span id="636">636</span>
<span id="637">637</span>
<span id="638">638</span>
<span id="639">639</span>
<span id="640">640</span>
<span id="641">641</span>
<span id="642">642</span>
<span id="643">643</span>
<span id="644">644</span>
<span id="645">645</span>
<span id="646">646</span>
<span id="647">647</span>
<span id="648">648</span>
<span id="649">649</span>
<span id="650">650</span>
<span id="651">651</span>
<span id="652">652</span>
<span id="653">653</span>
<span id="654">654</span>
<span id="655">655</span>
<span id="656">656</span>
<span id="657">657</span>
<span id="658">658</span>
<span id="659">659</span>
<span id="660">660</span>
<span id="661">661</span>
<span id="662">662</span>
<span id="663">663</span>
<span id="664">664</span>
<span id="665">665</span>
<span id="666">666</span>
<span id="667">667</span>
<span id="668">668</span>
<span id="669">669</span>
<span id="670">670</span>
<span id="671">671</span>
<span id="672">672</span>
<span id="673">673</span>
<span id="674">674</span>
<span id="675">675</span>
<span id="676">676</span>
<span id="677">677</span>
<span id="678">678</span>
<span id="679">679</span>
<span id="680">680</span>
<span id="681">681</span>
<span id="682">682</span>
<span id="683">683</span>
<span id="684">684</span>
<span id="685">685</span>
<span id="686">686</span>
<span id="687">687</span>
<span id="688">688</span>
<span id="689">689</span>
<span id="690">690</span>
<span id="691">691</span>
<span id="692">692</span>
<span id="693">693</span>
<span id="694">694</span>
<span id="695">695</span>
<span id="696">696</span>
<span id="697">697</span>
<span id="698">698</span>
<span id="699">699</span>
<span id="700">700</span>
<span id="701">701</span>
<span id="702">702</span>
<span id="703">703</span>
<span id="704">704</span>
<span id="705">705</span>
<span id="706">706</span>
<span id="707">707</span>
<span id="708">708</span>
<span id="709">709</span>
<span id="710">710</span>
<span id="711">711</span>
<span id="712">712</span>
<span id="713">713</span>
<span id="714">714</span>
<span id="715">715</span>
<span id="716">716</span>
<span id="717">717</span>
<span id="718">718</span>
<span id="719">719</span>
<span id="720">720</span>
<span id="721">721</span>
<span id="722">722</span>
<span id="723">723</span>
<span id="724">724</span>
<span id="725">725</span>
<span id="726">726</span>
<span id="727">727</span>
<span id="728">728</span>
<span id="729">729</span>
<span id="730">730</span>
<span id="731">731</span>
<span id="732">732</span>
<span id="733">733</span>
<span id="734">734</span>
<span id="735">735</span>
<span id="736">736</span>
<span id="737">737</span>
<span id="738">738</span>
<span id="739">739</span>
<span id="740">740</span>
<span id="741">741</span>
<span id="742">742</span>
<span id="743">743</span>
<span id="744">744</span>
<span id="745">745</span>
<span id="746">746</span>
<span id="747">747</span>
<span id="748">748</span>
<span id="749">749</span>
<span id="750">750</span>
<span id="751">751</span>
<span id="752">752</span>
<span id="753">753</span>
<span id="754">754</span>
<span id="755">755</span>
<span id="756">756</span>
<span id="757">757</span>
<span id="758">758</span>
<span id="759">759</span>
<span id="760">760</span>
<span id="761">761</span>
<span id="762">762</span>
<span id="763">763</span>
<span id="764">764</span>
<span id="765">765</span>
<span id="766">766</span>
<span id="767">767</span>
<span id="768">768</span>
<span id="769">769</span>
<span id="770">770</span>
<span id="771">771</span>
<span id="772">772</span>
<span id="773">773</span>
<span id="774">774</span>
<span id="775">775</span>
<span id="776">776</span>
<span id="777">777</span>
<span id="778">778</span>
<span id="779">779</span>
<span id="780">780</span>
<span id="781">781</span>
<span id="782">782</span>
<span id="783">783</span>
<span id="784">784</span>
<span id="785">785</span>
<span id="786">786</span>
<span id="787">787</span>
<span id="788">788</span>
<span id="789">789</span>
<span id="790">790</span>
<span id="791">791</span>
<span id="792">792</span>
<span id="793">793</span>
<span id="794">794</span>
<span id="795">795</span>
<span id="796">796</span>
<span id="797">797</span>
<span id="798">798</span>
<span id="799">799</span>
<span id="800">800</span>
<span id="801">801</span>
<span id="802">802</span>
<span id="803">803</span>
<span id="804">804</span>
<span id="805">805</span>
<span id="806">806</span>
<span id="807">807</span>
<span id="808">808</span>
<span id="809">809</span>
<span id="810">810</span>
<span id="811">811</span>
<span id="812">812</span>
<span id="813">813</span>
<span id="814">814</span>
<span id="815">815</span>
<span id="816">816</span>
<span id="817">817</span>
<span id="818">818</span>
<span id="819">819</span>
<span id="820">820</span>
<span id="821">821</span>
<span id="822">822</span>
<span id="823">823</span>
<span id="824">824</span>
<span id="825">825</span>
<span id="826">826</span>
<span id="827">827</span>
<span id="828">828</span>
<span id="829">829</span>
<span id="830">830</span>
<span id="831">831</span>
<span id="832">832</span>
<span id="833">833</span>
<span id="834">834</span>
<span id="835">835</span>
<span id="836">836</span>
<span id="837">837</span>
<span id="838">838</span>
<span id="839">839</span>
<span id="840">840</span>
<span id="841">841</span>
<span id="842">842</span>
<span id="843">843</span>
<span id="844">844</span>
<span id="845">845</span>
<span id="846">846</span>
<span id="847">847</span>
<span id="848">848</span>
<span id="849">849</span>
<span id="850">850</span>
<span id="851">851</span>
<span id="852">852</span>
<span id="853">853</span>
<span id="854">854</span>
<span id="855">855</span>
<span id="856">856</span>
<span id="857">857</span>
<span id="858">858</span>
<span id="859">859</span>
<span id="860">860</span>
<span id="861">861</span>
<span id="862">862</span>
<span id="863">863</span>
<span id="864">864</span>
<span id="865">865</span>
<span id="866">866</span>
<span id="867">867</span>
<span id="868">868</span>
<span id="869">869</span>
<span id="870">870</span>
<span id="871">871</span>
<span id="872">872</span>
<span id="873">873</span>
<span id="874">874</span>
<span id="875">875</span>
<span id="876">876</span>
<span id="877">877</span>
<span id="878">878</span>
<span id="879">879</span>
<span id="880">880</span>
<span id="881">881</span>
<span id="882">882</span>
<span id="883">883</span>
<span id="884">884</span>
<span id="885">885</span>
<span id="886">886</span>
<span id="887">887</span>
<span id="888">888</span>
<span id="889">889</span>
<span id="890">890</span>
<span id="891">891</span>
<span id="892">892</span>
<span id="893">893</span>
<span id="894">894</span>
<span id="895">895</span>
<span id="896">896</span>
<span id="897">897</span>
<span id="898">898</span>
<span id="899">899</span>
<span id="900">900</span>
<span id="901">901</span>
<span id="902">902</span>
<span id="903">903</span>
<span id="904">904</span>
<span id="905">905</span>
<span id="906">906</span>
<span id="907">907</span>
<span id="908">908</span>
<span id="909">909</span>
<span id="910">910</span>
<span id="911">911</span>
<span id="912">912</span>
<span id="913">913</span>
<span id="914">914</span>
<span id="915">915</span>
<span id="916">916</span>
<span id="917">917</span>
<span id="918">918</span>
<span id="919">919</span>
<span id="920">920</span>
<span id="921">921</span>
<span id="922">922</span>
<span id="923">923</span>
<span id="924">924</span>
<span id="925">925</span>
<span id="926">926</span>
<span id="927">927</span>
<span id="928">928</span>
<span id="929">929</span>
<span id="930">930</span>
<span id="931">931</span>
<span id="932">932</span>
<span id="933">933</span>
<span id="934">934</span>
<span id="935">935</span>
<span id="936">936</span>
<span id="937">937</span>
<span id="938">938</span>
<span id="939">939</span>
<span id="940">940</span>
<span id="941">941</span>
<span id="942">942</span>
<span id="943">943</span>
<span id="944">944</span>
<span id="945">945</span>
<span id="946">946</span>
<span id="947">947</span>
<span id="948">948</span>
<span id="949">949</span>
<span id="950">950</span>
<span id="951">951</span>
<span id="952">952</span>
<span id="953">953</span>
<span id="954">954</span>
<span id="955">955</span>
<span id="956">956</span>
<span id="957">957</span>
<span id="958">958</span>
<span id="959">959</span>
<span id="960">960</span>
<span id="961">961</span>
<span id="962">962</span>
<span id="963">963</span>
<span id="964">964</span>
<span id="965">965</span>
<span id="966">966</span>
<span id="967">967</span>
<span id="968">968</span>
<span id="969">969</span>
<span id="970">970</span>
<span id="971">971</span>
<span id="972">972</span>
<span id="973">973</span>
<span id="974">974</span>
<span id="975">975</span>
<span id="976">976</span>
<span id="977">977</span>
<span id="978">978</span>
<span id="979">979</span>
<span id="980">980</span>
<span id="981">981</span>
<span id="982">982</span>
<span id="983">983</span>
<span id="984">984</span>
<span id="985">985</span>
<span id="986">986</span>
<span id="987">987</span>
<span id="988">988</span>
<span id="989">989</span>
<span id="990">990</span>
<span id="991">991</span>
<span id="992">992</span>
<span id="993">993</span>
<span id="994">994</span>
<span id="995">995</span>
<span id="996">996</span>
<span id="997">997</span>
<span id="998">998</span>
<span id="999">999</span>
<span id="1000">1000</span>
<span id="1001">1001</span>
<span id="1002">1002</span>
<span id="1003">1003</span>
<span id="1004">1004</span>
<span id="1005">1005</span>
<span id="1006">1006</span>
<span id="1007">1007</span>
<span id="1008">1008</span>
<span id="1009">1009</span>
<span id="1010">1010</span>
<span id="1011">1011</span>
<span id="1012">1012</span>
<span id="1013">1013</span>
<span id="1014">1014</span>
<span id="1015">1015</span>
<span id="1016">1016</span>
<span id="1017">1017</span>
<span id="1018">1018</span>
<span id="1019">1019</span>
<span id="1020">1020</span>
<span id="1021">1021</span>
<span id="1022">1022</span>
<span id="1023">1023</span>
<span id="1024">1024</span>
<span id="1025">1025</span>
<span id="1026">1026</span>
<span id="1027">1027</span>
<span id="1028">1028</span>
<span id="1029">1029</span>
<span id="1030">1030</span>
<span id="1031">1031</span>
<span id="1032">1032</span>
<span id="1033">1033</span>
<span id="1034">1034</span>
<span id="1035">1035</span>
<span id="1036">1036</span>
<span id="1037">1037</span>
<span id="1038">1038</span>
<span id="1039">1039</span>
<span id="1040">1040</span>
<span id="1041">1041</span>
<span id="1042">1042</span>
<span id="1043">1043</span>
<span id="1044">1044</span>
<span id="1045">1045</span>
<span id="1046">1046</span>
<span id="1047">1047</span>
<span id="1048">1048</span>
<span id="1049">1049</span>
<span id="1050">1050</span>
<span id="1051">1051</span>
<span id="1052">1052</span>
<span id="1053">1053</span>
<span id="1054">1054</span>
<span id="1055">1055</span>
<span id="1056">1056</span>
<span id="1057">1057</span>
<span id="1058">1058</span>
<span id="1059">1059</span>
<span id="1060">1060</span>
<span id="1061">1061</span>
<span id="1062">1062</span>
<span id="1063">1063</span>
<span id="1064">1064</span>
<span id="1065">1065</span>
<span id="1066">1066</span>
<span id="1067">1067</span>
<span id="1068">1068</span>
<span id="1069">1069</span>
<span id="1070">1070</span>
<span id="1071">1071</span>
<span id="1072">1072</span>
<span id="1073">1073</span>
<span id="1074">1074</span>
<span id="1075">1075</span>
<span id="1076">1076</span>
<span id="1077">1077</span>
<span id="1078">1078</span>
<span id="1079">1079</span>
<span id="1080">1080</span>
<span id="1081">1081</span>
<span id="1082">1082</span>
<span id="1083">1083</span>
<span id="1084">1084</span>
<span id="1085">1085</span>
<span id="1086">1086</span>
<span id="1087">1087</span>
<span id="1088">1088</span>
<span id="1089">1089</span>
<span id="1090">1090</span>
<span id="1091">1091</span>
<span id="1092">1092</span>
<span id="1093">1093</span>
<span id="1094">1094</span>
<span id="1095">1095</span>
<span id="1096">1096</span>
<span id="1097">1097</span>
<span id="1098">1098</span>
<span id="1099">1099</span>
<span id="1100">1100</span>
<span id="1101">1101</span>
<span id="1102">1102</span>
<span id="1103">1103</span>
<span id="1104">1104</span>
<span id="1105">1105</span>
<span id="1106">1106</span>
<span id="1107">1107</span>
<span id="1108">1108</span>
<span id="1109">1109</span>
<span id="1110">1110</span>
<span id="1111">1111</span>
<span id="1112">1112</span>
<span id="1113">1113</span>
<span id="1114">1114</span>
<span id="1115">1115</span>
<span id="1116">1116</span>
<span id="1117">1117</span>
<span id="1118">1118</span>
<span id="1119">1119</span>
<span id="1120">1120</span>
<span id="1121">1121</span>
<span id="1122">1122</span>
<span id="1123">1123</span>
<span id="1124">1124</span>
<span id="1125">1125</span>
<span id="1126">1126</span>
<span id="1127">1127</span>
<span id="1128">1128</span>
<span id="1129">1129</span>
<span id="1130">1130</span>
<span id="1131">1131</span>
<span id="1132">1132</span>
<span id="1133">1133</span>
<span id="1134">1134</span>
<span id="1135">1135</span>
<span id="1136">1136</span>
<span id="1137">1137</span>
<span id="1138">1138</span>
<span id="1139">1139</span>
<span id="1140">1140</span>
<span id="1141">1141</span>
<span id="1142">1142</span>
<span id="1143">1143</span>
<span id="1144">1144</span>
<span id="1145">1145</span>
<span id="1146">1146</span>
<span id="1147">1147</span>
<span id="1148">1148</span>
<span id="1149">1149</span>
<span id="1150">1150</span>
<span id="1151">1151</span>
<span id="1152">1152</span>
<span id="1153">1153</span>
<span id="1154">1154</span>
<span id="1155">1155</span>
<span id="1156">1156</span>
<span id="1157">1157</span>
<span id="1158">1158</span>
<span id="1159">1159</span>
<span id="1160">1160</span>
<span id="1161">1161</span>
<span id="1162">1162</span>
<span id="1163">1163</span>
<span id="1164">1164</span>
<span id="1165">1165</span>
<span id="1166">1166</span>
<span id="1167">1167</span>
<span id="1168">1168</span>
<span id="1169">1169</span>
<span id="1170">1170</span>
<span id="1171">1171</span>
<span id="1172">1172</span>
<span id="1173">1173</span>
<span id="1174">1174</span>
<span id="1175">1175</span>
<span id="1176">1176</span>
<span id="1177">1177</span>
<span id="1178">1178</span>
<span id="1179">1179</span>
<span id="1180">1180</span>
<span id="1181">1181</span>
<span id="1182">1182</span>
<span id="1183">1183</span>
<span id="1184">1184</span>
<span id="1185">1185</span>
<span id="1186">1186</span>
<span id="1187">1187</span>
<span id="1188">1188</span>
<span id="1189">1189</span>
<span id="1190">1190</span>
<span id="1191">1191</span>
<span id="1192">1192</span>
<span id="1193">1193</span>
<span id="1194">1194</span>
<span id="1195">1195</span>
<span id="1196">1196</span>
<span id="1197">1197</span>
<span id="1198">1198</span>
<span id="1199">1199</span>
<span id="1200">1200</span>
<span id="1201">1201</span>
<span id="1202">1202</span>
<span id="1203">1203</span>
<span id="1204">1204</span>
<span id="1205">1205</span>
<span id="1206">1206</span>
<span id="1207">1207</span>
<span id="1208">1208</span>
<span id="1209">1209</span>
<span id="1210">1210</span>
<span id="1211">1211</span>
<span id="1212">1212</span>
<span id="1213">1213</span>
<span id="1214">1214</span>
<span id="1215">1215</span>
<span id="1216">1216</span>
<span id="1217">1217</span>
<span id="1218">1218</span>
<span id="1219">1219</span>
<span id="1220">1220</span>
<span id="1221">1221</span>
<span id="1222">1222</span>
<span id="1223">1223</span>
<span id="1224">1224</span>
<span id="1225">1225</span>
<span id="1226">1226</span>
<span id="1227">1227</span>
<span id="1228">1228</span>
<span id="1229">1229</span>
<span id="1230">1230</span>
<span id="1231">1231</span>
<span id="1232">1232</span>
<span id="1233">1233</span>
<span id="1234">1234</span>
<span id="1235">1235</span>
<span id="1236">1236</span>
<span id="1237">1237</span>
<span id="1238">1238</span>
<span id="1239">1239</span>
<span id="1240">1240</span>
<span id="1241">1241</span>
<span id="1242">1242</span>
<span id="1243">1243</span>
<span id="1244">1244</span>
<span id="1245">1245</span>
<span id="1246">1246</span>
<span id="1247">1247</span>
<span id="1248">1248</span>
<span id="1249">1249</span>
<span id="1250">1250</span>
<span id="1251">1251</span>
<span id="1252">1252</span>
<span id="1253">1253</span>
<span id="1254">1254</span>
<span id="1255">1255</span>
<span id="1256">1256</span>
<span id="1257">1257</span>
<span id="1258">1258</span>
<span id="1259">1259</span>
<span id="1260">1260</span>
<span id="1261">1261</span>
<span id="1262">1262</span>
<span id="1263">1263</span>
<span id="1264">1264</span>
<span id="1265">1265</span>
<span id="1266">1266</span>
<span id="1267">1267</span>
<span id="1268">1268</span>
<span id="1269">1269</span>
<span id="1270">1270</span>
<span id="1271">1271</span>
<span id="1272">1272</span>
<span id="1273">1273</span>
<span id="1274">1274</span>
<span id="1275">1275</span>
<span id="1276">1276</span>
<span id="1277">1277</span>
<span id="1278">1278</span>
<span id="1279">1279</span>
<span id="1280">1280</span>
<span id="1281">1281</span>
<span id="1282">1282</span>
<span id="1283">1283</span>
<span id="1284">1284</span>
<span id="1285">1285</span>
<span id="1286">1286</span>
<span id="1287">1287</span>
<span id="1288">1288</span>
<span id="1289">1289</span>
<span id="1290">1290</span>
<span id="1291">1291</span>
<span id="1292">1292</span>
<span id="1293">1293</span>
<span id="1294">1294</span>
<span id="1295">1295</span>
<span id="1296">1296</span>
<span id="1297">1297</span>
<span id="1298">1298</span>
<span id="1299">1299</span>
<span id="1300">1300</span>
<span id="1301">1301</span>
<span id="1302">1302</span>
<span id="1303">1303</span>
<span id="1304">1304</span>
<span id="1305">1305</span>
<span id="1306">1306</span>
<span id="1307">1307</span>
<span id="1308">1308</span>
<span id="1309">1309</span>
<span id="1310">1310</span>
<span id="1311">1311</span>
<span id="1312">1312</span>
<span id="1313">1313</span>
<span id="1314">1314</span>
<span id="1315">1315</span>
<span id="1316">1316</span>
<span id="1317">1317</span>
<span id="1318">1318</span>
<span id="1319">1319</span>
<span id="1320">1320</span>
<span id="1321">1321</span>
<span id="1322">1322</span>
<span id="1323">1323</span>
<span id="1324">1324</span>
<span id="1325">1325</span>
<span id="1326">1326</span>
<span id="1327">1327</span>
<span id="1328">1328</span>
<span id="1329">1329</span>
<span id="1330">1330</span>
<span id="1331">1331</span>
<span id="1332">1332</span>
<span id="1333">1333</span>
<span id="1334">1334</span>
<span id="1335">1335</span>
<span id="1336">1336</span>
<span id="1337">1337</span>
<span id="1338">1338</span>
<span id="1339">1339</span>
<span id="1340">1340</span>
<span id="1341">1341</span>
<span id="1342">1342</span>
<span id="1343">1343</span>
<span id="1344">1344</span>
<span id="1345">1345</span>
<span id="1346">1346</span>
<span id="1347">1347</span>
<span id="1348">1348</span>
<span id="1349">1349</span>
<span id="1350">1350</span>
<span id="1351">1351</span>
<span id="1352">1352</span>
<span id="1353">1353</span>
<span id="1354">1354</span>
<span id="1355">1355</span>
<span id="1356">1356</span>
<span id="1357">1357</span>
<span id="1358">1358</span>
<span id="1359">1359</span>
<span id="1360">1360</span>
<span id="1361">1361</span>
<span id="1362">1362</span>
<span id="1363">1363</span>
<span id="1364">1364</span>
<span id="1365">1365</span>
<span id="1366">1366</span>
<span id="1367">1367</span>
<span id="1368">1368</span>
<span id="1369">1369</span>
<span id="1370">1370</span>
<span id="1371">1371</span>
<span id="1372">1372</span>
<span id="1373">1373</span>
<span id="1374">1374</span>
<span id="1375">1375</span>
<span id="1376">1376</span>
<span id="1377">1377</span>
<span id="1378">1378</span>
<span id="1379">1379</span>
<span id="1380">1380</span>
<span id="1381">1381</span>
<span id="1382">1382</span>
<span id="1383">1383</span>
<span id="1384">1384</span>
<span id="1385">1385</span>
<span id="1386">1386</span>
<span id="1387">1387</span>
<span id="1388">1388</span>
<span id="1389">1389</span>
<span id="1390">1390</span>
<span id="1391">1391</span>
<span id="1392">1392</span>
<span id="1393">1393</span>
<span id="1394">1394</span>
<span id="1395">1395</span>
<span id="1396">1396</span>
<span id="1397">1397</span>
<span id="1398">1398</span>
<span id="1399">1399</span>
<span id="1400">1400</span>
<span id="1401">1401</span>
<span id="1402">1402</span>
<span id="1403">1403</span>
<span id="1404">1404</span>
<span id="1405">1405</span>
<span id="1406">1406</span>
<span id="1407">1407</span>
<span id="1408">1408</span>
<span id="1409">1409</span>
<span id="1410">1410</span>
<span id="1411">1411</span>
<span id="1412">1412</span>
<span id="1413">1413</span>
<span id="1414">1414</span>
<span id="1415">1415</span>
<span id="1416">1416</span>
<span id="1417">1417</span>
<span id="1418">1418</span>
<span id="1419">1419</span>
<span id="1420">1420</span>
<span id="1421">1421</span>
<span id="1422">1422</span>
<span id="1423">1423</span>
<span id="1424">1424</span>
<span id="1425">1425</span>
<span id="1426">1426</span>
<span id="1427">1427</span>
<span id="1428">1428</span>
<span id="1429">1429</span>
<span id="1430">1430</span>
<span id="1431">1431</span>
<span id="1432">1432</span>
<span id="1433">1433</span>
<span id="1434">1434</span>
<span id="1435">1435</span>
<span id="1436">1436</span>
<span id="1437">1437</span>
<span id="1438">1438</span>
<span id="1439">1439</span>
<span id="1440">1440</span>
<span id="1441">1441</span>
<span id="1442">1442</span>
<span id="1443">1443</span>
<span id="1444">1444</span>
<span id="1445">1445</span>
<span id="1446">1446</span>
<span id="1447">1447</span>
<span id="1448">1448</span>
<span id="1449">1449</span>
<span id="1450">1450</span>
<span id="1451">1451</span>
<span id="1452">1452</span>
<span id="1453">1453</span>
<span id="1454">1454</span>
<span id="1455">1455</span>
<span id="1456">1456</span>
<span id="1457">1457</span>
<span id="1458">1458</span>
<span id="1459">1459</span>
<span id="1460">1460</span>
<span id="1461">1461</span>
<span id="1462">1462</span>
<span id="1463">1463</span>
<span id="1464">1464</span>
<span id="1465">1465</span>
<span id="1466">1466</span>
<span id="1467">1467</span>
<span id="1468">1468</span>
<span id="1469">1469</span>
<span id="1470">1470</span>
<span id="1471">1471</span>
<span id="1472">1472</span>
<span id="1473">1473</span>
<span id="1474">1474</span>
<span id="1475">1475</span>
<span id="1476">1476</span>
<span id="1477">1477</span>
<span id="1478">1478</span>
<span id="1479">1479</span>
<span id="1480">1480</span>
<span id="1481">1481</span>
<span id="1482">1482</span>
<span id="1483">1483</span>
<span id="1484">1484</span>
<span id="1485">1485</span>
<span id="1486">1486</span>
<span id="1487">1487</span>
<span id="1488">1488</span>
<span id="1489">1489</span>
<span id="1490">1490</span>
<span id="1491">1491</span>
<span id="1492">1492</span>
<span id="1493">1493</span>
<span id="1494">1494</span>
<span id="1495">1495</span>
<span id="1496">1496</span>
<span id="1497">1497</span>
<span id="1498">1498</span>
<span id="1499">1499</span>
<span id="1500">1500</span>
<span id="1501">1501</span>
<span id="1502">1502</span>
<span id="1503">1503</span>
<span id="1504">1504</span>
<span id="1505">1505</span>
<span id="1506">1506</span>
<span id="1507">1507</span>
<span id="1508">1508</span>
<span id="1509">1509</span>
<span id="1510">1510</span>
<span id="1511">1511</span>
<span id="1512">1512</span>
<span id="1513">1513</span>
<span id="1514">1514</span>
<span id="1515">1515</span>
<span id="1516">1516</span>
<span id="1517">1517</span>
<span id="1518">1518</span>
<span id="1519">1519</span>
<span id="1520">1520</span>
<span id="1521">1521</span>
<span id="1522">1522</span>
<span id="1523">1523</span>
<span id="1524">1524</span>
<span id="1525">1525</span>
<span id="1526">1526</span>
<span id="1527">1527</span>
<span id="1528">1528</span>
<span id="1529">1529</span>
<span id="1530">1530</span>
<span id="1531">1531</span>
<span id="1532">1532</span>
<span id="1533">1533</span>
<span id="1534">1534</span>
<span id="1535">1535</span>
<span id="1536">1536</span>
<span id="1537">1537</span>
<span id="1538">1538</span>
<span id="1539">1539</span>
<span id="1540">1540</span>
<span id="1541">1541</span>
<span id="1542">1542</span>
<span id="1543">1543</span>
<span id="1544">1544</span>
<span id="1545">1545</span>
<span id="1546">1546</span>
<span id="1547">1547</span>
<span id="1548">1548</span>
<span id="1549">1549</span>
<span id="1550">1550</span>
<span id="1551">1551</span>
<span id="1552">1552</span>
<span id="1553">1553</span>
<span id="1554">1554</span>
<span id="1555">1555</span>
<span id="1556">1556</span>
<span id="1557">1557</span>
<span id="1558">1558</span>
<span id="1559">1559</span>
<span id="1560">1560</span>
<span id="1561">1561</span>
<span id="1562">1562</span>
<span id="1563">1563</span>
<span id="1564">1564</span>
<span id="1565">1565</span>
<span id="1566">1566</span>
<span id="1567">1567</span>
<span id="1568">1568</span>
<span id="1569">1569</span>
<span id="1570">1570</span>
<span id="1571">1571</span>
<span id="1572">1572</span>
<span id="1573">1573</span>
<span id="1574">1574</span>
<span id="1575">1575</span>
<span id="1576">1576</span>
<span id="1577">1577</span>
<span id="1578">1578</span>
<span id="1579">1579</span>
<span id="1580">1580</span>
<span id="1581">1581</span>
<span id="1582">1582</span>
<span id="1583">1583</span>
<span id="1584">1584</span>
<span id="1585">1585</span>
<span id="1586">1586</span>
<span id="1587">1587</span>
<span id="1588">1588</span>
<span id="1589">1589</span>
<span id="1590">1590</span>
<span id="1591">1591</span>
<span id="1592">1592</span>
<span id="1593">1593</span>
<span id="1594">1594</span>
<span id="1595">1595</span>
<span id="1596">1596</span>
<span id="1597">1597</span>
<span id="1598">1598</span>
<span id="1599">1599</span>
<span id="1600">1600</span>
<span id="1601">1601</span>
<span id="1602">1602</span>
<span id="1603">1603</span>
<span id="1604">1604</span>
<span id="1605">1605</span>
<span id="1606">1606</span>
<span id="1607">1607</span>
<span id="1608">1608</span>
<span id="1609">1609</span>
<span id="1610">1610</span>
<span id="1611">1611</span>
<span id="1612">1612</span>
<span id="1613">1613</span>
<span id="1614">1614</span>
<span id="1615">1615</span>
<span id="1616">1616</span>
<span id="1617">1617</span>
<span id="1618">1618</span>
<span id="1619">1619</span>
<span id="1620">1620</span>
<span id="1621">1621</span>
<span id="1622">1622</span>
<span id="1623">1623</span>
<span id="1624">1624</span>
<span id="1625">1625</span>
<span id="1626">1626</span>
<span id="1627">1627</span>
<span id="1628">1628</span>
<span id="1629">1629</span>
<span id="1630">1630</span>
<span id="1631">1631</span>
<span id="1632">1632</span>
<span id="1633">1633</span>
<span id="1634">1634</span>
<span id="1635">1635</span>
<span id="1636">1636</span>
<span id="1637">1637</span>
<span id="1638">1638</span>
<span id="1639">1639</span>
<span id="1640">1640</span>
<span id="1641">1641</span>
<span id="1642">1642</span>
<span id="1643">1643</span>
<span id="1644">1644</span>
<span id="1645">1645</span>
<span id="1646">1646</span>
<span id="1647">1647</span>
<span id="1648">1648</span>
<span id="1649">1649</span>
<span id="1650">1650</span>
<span id="1651">1651</span>
<span id="1652">1652</span>
<span id="1653">1653</span>
<span id="1654">1654</span>
<span id="1655">1655</span>
<span id="1656">1656</span>
<span id="1657">1657</span>
<span id="1658">1658</span>
<span id="1659">1659</span>
<span id="1660">1660</span>
<span id="1661">1661</span>
<span id="1662">1662</span>
<span id="1663">1663</span>
<span id="1664">1664</span>
<span id="1665">1665</span>
<span id="1666">1666</span>
<span id="1667">1667</span>
<span id="1668">1668</span>
<span id="1669">1669</span>
<span id="1670">1670</span>
<span id="1671">1671</span>
<span id="1672">1672</span>
<span id="1673">1673</span>
<span id="1674">1674</span>
<span id="1675">1675</span>
<span id="1676">1676</span>
<span id="1677">1677</span>
<span id="1678">1678</span>
<span id="1679">1679</span>
<span id="1680">1680</span>
<span id="1681">1681</span>
<span id="1682">1682</span>
<span id="1683">1683</span>
<span id="1684">1684</span>
<span id="1685">1685</span>
<span id="1686">1686</span>
<span id="1687">1687</span>
<span id="1688">1688</span>
<span id="1689">1689</span>
<span id="1690">1690</span>
<span id="1691">1691</span>
<span id="1692">1692</span>
<span id="1693">1693</span>
<span id="1694">1694</span>
<span id="1695">1695</span>
<span id="1696">1696</span>
<span id="1697">1697</span>
<span id="1698">1698</span>
<span id="1699">1699</span>
</pre><pre class="rust"><code><span class="doccomment">//! Provides the generics and interfaces for the specific Layers.</span>
<span class="doccomment">//!</span>
<span class="doccomment">//! See [Layers][layers]</span>
<span class="doccomment">//! [layers]: ../layers/index.html</span>

<span class="kw">use</span> <span class="ident">std::cmp</span>;
<span class="kw">use</span> <span class="ident">std::collections</span>::{<span class="ident">HashMap</span>, <span class="ident">HashSet</span>};
<span class="kw">use</span> <span class="ident">std::fmt</span>;
<span class="kw">use</span> <span class="ident">std::fs::File</span>;
<span class="kw">use</span> <span class="ident">std::io</span>::{<span class="self">self</span>, <span class="ident">BufReader</span>};
<span class="kw">use</span> <span class="ident">std::path::Path</span>;
<span class="kw">use</span> <span class="ident">std::rc::Rc</span>;
<span class="kw">use</span> <span class="ident">std::sync</span>::{<span class="ident">Arc</span>, <span class="ident">RwLock</span>};

<span class="kw">use</span> <span class="ident"><span class="kw">crate</span>::capnp_util</span>::<span class="kw-2">*</span>;
<span class="kw">use</span> <span class="ident"><span class="kw">crate</span>::co::prelude</span>::<span class="kw-2">*</span>;
<span class="kw">use</span> <span class="ident"><span class="kw">crate</span>::juice_capnp::layer</span> <span class="kw">as</span> <span class="ident">capnp_layer</span>;
<span class="kw">use</span> <span class="ident"><span class="kw">crate</span>::juice_capnp::layer_config</span> <span class="kw">as</span> <span class="ident">capnp_layer_config</span>;
<span class="kw">use</span> <span class="ident"><span class="kw">crate</span>::juice_capnp::layer_config::layer_type</span> <span class="kw">as</span> <span class="ident">capnp_layer_type</span>;
<span class="kw">use</span> <span class="ident"><span class="kw">crate</span>::layers</span>::<span class="kw-2">*</span>;
<span class="kw">use</span> <span class="ident"><span class="kw">crate</span>::util</span>::{<span class="ident">ArcLock</span>, <span class="ident">LayerOps</span>};
<span class="kw">use</span> <span class="ident"><span class="kw">crate</span>::weight::WeightConfig</span>;

<span class="attribute">#[<span class="ident">derive</span>(<span class="ident">Debug</span>)]</span>
<span class="doccomment">/// The generic Layer</span>
<span class="kw">pub</span> <span class="kw">struct</span> <span class="ident">Layer</span><span class="op">&lt;</span><span class="ident">B</span>: <span class="ident">IBackend</span><span class="op">&gt;</span> {
    <span class="doccomment">/// Identifies the Network</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// The name is mainly used for logging purposes.</span>
    <span class="kw">pub</span> <span class="ident">name</span>: <span class="ident">String</span>,
    <span class="doccomment">/// The configuration of the Layer</span>
    <span class="kw">pub</span> <span class="ident">config</span>: <span class="ident">Box</span><span class="op">&lt;</span><span class="ident">LayerConfig</span><span class="op">&gt;</span>,
    <span class="doccomment">/// The [implementation][1] of the Layer.</span>
    <span class="doccomment">/// [1]: ../layers/index.html</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// This is the part that does most of the work ([forward][2]/[backward][3]).</span>
    <span class="doccomment">/// [2]: ./trait.ILayer.html#method.forward</span>
    <span class="doccomment">/// [3]: ./trait.ILayer.html#method.backward</span>
    <span class="kw">pub</span> <span class="ident">worker</span>: <span class="ident">Box</span><span class="op">&lt;</span><span class="kw">dyn</span> <span class="ident">ILayer</span><span class="op">&lt;</span><span class="ident">B</span><span class="op">&gt;</span><span class="op">&gt;</span>,

    <span class="ident">backend</span>: <span class="ident">Rc</span><span class="op">&lt;</span><span class="ident">B</span><span class="op">&gt;</span>,

    <span class="doccomment">/// Determines if layer will skip computations for [backward][1] step.</span>
    <span class="doccomment">/// [1]: ./trait.ILayer.html#method.backward</span>
    <span class="ident">needs_backward</span>: <span class="ident">bool</span>,

    <span class="doccomment">/// The vector that stores shared references to the weights in the form of blobs.</span>
    <span class="kw">pub</span> <span class="ident">weights_data</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span>,
    <span class="doccomment">/// The vector that stores shared references to the weights in the form of blobs.</span>
    <span class="kw">pub</span> <span class="ident">weights_gradient</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span>,
    <span class="comment">// contains all the learnable weights (does not include bias(?) and shared weights)</span>
    <span class="ident">learnable_weights</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span>,
    <span class="comment">// learning rate for each weight</span>
    <span class="ident">weights_lr</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="prelude-ty">Option</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span>,
    <span class="comment">// weight decay for each weight</span>
    <span class="ident">weights_weight_decay</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="prelude-ty">Option</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span>,
    <span class="comment">// display name for each weight</span>
    <span class="ident">weights_display_names</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">String</span><span class="op">&gt;</span>,

    <span class="doccomment">/// Vector indicating whether to compute the diff of each weight blob.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// You can safely ignore false values and always compute gradients</span>
    <span class="doccomment">/// for all weights, but possibly with wasteful computation.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// Can be used by some [Layer implementations][1] to optimize performance.</span>
    <span class="doccomment">/// [1]: ../layers/index.html</span>
    <span class="ident">weight_propagate_down</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">bool</span><span class="op">&gt;</span>,

    <span class="doccomment">/// References to all the input blobs of the layer.</span>
    <span class="kw">pub</span> <span class="ident">input_blobs_data</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span>,
    <span class="doccomment">/// References to all the input blobs of the layer.</span>
    <span class="kw">pub</span> <span class="ident">input_blobs_gradient</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span>,
    <span class="doccomment">/// Names for all the input blobs of the layer.</span>
    <span class="kw">pub</span> <span class="ident">input_blob_names</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">String</span><span class="op">&gt;</span>,
    <span class="ident">input_need_backwards</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">bool</span><span class="op">&gt;</span>,

    <span class="doccomment">/// References to all the output blobs of the layer.</span>
    <span class="kw">pub</span> <span class="ident">output_blobs_data</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span>,
    <span class="doccomment">/// References to all the output blobs of the layer.</span>
    <span class="kw">pub</span> <span class="ident">output_blobs_gradient</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span>,
    <span class="ident">output_blob_names</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">String</span><span class="op">&gt;</span>,
    <span class="doccomment">/// The vector that indicates whether each output blob contributes to</span>
    <span class="doccomment">/// the [loss][1] of the network and with which weight.</span>
    <span class="doccomment">/// [1]: http://caffe.berkeleyvision.org/tutorial/loss.html</span>
    <span class="ident">loss</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>,

    <span class="doccomment">/// All the blobs of the layer that can be addressed by name.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// Does not contain anonymous blobs.</span>
    <span class="kw">pub</span> <span class="ident">blob_names</span>: <span class="ident">HashMap</span><span class="op">&lt;</span><span class="ident">String</span>, (<span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span>, <span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span>)<span class="op">&gt;</span>,
}

<span class="kw">impl</span><span class="op">&lt;</span><span class="ident">B</span>: <span class="ident">IBackend</span><span class="op">&gt;</span> <span class="ident">Layer</span><span class="op">&lt;</span><span class="ident">B</span><span class="op">&gt;</span> {
    <span class="doccomment">/// Connect the layer to another layers and set up tensors for intermediate results and weights.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// Connects to the outputs provided by other layers via the `registry`.</span>
    <span class="doccomment">/// Adds output blobs to the layer and then adds them to the `registry`, so the next</span>
    <span class="doccomment">/// layers can connect them as their inputs.</span>
    <span class="doccomment">/// In the end it initializes the underlying [layer implementation][2].</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// [2]: ./trait.ILayer.html</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// Called during initialization of containter layers.</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">connect</span>(
        <span class="kw-2">&amp;mut</span> <span class="self">self</span>,
        <span class="ident">registry</span>: <span class="kw-2">&amp;mut</span> <span class="ident">HashMap</span><span class="op">&lt;</span><span class="ident">String</span>, (<span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span>, <span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span>)<span class="op">&gt;</span>,
        <span class="ident">weight_registry</span>: <span class="kw-2">&amp;mut</span> <span class="ident">HashMap</span><span class="op">&lt;</span>
            <span class="ident">String</span>,
            (
                <span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span>,
                <span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span>,
                <span class="prelude-ty">Option</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>,
                <span class="prelude-ty">Option</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>,
            ),
        <span class="op">&gt;</span>,
    ) {
        <span class="comment">// connect to all required inputs</span>
        <span class="kw">for</span> <span class="ident">input_name</span> <span class="kw">in</span> <span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">config</span>.<span class="ident">inputs</span>.<span class="ident">clone</span>() {
            <span class="self">self</span>.<span class="ident">connect_input</span>(<span class="ident">input_name</span>, <span class="ident">registry</span>)
        }
        <span class="comment">// setup outputs</span>
        <span class="kw">for</span> (<span class="ident">output_id</span>, <span class="kw">_</span>) <span class="kw">in</span> <span class="self">self</span>.<span class="ident">config</span>.<span class="ident">outputs</span>.<span class="ident">clone</span>().<span class="ident">iter</span>().<span class="ident">rev</span>().<span class="ident">enumerate</span>() {
            <span class="self">self</span>.<span class="ident">append_output</span>(<span class="ident">output_id</span>, <span class="ident">registry</span>);
        }
        <span class="kw">let</span> <span class="ident">config</span> <span class="op">=</span> <span class="self">self</span>.<span class="ident">config</span>.<span class="ident">clone</span>();
        <span class="kw">for</span> (<span class="ident">output_id</span>, <span class="kw">_</span>) <span class="kw">in</span> <span class="self">self</span>.<span class="ident">config</span>.<span class="ident">outputs</span>.<span class="ident">clone</span>().<span class="ident">iter</span>().<span class="ident">rev</span>().<span class="ident">enumerate</span>() {
            <span class="self">self</span>.<span class="ident">append_weight</span>(<span class="kw-2">&amp;</span><span class="ident">config</span>, <span class="ident">weight_registry</span>, <span class="number">0</span>, <span class="ident">output_id</span>);
        }

        <span class="comment">// If the layer specifies that AutoTopBlobs() -&gt; true and the LayerParameter</span>
        <span class="comment">// specified fewer than the required number (as specified by</span>
        <span class="comment">// exact_num_top_blobs() or min_output_blobs()), allocate them here.</span>
        <span class="kw">let</span> <span class="ident">auto_output_blobs</span> <span class="op">=</span> <span class="self">self</span>.<span class="ident">worker</span>.<span class="ident">auto_output_blobs</span>();
        <span class="macro">debug!</span>(<span class="string">&quot;Layer {} - auto_output_blobs: {}&quot;</span>, <span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">name</span>, <span class="kw-2">&amp;</span><span class="ident">auto_output_blobs</span>);
        <span class="kw">let</span> <span class="ident">min_output_blobs</span> <span class="op">=</span> <span class="self">self</span>.<span class="ident">worker</span>.<span class="ident">min_output_blobs</span>();
        <span class="kw">let</span> <span class="ident">exact_num_output_blobs</span> <span class="op">=</span> <span class="self">self</span>.<span class="ident">worker</span>.<span class="ident">exact_num_output_blobs</span>().<span class="ident">unwrap_or</span>(<span class="number">0</span>);
        <span class="kw">if</span> <span class="ident">auto_output_blobs</span> {
            <span class="kw">let</span> <span class="ident">needed_num_outputs</span> <span class="op">=</span> <span class="ident">cmp::max</span>(<span class="ident">min_output_blobs</span>, <span class="ident">exact_num_output_blobs</span>);
            <span class="kw">for</span> <span class="kw">_</span> <span class="kw">in</span> <span class="number">0</span>..(<span class="ident">needed_num_outputs</span> <span class="op">-</span> <span class="self">self</span>.<span class="ident">output_blobs_data</span>.<span class="ident">len</span>()) {
                <span class="comment">// Add &quot;anonymous&quot; output blobs -- do not add to registry</span>
                <span class="comment">// as we don&#39;t want these blobs to be usable as input</span>
                <span class="comment">// to other layers.</span>
                <span class="macro">info!</span>(<span class="string">&quot;Adding anonymous output blob for layer {}&quot;</span>, <span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">name</span>);
                <span class="self">self</span>.<span class="ident">create_anonymous_output</span>();
            }
        }

        <span class="self">self</span>.<span class="ident">worker</span>.<span class="ident">init</span>(<span class="self">self</span>.<span class="ident">backend</span>.<span class="ident">clone</span>());
        <span class="self">self</span>.<span class="ident">reshape</span>();
        <span class="self">self</span>.<span class="ident">worker</span>.<span class="ident">resize_shared_workspace</span>(<span class="self">self</span>.<span class="ident">backend</span>.<span class="ident">clone</span>(), <span class="prelude-val">None</span>);
        <span class="kw">for</span> <span class="ident">t</span> <span class="kw">in</span> <span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">output_blobs_data</span> {
            <span class="macro">debug!</span>(<span class="string">&quot;Layer {} - output shape: {:?}&quot;</span>, <span class="self">self</span>.<span class="ident">name</span>, <span class="ident">t</span>.<span class="ident">read</span>().<span class="ident">unwrap</span>().<span class="ident">desc</span>());
        }
    }

    <span class="doccomment">/// Append blob as [input blob][1] to the Layer.</span>
    <span class="doccomment">/// [1]: ../layer/index.html</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// During network initalization the blobs will be appended to the Layers as per their</span>
    <span class="doccomment">/// [LayerConfig][3]. It is also determined if a output blob skips backpropagation</span>
    <span class="doccomment">/// from [LayerConfig.propagate_down][3] (see also [init_backprop][5]).</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// [3]: ../layer/struct.LayerConfig.html</span>
    <span class="doccomment">/// [5]: #method.init_backprop</span>
    <span class="kw">fn</span> <span class="ident">connect_input</span>(
        <span class="kw-2">&amp;mut</span> <span class="self">self</span>,
        <span class="ident">blob_name</span>: <span class="kw-2">&amp;</span><span class="ident">str</span>,
        <span class="ident">available_blobs</span>: <span class="kw-2">&amp;mut</span> <span class="ident">HashMap</span><span class="op">&lt;</span><span class="ident">String</span>, (<span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span>, <span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span>)<span class="op">&gt;</span>,
    ) {
        <span class="kw">let</span> <span class="ident">input_id</span> <span class="op">=</span> <span class="self">self</span>
            .<span class="ident">config</span>
            .<span class="ident">inputs</span>
            .<span class="ident">iter</span>()
            .<span class="ident">position</span>(<span class="op">|</span><span class="ident">input_name</span><span class="op">|</span> <span class="ident">input_name</span> <span class="op">==</span> <span class="ident">blob_name</span>)
            .<span class="ident">unwrap</span>();

        <span class="kw">if</span> <span class="op">!</span><span class="ident">available_blobs</span>.<span class="ident">contains_key</span>(<span class="kw-2">&amp;</span><span class="kw-2">*</span><span class="ident">blob_name</span>) {
            <span class="macro">error!</span>(
                <span class="string">&quot;Unknown input blob {} (layer &#39;{}&#39;, input_id: {})&quot;</span>,
                <span class="ident">blob_name</span>, <span class="self">self</span>.<span class="ident">name</span>, <span class="ident">input_id</span>
            );
        }
        <span class="macro">info!</span>(<span class="string">&quot;Input {:&lt;15} -&gt; Layer {:&gt;15}&quot;</span>, <span class="ident">blob_name</span>, <span class="self">self</span>.<span class="ident">name</span>);

        <span class="self">self</span>.<span class="ident">input_blob_names</span>.<span class="ident">push</span>(<span class="ident">blob_name</span>.<span class="ident">to_owned</span>());
        <span class="self">self</span>.<span class="ident">input_blobs_data</span>.<span class="ident">push</span>(
            <span class="ident">available_blobs</span>
                .<span class="ident">get</span>(<span class="kw-2">&amp;</span><span class="kw-2">*</span><span class="ident">blob_name</span>)
                .<span class="ident">expect</span>(<span class="kw-2">&amp;</span><span class="macro">format!</span>(<span class="string">&quot;Unknown blob name {}&quot;</span>, <span class="ident">blob_name</span>))
                .<span class="number">0</span>
                .<span class="ident">clone</span>(),
        );
        <span class="self">self</span>.<span class="ident">input_blobs_gradient</span>.<span class="ident">push</span>(
            <span class="ident">available_blobs</span>
                .<span class="ident">get</span>(<span class="kw-2">&amp;</span><span class="kw-2">*</span><span class="ident">blob_name</span>)
                .<span class="ident">expect</span>(<span class="kw-2">&amp;</span><span class="macro">format!</span>(<span class="string">&quot;Unknown blob name {}&quot;</span>, <span class="ident">blob_name</span>))
                .<span class="number">1</span>
                .<span class="ident">clone</span>(),
        );
        <span class="comment">// available_blobs.remove(&amp;*blob_name);</span>

        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">propagate_down</span> <span class="op">=</span> <span class="bool-val">true</span>;
        <span class="comment">// Check if the backpropagation on input_id should be skipped</span>
        <span class="kw">if</span> <span class="op">!</span><span class="self">self</span>.<span class="ident">config</span>.<span class="ident">propagate_down</span>.<span class="ident">is_empty</span>() {
            <span class="ident">propagate_down</span> <span class="op">=</span> <span class="self">self</span>.<span class="ident">config</span>.<span class="ident">propagate_down</span>[<span class="ident">input_id</span>];
        }
        <span class="kw">let</span> <span class="ident">need_backward</span> <span class="op">=</span> <span class="ident">propagate_down</span>;
        <span class="self">self</span>.<span class="ident">input_need_backwards</span>.<span class="ident">push</span>(<span class="ident">need_backward</span>);
    }

    <span class="doccomment">/// Append blob as [output blob][1] to the Layer.</span>
    <span class="doccomment">/// [1]: ../layer/index.html</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// During network initalization the blobs will be appended to the Layers as per their</span>
    <span class="doccomment">/// [LayerConfig][2]. It is also determined if computations can be done in-place, in which</span>
    <span class="doccomment">/// no additional Blob will be allocated.&lt;/br&gt;</span>
    <span class="doccomment">/// Finally, the new blob will be added to the registry, so that the other layers can</span>
    <span class="doccomment">/// connect it as their input.</span>
    <span class="doccomment">/// [2]: ../layer/struct.LayerConfig.html</span>
    <span class="kw">fn</span> <span class="ident">append_output</span>(
        <span class="kw-2">&amp;mut</span> <span class="self">self</span>,
        <span class="ident">output_id</span>: <span class="ident">usize</span>,
        <span class="ident">registry</span>: <span class="kw-2">&amp;mut</span> <span class="ident">HashMap</span><span class="op">&lt;</span><span class="ident">String</span>, (<span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span>, <span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span>)<span class="op">&gt;</span>,
    ) {
        <span class="kw">let</span> <span class="ident">layer_config</span> <span class="op">=</span> <span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">config</span>;

        <span class="kw">let</span> <span class="ident">blob_name</span> <span class="op">=</span> <span class="ident">layer_config</span>.<span class="ident">output</span>(<span class="ident">output_id</span>).<span class="ident">unwrap</span>().<span class="ident">clone</span>();
        <span class="kw">let</span> <span class="ident">blob_data</span>: <span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span>;
        <span class="kw">let</span> <span class="ident">blob_gradient</span>: <span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span>;

        <span class="kw">if</span> <span class="ident">layer_config</span>.<span class="ident">input</span>(<span class="ident">output_id</span>).<span class="ident">is_some</span>() <span class="op">&amp;&amp;</span> <span class="kw-2">*</span><span class="ident">layer_config</span>.<span class="ident">input</span>(<span class="ident">output_id</span>).<span class="ident">unwrap</span>() <span class="op">==</span> <span class="ident">blob_name</span> {
            <span class="macro">info!</span>(<span class="string">&quot;Layer {:&lt;15} -&gt; Output {:&gt;15} (in-place)&quot;</span>, <span class="ident">layer_config</span>.<span class="ident">name</span>, <span class="ident">blob_name</span>);
            <span class="ident">blob_data</span> <span class="op">=</span> <span class="ident">registry</span>[<span class="kw-2">&amp;</span><span class="ident">blob_name</span>].<span class="number">0</span>.<span class="ident">clone</span>();
            <span class="ident">blob_gradient</span> <span class="op">=</span> <span class="ident">registry</span>[<span class="kw-2">&amp;</span><span class="ident">blob_name</span>].<span class="number">1</span>.<span class="ident">clone</span>();
        } <span class="kw">else</span> <span class="kw">if</span> <span class="ident">registry</span>.<span class="ident">contains_key</span>(<span class="kw-2">&amp;</span><span class="ident">blob_name</span>) {
            <span class="comment">// If we are not doing in-place computation but have duplicated blobs, raise an</span>
            <span class="comment">// error.</span>
            <span class="macro">error!</span>(<span class="string">&quot;Top blob {} produced by multiple sources.&quot;</span>, <span class="ident">blob_name</span>);
            <span class="kw">return</span>;
        } <span class="kw">else</span> {
            {
                <span class="macro">info!</span>(<span class="string">&quot;Layer {:&lt;15} -&gt; Output {:&gt;15}&quot;</span>, <span class="self">self</span>.<span class="ident">name</span>, <span class="ident">blob_name</span>);
                <span class="macro">info!</span>(<span class="string">&quot;Output {} = {}&quot;</span>, <span class="ident">output_id</span>, <span class="ident">blob_name</span>);
            }

            <span class="kw">let</span> <span class="ident">backend</span>: <span class="ident">Rc</span><span class="op">&lt;</span><span class="kw">dyn</span> <span class="ident">IBackend</span><span class="op">&lt;</span><span class="ident">F</span> <span class="op">=</span> <span class="ident">B::F</span><span class="op">&gt;</span><span class="op">&gt;</span> <span class="op">=</span> <span class="self">self</span>.<span class="ident">backend</span>.<span class="ident">clone</span>();
            <span class="ident">blob_data</span> <span class="op">=</span> <span class="ident">Arc::new</span>(<span class="ident">RwLock::new</span>(<span class="ident">SharedTensor::new</span>(<span class="kw-2">&amp;</span>[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))); <span class="comment">// [1,1,1] for CUDA</span>
            <span class="ident">blob_gradient</span> <span class="op">=</span> <span class="ident">Arc::new</span>(<span class="ident">RwLock::new</span>(<span class="ident">SharedTensor::new</span>(<span class="kw-2">&amp;</span>[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])));
            <span class="comment">// [1,1,1] for CUDA</span>
        }
        <span class="self">self</span>.<span class="ident">output_blob_names</span>.<span class="ident">push</span>(<span class="ident">blob_name</span>.<span class="ident">clone</span>());
        <span class="self">self</span>.<span class="ident">output_blobs_data</span>.<span class="ident">push</span>(<span class="ident">blob_data</span>.<span class="ident">clone</span>());
        <span class="self">self</span>.<span class="ident">output_blobs_gradient</span>.<span class="ident">push</span>(<span class="ident">blob_gradient</span>.<span class="ident">clone</span>());
        <span class="self">self</span>.<span class="ident">blob_names</span>
            .<span class="ident">insert</span>(<span class="ident">blob_name</span>.<span class="ident">clone</span>(), (<span class="ident">blob_data</span>.<span class="ident">clone</span>(), <span class="ident">blob_gradient</span>.<span class="ident">clone</span>()));
        <span class="ident">registry</span>.<span class="ident">insert</span>(<span class="ident">blob_name</span>.<span class="ident">clone</span>(), (<span class="ident">blob_data</span>.<span class="ident">clone</span>(), <span class="ident">blob_gradient</span>.<span class="ident">clone</span>()));
    }

    <span class="doccomment">/// Append anonymous blob as [output blob][1] to the Layer.</span>
    <span class="doccomment">/// [1]: ../layer/index.html</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// [Layer implementations][2] may request creation of anonymous output blobs</span>
    <span class="doccomment">/// via [auto_output_blobs][3]. Since the blobs are not named, other layers can</span>
    <span class="doccomment">/// not use them as their input blobs.</span>
    <span class="doccomment">/// [2]: ./trait.ILayer.html</span>
    <span class="doccomment">/// [3]: ./trait.ILayer.html#method.auto_output_blobs</span>
    <span class="kw">fn</span> <span class="ident">create_anonymous_output</span>(<span class="kw-2">&amp;mut</span> <span class="self">self</span>) {
        <span class="kw">let</span> <span class="ident">blob_name</span> <span class="op">=</span> <span class="string">&quot;(automatic)&quot;</span>.<span class="ident">to_owned</span>();

        <span class="macro">info!</span>(<span class="string">&quot;{} -&gt; {}&quot;</span>, <span class="self">self</span>.<span class="ident">name</span>, <span class="ident">blob_name</span>);

        <span class="kw">let</span> <span class="ident">backend</span>: <span class="ident">Rc</span><span class="op">&lt;</span><span class="kw">dyn</span> <span class="ident">IBackend</span><span class="op">&lt;</span><span class="ident">F</span> <span class="op">=</span> <span class="ident">B::F</span><span class="op">&gt;</span><span class="op">&gt;</span> <span class="op">=</span> <span class="self">self</span>.<span class="ident">backend</span>.<span class="ident">clone</span>();
        <span class="kw">let</span> <span class="ident">output_data</span> <span class="op">=</span> <span class="ident">Arc::new</span>(<span class="ident">RwLock::new</span>(<span class="ident">SharedTensor::new</span>(<span class="kw-2">&amp;</span>[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))); <span class="comment">// [1,1,1] for CUDA</span>
        <span class="kw">let</span> <span class="ident">output_gradient</span> <span class="op">=</span> <span class="ident">Arc::new</span>(<span class="ident">RwLock::new</span>(<span class="ident">SharedTensor::new</span>(<span class="kw-2">&amp;</span>[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))); <span class="comment">// [1,1,1] for CUDA</span>
        <span class="self">self</span>.<span class="ident">output_blobs_data</span>.<span class="ident">push</span>(<span class="ident">output_data</span>);
        <span class="self">self</span>.<span class="ident">output_blobs_gradient</span>.<span class="ident">push</span>(<span class="ident">output_gradient</span>);
    }

    <span class="kw">fn</span> <span class="ident">append_weight</span>(
        <span class="kw-2">&amp;mut</span> <span class="self">self</span>,
        <span class="ident">layer_config</span>: <span class="kw-2">&amp;</span><span class="ident">LayerConfig</span>,
        <span class="ident">registry</span>: <span class="kw-2">&amp;mut</span> <span class="ident">HashMap</span><span class="op">&lt;</span>
            <span class="ident">String</span>,
            (
                <span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span>,
                <span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span>,
                <span class="prelude-ty">Option</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>,
                <span class="prelude-ty">Option</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>,
            ),
        <span class="op">&gt;</span>,
        <span class="ident">layer_id</span>: <span class="ident">usize</span>,
        <span class="ident">weight_id</span>: <span class="ident">usize</span>,
    ) {
        <span class="kw">if</span> <span class="self">self</span>.<span class="ident">worker</span>.<span class="ident">auto_weight_blobs</span>() {
            <span class="macro">info!</span>(<span class="string">&quot;Layer {} - appending weight&quot;</span>, <span class="kw-2">&amp;</span><span class="ident">layer_config</span>.<span class="ident">name</span>);
            <span class="kw">let</span> <span class="ident">weights_len</span> <span class="op">=</span> <span class="self">self</span>.<span class="ident">weights_data</span>.<span class="ident">len</span>();
            <span class="kw">let</span> <span class="ident">weight_name</span> <span class="op">=</span> <span class="kw">if</span> <span class="ident">weights_len</span> <span class="op">&gt;</span> <span class="ident">weight_id</span> {
                <span class="ident">layer_config</span>.<span class="ident">param</span>(<span class="ident">weight_id</span>).<span class="ident">unwrap</span>().<span class="ident">name</span>.<span class="ident">clone</span>()
            } <span class="kw">else</span> {
                <span class="string">&quot;&quot;</span>.<span class="ident">to_owned</span>()
            };

            <span class="comment">// use weight_name (or weight_id as a fallback) as display_name</span>
            <span class="kw">let</span> <span class="ident">display_name</span> <span class="op">=</span> <span class="kw">if</span> <span class="op">!</span><span class="ident">weight_name</span>.<span class="ident">is_empty</span>() {
                <span class="ident">weight_name</span>.<span class="ident">clone</span>()
            } <span class="kw">else</span> {
                <span class="macro">format!</span>(<span class="string">&quot;{}-{}&quot;</span>, <span class="self">self</span>.<span class="ident">name</span>, <span class="ident">weight_id</span>)
            };
            <span class="self">self</span>.<span class="ident">weights_display_names</span>.<span class="ident">push</span>(<span class="ident">display_name</span>.<span class="ident">clone</span>());
            <span class="comment">// create name for registry</span>
            <span class="kw">let</span> <span class="ident">registry_name</span> <span class="op">=</span> <span class="macro">format!</span>(<span class="string">&quot;SHARED_WEIGHT_{}&quot;</span>, <span class="ident">display_name</span>);

            <span class="comment">// add to tracking vectors</span>
            <span class="kw">let</span> <span class="ident">net_weight_id</span> <span class="op">=</span> <span class="ident">weights_len</span>;
            <span class="kw">let</span> <span class="ident">output_data</span> <span class="op">=</span> <span class="self">self</span>.<span class="ident">output_blobs_data</span>[<span class="ident">weight_id</span>].<span class="ident">read</span>().<span class="ident">unwrap</span>();
            <span class="macro">debug!</span>(
                <span class="string">&quot;Layer {} - creating weight and gradient of size {:?}&quot;</span>,
                <span class="kw-2">&amp;</span><span class="ident">layer_config</span>.<span class="ident">name</span>,
                <span class="ident">output_data</span>.<span class="ident">desc</span>()
            );
            <span class="kw">let</span> <span class="ident">weight_data</span> <span class="op">=</span> <span class="ident">Arc::new</span>(<span class="ident">RwLock::new</span>(<span class="ident">SharedTensor::new</span>(<span class="ident">output_data</span>.<span class="ident">desc</span>())));
            <span class="kw">let</span> <span class="ident">weight_bias</span> <span class="op">=</span> <span class="ident">Arc::new</span>(<span class="ident">RwLock::new</span>(<span class="ident">SharedTensor::new</span>(<span class="ident">output_data</span>.<span class="ident">desc</span>())));
            <span class="kw">let</span> <span class="ident">weight_gradient</span> <span class="op">=</span> <span class="ident">Arc::new</span>(<span class="ident">RwLock::new</span>(<span class="ident">SharedTensor::new</span>(<span class="ident">output_data</span>.<span class="ident">desc</span>())));
            <span class="kw">let</span> <span class="ident">weight_bias_gradient</span> <span class="op">=</span> <span class="ident">Arc::new</span>(<span class="ident">RwLock::new</span>(<span class="ident">SharedTensor::new</span>(<span class="ident">output_data</span>.<span class="ident">desc</span>())));
            <span class="self">self</span>.<span class="ident">weights_data</span>.<span class="ident">push</span>(<span class="ident">weight_data</span>.<span class="ident">clone</span>());
            <span class="comment">// Add Bias</span>
            <span class="self">self</span>.<span class="ident">weights_data</span>.<span class="ident">push</span>(<span class="ident">weight_bias</span>.<span class="ident">clone</span>());
            <span class="self">self</span>.<span class="ident">weights_gradient</span>.<span class="ident">push</span>(<span class="ident">weight_gradient</span>.<span class="ident">clone</span>());
            <span class="comment">// Add Bias</span>
            <span class="self">self</span>.<span class="ident">weights_gradient</span>.<span class="ident">push</span>(<span class="ident">weight_bias_gradient</span>.<span class="ident">clone</span>());

            <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">weight_config</span> <span class="op">=</span> <span class="kw-2">&amp;</span><span class="ident">WeightConfig::default</span>();
            <span class="kw">if</span> <span class="ident">layer_config</span>.<span class="ident">params_len</span>() <span class="op">&gt;</span> <span class="ident">weight_id</span> {
                <span class="ident">weight_config</span> <span class="op">=</span> <span class="ident">layer_config</span>.<span class="ident">param</span>(<span class="ident">weight_id</span>).<span class="ident">unwrap</span>();
            }
            <span class="comment">// This layer &quot;owns&quot; this weight blob -- it is either anonymous</span>
            <span class="comment">// (i.e., not given a weight_name) or explicitly given a name that we</span>
            <span class="comment">// haven&#39;t already seen.</span>
            <span class="kw">if</span> <span class="ident">weight_name</span>.<span class="ident">is_empty</span>() <span class="op">|</span><span class="op">|</span> <span class="op">!</span><span class="ident">registry</span>.<span class="ident">contains_key</span>(<span class="kw-2">&amp;</span><span class="ident">registry_name</span>) {
                <span class="comment">// self.weight_owners.push(None);</span>
                <span class="kw">if</span> <span class="op">!</span><span class="ident">weight_name</span>.<span class="ident">is_empty</span>() {
                    <span class="ident">registry</span>.<span class="ident">insert</span>(
                        <span class="ident">weight_name</span>.<span class="ident">clone</span>(),
                        (
                            <span class="ident">weight_data</span>.<span class="ident">clone</span>(),
                            <span class="ident">weight_gradient</span>.<span class="ident">clone</span>(),
                            <span class="ident">weight_config</span>.<span class="ident">lr_mult</span>,
                            <span class="ident">weight_config</span>.<span class="ident">decay_mult</span>,
                        ),
                    );
                }
                <span class="kw">let</span> <span class="ident">learnable_weight_id</span> <span class="op">=</span> <span class="self">self</span>.<span class="ident">learnable_weights</span>.<span class="ident">len</span>();
                <span class="self">self</span>.<span class="ident">learnable_weights</span>.<span class="ident">push</span>(<span class="ident">weight_data</span>.<span class="ident">clone</span>());
                <span class="comment">// self.learnable_weight_ids.push(learnable_weight_id);</span>
                <span class="self">self</span>.<span class="ident">weights_lr</span>.<span class="ident">push</span>(<span class="ident">weight_config</span>.<span class="ident">lr_mult</span>);
                <span class="self">self</span>.<span class="ident">weights_weight_decay</span>.<span class="ident">push</span>(<span class="ident">weight_config</span>.<span class="ident">decay_mult</span>);
            } <span class="kw">else</span> {
                <span class="comment">// Named weight blob with name we&#39;ve seen before: share weights</span>

                <span class="kw">let</span> (<span class="ident">shared_weight_data</span>, <span class="ident">shared_weight_gradient</span>, <span class="ident">shared_lr</span>, <span class="ident">shared_decay_mult</span>) <span class="op">=</span>
                    <span class="ident">registry</span>.<span class="ident">get</span>(<span class="kw-2">&amp;</span><span class="ident">registry_name</span>).<span class="ident">unwrap</span>().<span class="ident">clone</span>();
                <span class="macro">info!</span>(<span class="string">&quot;Sharing weight blob &#39;{}&#39;&quot;</span>, <span class="ident">weight_name</span>.<span class="ident">clone</span>());

                <span class="comment">// can only share parameters if both have same lr_mult</span>
                <span class="kw">if</span> <span class="kw">let</span> <span class="prelude-val">Some</span>(<span class="ident">lr_mult</span>) <span class="op">=</span> <span class="ident">weight_config</span>.<span class="ident">lr_mult</span> {
                    <span class="kw">if</span> <span class="kw">let</span> <span class="prelude-val">Some</span>(<span class="ident">owner_lr_mult</span>) <span class="op">=</span> <span class="ident">shared_lr</span> {
                        <span class="kw">if</span> <span class="op">!</span><span class="ident">lr_mult</span>.<span class="ident">eq</span>(<span class="kw-2">&amp;</span><span class="ident">owner_lr_mult</span>) {
                            <span class="macro">error!</span>(<span class="string">&quot;Shared param &#39;{}&#39; has mismatched lr_mult.&quot;</span>, <span class="ident">weight_name</span>.<span class="ident">clone</span>());
                        }
                    } <span class="kw">else</span> {
                        <span class="comment">// this is the first shared instance that has a lr_mult value so we take that</span>
                        <span class="ident">registry</span>.<span class="ident">remove</span>(<span class="kw-2">&amp;</span><span class="ident">registry_name</span>).<span class="ident">unwrap</span>();
                        <span class="ident">registry</span>.<span class="ident">insert</span>(
                            <span class="ident">registry_name</span>.<span class="ident">clone</span>(),
                            (
                                <span class="ident">shared_weight_data</span>.<span class="ident">clone</span>(),
                                <span class="ident">shared_weight_gradient</span>.<span class="ident">clone</span>(),
                                <span class="ident">weight_config</span>.<span class="ident">lr_mult</span>,
                                <span class="ident">shared_decay_mult</span>,
                            ),
                        );
                    }
                }
                <span class="comment">// can only share weights if both have same decay_mult</span>
                <span class="kw">if</span> <span class="kw">let</span> <span class="prelude-val">Some</span>(<span class="ident">decay_mult</span>) <span class="op">=</span> <span class="ident">weight_config</span>.<span class="ident">decay_mult</span> {
                    <span class="kw">if</span> <span class="kw">let</span> <span class="prelude-val">Some</span>(<span class="ident">owner_decay_mult</span>) <span class="op">=</span> <span class="ident">shared_decay_mult</span> {
                        <span class="kw">if</span> <span class="op">!</span><span class="ident">decay_mult</span>.<span class="ident">eq</span>(<span class="kw-2">&amp;</span><span class="ident">owner_decay_mult</span>) {
                            <span class="macro">error!</span>(<span class="string">&quot;Shared param &#39;{}&#39; has mismatched decay_mult.&quot;</span>, <span class="ident">weight_name</span>.<span class="ident">clone</span>());
                        }
                    } <span class="kw">else</span> {
                        <span class="comment">// this is the first shared instance that has a decay_mult value so we take that</span>
                        <span class="ident">registry</span>.<span class="ident">remove</span>(<span class="kw-2">&amp;</span><span class="ident">registry_name</span>).<span class="ident">unwrap</span>();
                        <span class="ident">registry</span>.<span class="ident">insert</span>(
                            <span class="ident">registry_name</span>,
                            (
                                <span class="ident">shared_weight_data</span>.<span class="ident">clone</span>(),
                                <span class="ident">shared_weight_gradient</span>.<span class="ident">clone</span>(),
                                <span class="ident">shared_lr</span>,
                                <span class="ident">weight_config</span>.<span class="ident">decay_mult</span>,
                            ),
                        );
                    }
                }
            }
        }
    }

    <span class="kw">fn</span> <span class="ident">reshape</span>(<span class="kw-2">&amp;mut</span> <span class="self">self</span>) {
        <span class="kw">match</span> <span class="self">self</span>.<span class="ident">is_using_in_place</span>() {
            <span class="bool-val">false</span> =&gt; {
                <span class="self">self</span>.<span class="ident">worker</span>.<span class="ident">reshape</span>(
                    <span class="self">self</span>.<span class="ident">backend</span>.<span class="ident">clone</span>(),
                    <span class="kw-2">&amp;mut</span> <span class="self">self</span>.<span class="ident">input_blobs_data</span>,
                    <span class="kw-2">&amp;mut</span> <span class="self">self</span>.<span class="ident">input_blobs_gradient</span>,
                    <span class="kw-2">&amp;mut</span> <span class="self">self</span>.<span class="ident">weights_data</span>,
                    <span class="kw-2">&amp;mut</span> <span class="self">self</span>.<span class="ident">weights_gradient</span>,
                    <span class="kw-2">&amp;mut</span> <span class="self">self</span>.<span class="ident">output_blobs_data</span>,
                    <span class="kw-2">&amp;mut</span> <span class="self">self</span>.<span class="ident">output_blobs_gradient</span>,
                );
            }
            <span class="bool-val">true</span> =&gt; {
                <span class="self">self</span>.<span class="ident">worker</span>.<span class="ident">reshape</span>(
                    <span class="self">self</span>.<span class="ident">backend</span>.<span class="ident">clone</span>(),
                    <span class="kw-2">&amp;mut</span> <span class="macro">vec!</span>[],
                    <span class="kw-2">&amp;mut</span> <span class="macro">vec!</span>[],
                    <span class="kw-2">&amp;mut</span> <span class="self">self</span>.<span class="ident">weights_data</span>,
                    <span class="kw-2">&amp;mut</span> <span class="self">self</span>.<span class="ident">weights_gradient</span>,
                    <span class="kw-2">&amp;mut</span> <span class="self">self</span>.<span class="ident">output_blobs_data</span>,
                    <span class="kw-2">&amp;mut</span> <span class="self">self</span>.<span class="ident">output_blobs_gradient</span>,
                );
            }
        }
    }

    <span class="doccomment">/// Initializes layer for [backpropagation][1]</span>
    <span class="doccomment">/// [1]: https://en.wikipedia.org/wiki/Backpropagation</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// Go through all the blobs of a layer to determine which blobs contribute to the</span>
    <span class="doccomment">/// loss of the next layer. We can skip backward computation for blobs that don&#39;t contribute</span>
    <span class="doccomment">/// to the loss.</span>
    <span class="doccomment">/// If all of the blobs skip backpropagation we set a flag to skip backpropagation</span>
    <span class="doccomment">/// of the whole layer.</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">init_backprop</span>(<span class="kw-2">&amp;mut</span> <span class="self">self</span>, <span class="ident">blobs_under_loss</span>: <span class="kw-2">&amp;mut</span> <span class="ident">HashSet</span><span class="op">&lt;</span><span class="ident">String</span><span class="op">&gt;</span>, <span class="ident">blobs_skip_backp</span>: <span class="kw-2">&amp;mut</span> <span class="ident">HashSet</span><span class="op">&lt;</span><span class="ident">String</span><span class="op">&gt;</span>) {
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">layer_contributes_loss</span> <span class="op">=</span> <span class="bool-val">false</span>;
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">layer_skip_propagate_down</span> <span class="op">=</span> <span class="bool-val">true</span>;
        <span class="kw">for</span> (<span class="ident">output_id</span>, <span class="kw">_</span>) <span class="kw">in</span> <span class="self">self</span>.<span class="ident">output_blobs_data</span>.<span class="ident">iter</span>().<span class="ident">enumerate</span>() {
            <span class="kw">let</span> <span class="ident">blob_name</span> <span class="op">=</span> <span class="self">self</span>.<span class="ident">output_blob_names</span>.<span class="ident">get</span>(<span class="ident">output_id</span>);

            <span class="comment">// layer is a loss layer or under a loss layer</span>
            <span class="kw">if</span> <span class="self">self</span>.<span class="ident">loss</span>(<span class="ident">output_id</span>).<span class="ident">is_some</span>() <span class="op">|</span><span class="op">|</span> <span class="ident">blob_name</span>.<span class="ident">is_some</span>() <span class="op">&amp;&amp;</span> <span class="ident">blobs_under_loss</span>.<span class="ident">contains</span>(<span class="ident">blob_name</span>.<span class="ident">unwrap</span>()) {
                <span class="ident">layer_contributes_loss</span> <span class="op">=</span> <span class="bool-val">true</span>;
            }
            <span class="comment">// layer is not marked to skip backpropagation</span>
            <span class="kw">if</span> <span class="ident">blob_name</span>.<span class="ident">is_none</span>() <span class="op">|</span><span class="op">|</span> <span class="ident">blob_name</span>.<span class="ident">is_some</span>() <span class="op">&amp;&amp;</span> <span class="op">!</span><span class="ident">blobs_skip_backp</span>.<span class="ident">contains</span>(<span class="ident">blob_name</span>.<span class="ident">unwrap</span>()) {
                <span class="ident">layer_skip_propagate_down</span> <span class="op">=</span> <span class="bool-val">false</span>;
            }
            <span class="comment">// layer contributes loss to some</span>
            <span class="kw">if</span> <span class="ident">layer_contributes_loss</span> <span class="op">&amp;&amp;</span> <span class="op">!</span><span class="ident">layer_skip_propagate_down</span> {
                <span class="kw">break</span>;
            }
        }

        <span class="comment">// If this layer can skip backward computation, also all his input blobs</span>
        <span class="comment">// don&#39;t need backpropagation</span>
        <span class="kw">if</span> <span class="self">self</span>.<span class="ident">needs_backward</span> <span class="op">&amp;&amp;</span> <span class="ident">layer_skip_propagate_down</span> {
            <span class="self">self</span>.<span class="ident">needs_backward</span> <span class="op">=</span> <span class="bool-val">false</span>;
            <span class="kw">for</span> (<span class="ident">input_id</span>, <span class="kw">_</span>) <span class="kw">in</span> <span class="self">self</span>.<span class="ident">input_blobs_data</span>.<span class="ident">iter</span>().<span class="ident">enumerate</span>() {
                <span class="self">self</span>.<span class="ident">input_need_backwards</span>[<span class="ident">input_id</span>] <span class="op">=</span> <span class="bool-val">false</span>;
            }
        }
        <span class="comment">// layer doesn&#39;t contribute loss so it does not need to be backpropagated</span>
        <span class="kw">if</span> <span class="op">!</span><span class="ident">layer_contributes_loss</span> {
            <span class="self">self</span>.<span class="ident">needs_backward</span> <span class="op">=</span> <span class="bool-val">false</span>;
        }
        {
            <span class="macro">info!</span>(<span class="string">&quot;{} needs backward computation: {}&quot;</span>, <span class="self">self</span>.<span class="ident">name</span>, <span class="self">self</span>.<span class="ident">needs_backward</span>);
        }

        <span class="kw">for</span> (<span class="ident">input_id</span>, <span class="ident">input_name</span>) <span class="kw">in</span> <span class="self">self</span>.<span class="ident">input_blob_names</span>.<span class="ident">iter</span>().<span class="ident">enumerate</span>() {
            <span class="kw">if</span> <span class="ident">layer_contributes_loss</span> {
                <span class="ident">blobs_under_loss</span>.<span class="ident">insert</span>(<span class="ident">input_name</span>.<span class="ident">clone</span>());
            } <span class="kw">else</span> {
                <span class="self">self</span>.<span class="ident">input_need_backwards</span>[<span class="ident">input_id</span>] <span class="op">=</span> <span class="bool-val">false</span>;
            }
            <span class="kw">if</span> <span class="op">!</span><span class="self">self</span>.<span class="ident">input_need_backwards</span>[<span class="ident">input_id</span>] {
                <span class="ident">blobs_skip_backp</span>.<span class="ident">insert</span>(<span class="ident">input_name</span>.<span class="ident">clone</span>());
            }
        }
    }

    <span class="doccomment">/// Set [backpropagation][1] flags to force this layer to backpropagate.</span>
    <span class="doccomment">/// [1]: https://en.wikipedia.org/wiki/Backpropagation</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// Is executed during Network initalization if [NetworkConfig][2].force_backward is true.</span>
    <span class="doccomment">/// Forcing backpropagation is useful for debugging.</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">init_force_backward</span>(<span class="kw-2">&amp;mut</span> <span class="self">self</span>) {
        <span class="self">self</span>.<span class="ident">needs_backward</span> <span class="op">=</span> <span class="bool-val">true</span>;
        <span class="kw">for</span> (<span class="ident">input_id</span>, <span class="kw">_</span>) <span class="kw">in</span> <span class="self">self</span>.<span class="ident">input_need_backwards</span>.<span class="ident">clone</span>().<span class="ident">iter</span>().<span class="ident">enumerate</span>() {
            <span class="self">self</span>.<span class="ident">input_need_backwards</span>[<span class="ident">input_id</span>] <span class="op">=</span> <span class="kw-2">*</span><span class="self">self</span>
                .<span class="ident">input_need_backwards</span>
                .<span class="ident">get</span>(<span class="ident">input_id</span>)
                .<span class="ident">unwrap_or</span>(<span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">worker</span>.<span class="ident">allow_force_backward</span>(<span class="ident">input_id</span>));
        }
        <span class="kw">for</span> (<span class="ident">weight_id</span>, <span class="kw">_</span>) <span class="kw">in</span> <span class="self">self</span>.<span class="ident">weights_data</span>.<span class="ident">clone</span>().<span class="ident">iter</span>().<span class="ident">enumerate</span>() {
            <span class="self">self</span>.<span class="ident">set_weight_propagate_down</span>(<span class="ident">weight_id</span>, <span class="bool-val">true</span>);
        }
    }

    <span class="doccomment">/// Expose the internal inputs of a container layer.</span>
    <span class="kw">fn</span> <span class="ident">expose_inputs</span>(<span class="kw-2">&amp;mut</span> <span class="self">self</span>) {
        <span class="kw">if</span> <span class="kw">let</span> <span class="prelude-val">Some</span>(<span class="ident">inputs</span>) <span class="op">=</span> <span class="self">self</span>.<span class="ident">worker</span>.<span class="ident">inputs_data</span>() {
            <span class="self">self</span>.<span class="ident">input_blobs_data</span> <span class="op">=</span> <span class="ident">inputs</span>;
        }
        <span class="kw">if</span> <span class="kw">let</span> <span class="prelude-val">Some</span>(<span class="ident">gradients</span>) <span class="op">=</span> <span class="self">self</span>.<span class="ident">worker</span>.<span class="ident">inputs_gradients</span>() {
            <span class="self">self</span>.<span class="ident">input_blobs_gradient</span> <span class="op">=</span> <span class="ident">gradients</span>;
        }
    }

    <span class="doccomment">/// Expose the internal outputs of a container layer.</span>
    <span class="kw">fn</span> <span class="ident">expose_outputs</span>(<span class="kw-2">&amp;mut</span> <span class="self">self</span>) {
        <span class="kw">if</span> <span class="kw">let</span> <span class="prelude-val">Some</span>(<span class="ident">outputs</span>) <span class="op">=</span> <span class="self">self</span>.<span class="ident">worker</span>.<span class="ident">outputs_data</span>() {
            <span class="self">self</span>.<span class="ident">output_blobs_data</span> <span class="op">=</span> <span class="ident">outputs</span>;
        }
        <span class="kw">if</span> <span class="kw">let</span> <span class="prelude-val">Some</span>(<span class="ident">gradients</span>) <span class="op">=</span> <span class="self">self</span>.<span class="ident">worker</span>.<span class="ident">outputs_gradients</span>() {
            <span class="self">self</span>.<span class="ident">output_blobs_gradient</span> <span class="op">=</span> <span class="ident">gradients</span>;
        }
    }

    <span class="doccomment">/// Uses the underlying layer implementation to compute a forward step.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// See [ILayer.forward](./trait.ILayer.html#method.forward)</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">forward</span>(<span class="kw-2">&amp;mut</span> <span class="self">self</span>, <span class="ident">inputs</span>: <span class="kw-2">&amp;</span>[<span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span>]) -&gt; <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span> {
        <span class="macro">debug!</span>(<span class="string">&quot;LAYER: {:?}&quot;</span>, <span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">name</span>);
        <span class="kw">for</span> (<span class="ident">input_i</span>, <span class="ident">input</span>) <span class="kw">in</span> <span class="ident">inputs</span>.<span class="ident">iter</span>().<span class="ident">enumerate</span>() {
            <span class="kw">let</span> <span class="ident">reshaped_shape</span> <span class="op">=</span> <span class="self">self</span>.<span class="ident">input_blobs_data</span>[<span class="ident">input_i</span>].<span class="ident">read</span>().<span class="ident">unwrap</span>().<span class="ident">desc</span>().<span class="ident">clone</span>();
            <span class="self">self</span>.<span class="ident">input_blobs_data</span>[<span class="ident">input_i</span>] <span class="op">=</span> <span class="ident">input</span>.<span class="ident">clone</span>();
            <span class="comment">// reshape input tensor to the reshaped shape</span>
            <span class="kw">let</span> <span class="ident">old_shape</span> <span class="op">=</span> <span class="self">self</span>.<span class="ident">input_blobs_data</span>[<span class="ident">input_i</span>].<span class="ident">read</span>().<span class="ident">unwrap</span>().<span class="ident">desc</span>().<span class="ident">clone</span>();
            <span class="kw">if</span> <span class="ident">old_shape</span>.<span class="ident">size</span>() <span class="op">!</span><span class="op">=</span> <span class="ident">reshaped_shape</span>.<span class="ident">size</span>() {
                <span class="macro">panic!</span>(
                    <span class="string">&quot;Input Shape Mismatch\nExpected {:?}\nActual {:?}&quot;</span>,
                    <span class="ident">reshaped_shape</span>, <span class="ident">old_shape</span>
                );
            }
            <span class="self">self</span>.<span class="ident">input_blobs_data</span>[<span class="ident">input_i</span>]
                .<span class="ident">write</span>()
                .<span class="ident">unwrap</span>()
                .<span class="ident">reshape</span>(<span class="kw-2">&amp;</span><span class="ident">reshaped_shape</span>)
                .<span class="ident">unwrap</span>();
        }

        <span class="kw">let</span> <span class="ident">forward_time</span> <span class="op">=</span> <span class="macro">timeit_loops!</span>(<span class="number">1</span>, {
            <span class="kw">if</span> <span class="self">self</span>.<span class="ident">is_using_in_place</span>() {
                <span class="self">self</span>.<span class="ident">worker</span>
                    .<span class="ident">forward</span>(<span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">backend</span>, <span class="kw-2">&amp;</span>[], <span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">weights_data</span>, <span class="kw-2">&amp;mut</span> <span class="self">self</span>.<span class="ident">output_blobs_data</span>);
            } <span class="kw">else</span> {
                <span class="self">self</span>.<span class="ident">worker</span>.<span class="ident">forward</span>(
                    <span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">backend</span>,
                    <span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">input_blobs_data</span>,
                    <span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">weights_data</span>,
                    <span class="kw-2">&amp;mut</span> <span class="self">self</span>.<span class="ident">output_blobs_data</span>,
                );
            }
        });
        <span class="macro">debug!</span>(<span class="string">&quot;{:&lt;15} - Forward time: {:.5} ms&quot;</span>, <span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">name</span>, <span class="ident">forward_time</span> <span class="op">/</span> <span class="number">0.001</span>);
        <span class="self">self</span>.<span class="ident">output_blobs_data</span>.<span class="ident">clone</span>()
    }

    <span class="doccomment">/// Uses the underlying layer implementation to compute a backward step.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// See [ILayer.backward](./trait.ILayer.html#method.backward)</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">backward</span>(<span class="kw-2">&amp;mut</span> <span class="self">self</span>, <span class="ident">output_gradients</span>: <span class="kw-2">&amp;</span>[<span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span>]) -&gt; <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span> {
        <span class="kw">if</span> <span class="self">self</span>.<span class="ident">needs_backward</span> {
            <span class="kw">let</span> <span class="ident">input_gradients</span> <span class="op">=</span> <span class="self">self</span>.<span class="ident">backward_input</span>(<span class="ident">output_gradients</span>);
            <span class="self">self</span>.<span class="ident">backward_parameters</span>();
            <span class="ident">input_gradients</span>
        } <span class="kw">else</span> {
            <span class="macro">vec!</span>[]
        }
    }

    <span class="doccomment">/// Calculate the gradient w.r.t. input.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// This method is mostly used when doing backpropagation.</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">backward_input</span>(
        <span class="kw-2">&amp;mut</span> <span class="self">self</span>,
        <span class="ident">output_gradients</span>: <span class="kw-2">&amp;</span>[<span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span>],
    ) -&gt; <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span> {
        <span class="kw">for</span> (<span class="ident">output_i</span>, <span class="ident">output</span>) <span class="kw">in</span> <span class="ident">output_gradients</span>.<span class="ident">iter</span>().<span class="ident">enumerate</span>() {
            <span class="self">self</span>.<span class="ident">output_blobs_gradient</span>[<span class="ident">output_i</span>] <span class="op">=</span> <span class="ident">output</span>.<span class="ident">clone</span>();
        }

        <span class="kw">if</span> <span class="self">self</span>.<span class="ident">is_using_in_place</span>() {
            <span class="self">self</span>.<span class="ident">worker</span>.<span class="ident">backward_input</span>(
                <span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">backend</span>,
                <span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">weights_data</span>,
                <span class="kw-2">&amp;</span>[],
                <span class="kw-2">&amp;</span>[],
                <span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">input_blobs_data</span>,
                <span class="kw-2">&amp;mut</span> <span class="self">self</span>.<span class="ident">input_blobs_gradient</span>,
            )
        } <span class="kw">else</span> {
            <span class="self">self</span>.<span class="ident">worker</span>.<span class="ident">backward_input</span>(
                <span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">backend</span>,
                <span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">weights_data</span>,
                <span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">output_blobs_data</span>,
                <span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">output_blobs_gradient</span>,
                <span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">input_blobs_data</span>,
                <span class="kw-2">&amp;mut</span> <span class="self">self</span>.<span class="ident">input_blobs_gradient</span>,
            )
        }

        <span class="self">self</span>.<span class="ident">input_blobs_gradient</span>.<span class="ident">clone</span>()
    }

    <span class="doccomment">/// Calculate the gradient w.r.t. parameters.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// &quot;Parameters&quot; here refers to weights and also possibly bias, depending on the layer.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// This method is mostly used when doing backpropagation.</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">backward_parameters</span>(<span class="kw-2">&amp;mut</span> <span class="self">self</span>) {
        <span class="self">self</span>.<span class="ident">worker</span>.<span class="ident">backward_parameters</span>(
            <span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">backend</span>,
            <span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">output_blobs_data</span>,
            <span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">output_blobs_gradient</span>,
            <span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">input_blobs_data</span>,
            <span class="kw-2">&amp;mut</span> <span class="self">self</span>.<span class="ident">weights_gradient</span>,
        )
    }

    <span class="doccomment">/// Synchronize the layers backend.</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">synchronize</span>(<span class="kw-2">&amp;</span><span class="self">self</span>) {
        <span class="self">self</span>.<span class="ident">backend</span>.<span class="ident">synchronize</span>().<span class="ident">unwrap</span>();
    }

    <span class="doccomment">/// Updates the [weights][1] with the weight update computed by the [Solver][2].</span>
    <span class="doccomment">/// [1]: https://en.wikipedia.org/wiki/Synaptic_weight</span>
    <span class="doccomment">/// [2]: ../solver/struct.Solver.html</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// Updating the weights is the last step of computing a [Solver][2] minibatch.</span>
    <span class="doccomment">/// The update value is computed in previous steps according to the [learning rate policy][3]</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// [3]: ../solver/enum.LRPolicy.html</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">update_weights</span><span class="op">&lt;</span><span class="ident">SolverB</span>: <span class="ident">IBackend</span> <span class="op">+</span> <span class="ident"><span class="kw">crate</span>::util::SolverOps</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span>(<span class="kw-2">&amp;mut</span> <span class="self">self</span>, <span class="ident">backend</span>: <span class="kw-2">&amp;</span><span class="ident">SolverB</span>) {
        <span class="comment">// PERF: allocate this scalar once</span>
        <span class="kw">let</span> <span class="ident">shared_a</span> <span class="op">=</span> <span class="ident"><span class="kw">crate</span>::util::native_scalar</span>(<span class="op">-</span><span class="number">1f32</span>);
        <span class="kw">for</span> (<span class="ident">weight_gradient</span>, <span class="ident">weight_data</span>) <span class="kw">in</span> <span class="self">self</span>
            .<span class="ident">learnable_weights_gradients</span>()
            .<span class="ident">iter</span>()
            .<span class="ident">zip</span>(<span class="kw-2">&amp;mut</span> <span class="self">self</span>.<span class="ident">learnable_weights_data</span>())
        {
            <span class="ident">backend</span>
                .<span class="ident">axpy</span>(
                    <span class="kw-2">&amp;</span><span class="ident">shared_a</span>,
                    <span class="kw-2">&amp;</span><span class="ident">weight_gradient</span>.<span class="ident">read</span>().<span class="ident">unwrap</span>(),
                    <span class="kw-2">&amp;mut</span> <span class="ident">weight_data</span>.<span class="ident">write</span>().<span class="ident">unwrap</span>(),
                )
                .<span class="ident">unwrap</span>();
        }
    }

    <span class="doccomment">/// Clears the [weights][1] gradients and zero-inits them.</span>
    <span class="doccomment">/// [1]: https://en.wikipedia.org/wiki/Synaptic_weight</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// The gradients for the weights accumulate over the backpropagation steps of</span>
    <span class="doccomment">/// a [Solver][2] minibatch and are cleared between each minibatch</span>
    <span class="doccomment">/// to start over with a clean slate.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// [2]: ../solver/struct.Solver.html</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">clear_weights_gradients</span>(<span class="kw-2">&amp;mut</span> <span class="self">self</span>) {
        <span class="kw">for</span> <span class="ident">weight_gradient</span> <span class="kw">in</span> <span class="kw-2">&amp;mut</span> <span class="self">self</span>.<span class="ident">learnable_weights_gradients</span>().<span class="ident">iter</span>() {
            <span class="kw">let</span> <span class="ident">filler</span> <span class="op">=</span> <span class="ident"><span class="kw">crate</span>::weight::FillerType::Constant</span> { <span class="ident">value</span>: <span class="number">0f32</span> };
            <span class="ident">filler</span>.<span class="ident">fill</span>(<span class="kw-2">&amp;mut</span> <span class="ident">weight_gradient</span>.<span class="ident">write</span>().<span class="ident">unwrap</span>());
        }
    }

    <span class="doccomment">/// Serialize the Layer and it&#39;s weights to a Cap&#39;n Proto file at the specified path.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// You can find the capnp schema [here](../../../../capnp/juice.capnp).</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// ```</span>
    <span class="doccomment">/// # #[cfg(feature = &quot;native&quot;)]</span>
    <span class="doccomment">/// # mod native {</span>
    <span class="doccomment">/// # use std::rc::Rc;</span>
    <span class="doccomment">/// # use juice::layer::*;</span>
    <span class="doccomment">/// # use juice::layers::*;</span>
    <span class="doccomment">/// # use juice::util;</span>
    <span class="doccomment">/// # pub fn test() {</span>
    <span class="doccomment">/// #</span>
    <span class="doccomment">/// let mut net_cfg = SequentialConfig::default();</span>
    <span class="doccomment">/// // ... set up network ...</span>
    <span class="doccomment">/// let cfg = LayerConfig::new(&quot;network&quot;, net_cfg);</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// let native_backend = Rc::new(util::native_backend());</span>
    <span class="doccomment">/// let mut layer = Layer::from_config(native_backend, &amp;cfg);</span>
    <span class="doccomment">/// // ... do stuff with the layer ...</span>
    <span class="doccomment">/// // ... and save it</span>
    <span class="doccomment">/// layer.save(&quot;mynetwork&quot;).unwrap();</span>
    <span class="doccomment">/// #</span>
    <span class="doccomment">/// # }}</span>
    <span class="doccomment">/// #</span>
    <span class="doccomment">/// # #[cfg(not(feature = &quot;native&quot;))]</span>
    <span class="doccomment">/// # mod native {</span>
    <span class="doccomment">/// # pub fn test() {}</span>
    <span class="doccomment">/// # }</span>
    <span class="doccomment">/// #</span>
    <span class="doccomment">/// # fn main() {</span>
    <span class="doccomment">/// #     if cfg!(feature = &quot;native&quot;) {</span>
    <span class="doccomment">/// #         crate::native::test();</span>
    <span class="doccomment">/// #    }</span>
    <span class="doccomment">/// # }</span>
    <span class="doccomment">/// ```</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">save</span><span class="op">&lt;</span><span class="ident">P</span>: <span class="ident">AsRef</span><span class="op">&lt;</span><span class="ident">Path</span><span class="op">&gt;</span><span class="op">&gt;</span>(<span class="kw-2">&amp;mut</span> <span class="self">self</span>, <span class="ident">path</span>: <span class="ident">P</span>) -&gt; <span class="ident">io::Result</span><span class="op">&lt;</span>()<span class="op">&gt;</span> {
        <span class="kw">let</span> <span class="ident">path</span> <span class="op">=</span> <span class="ident">path</span>.<span class="ident">as_ref</span>();
        <span class="kw">let</span> <span class="kw-2">ref</span> <span class="kw-2">mut</span> <span class="ident">out</span> <span class="op">=</span> <span class="ident">File::create</span>(<span class="ident">path</span>)<span class="question-mark">?</span>;

        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">message</span> <span class="op">=</span> <span class="ident">::capnp::message::Builder::new_default</span>();
        {
            <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">layer</span> <span class="op">=</span> <span class="ident">message</span>.<span class="ident">init_root</span>::<span class="op">&lt;</span><span class="ident">capnp_layer::Builder</span><span class="op">&gt;</span>();
            <span class="self">self</span>.<span class="ident">write_capnp</span>(<span class="kw-2">&amp;mut</span> <span class="ident">layer</span>);
        }
        <span class="ident">::capnp::serialize_packed::write_message</span>(<span class="ident">out</span>, <span class="kw-2">&amp;</span><span class="ident">message</span>).<span class="ident">unwrap</span>();

        <span class="prelude-val">Ok</span>(())
    }

    <span class="doccomment">/// Read a Cap&#39;n Proto file at the specified path and deserialize the Layer inside it.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// You can find the capnp schema [here](../../../../capnp/juice.capnp).</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// ```</span>
    <span class="doccomment">/// # extern crate juice;</span>
    <span class="doccomment">/// # extern crate coaster;</span>
    <span class="doccomment">/// # #[cfg(feature = &quot;native&quot;)]</span>
    <span class="doccomment">/// # mod native {</span>
    <span class="doccomment">/// # use std::rc::Rc;</span>
    <span class="doccomment">/// # use juice::layer::*;</span>
    <span class="doccomment">/// # use juice::layers::*;</span>
    <span class="doccomment">/// # use juice::util;</span>
    <span class="doccomment">/// use coaster::prelude::*;</span>
    <span class="doccomment">/// # pub fn test() {</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// let native_backend = Rc::new(util::native_backend());</span>
    <span class="doccomment">/// # let mut net_cfg = SequentialConfig::default();</span>
    <span class="doccomment">/// # let cfg = LayerConfig::new(&quot;network&quot;, net_cfg);</span>
    <span class="doccomment">/// # let mut layer = Layer::from_config(native_backend.clone(), &amp;cfg);</span>
    <span class="doccomment">/// # layer.save(&quot;mynetwork&quot;).unwrap();</span>
    <span class="doccomment">/// // Load layer from file &quot;mynetwork&quot;</span>
    <span class="doccomment">/// let layer = Layer::&lt;Backend&lt;Native&gt;&gt;::load(native_backend, &quot;mynetwork&quot;).unwrap();</span>
    <span class="doccomment">/// #</span>
    <span class="doccomment">/// # }}</span>
    <span class="doccomment">/// #</span>
    <span class="doccomment">/// # #[cfg(not(feature = &quot;native&quot;))]</span>
    <span class="doccomment">/// # mod native {</span>
    <span class="doccomment">/// # pub fn test() {}</span>
    <span class="doccomment">/// # }</span>
    <span class="doccomment">/// #</span>
    <span class="doccomment">/// # fn main() {</span>
    <span class="doccomment">/// #     if cfg!(feature = &quot;native&quot;) {</span>
    <span class="doccomment">/// #         crate::native::test();</span>
    <span class="doccomment">/// #    }</span>
    <span class="doccomment">/// # }</span>
    <span class="doccomment">/// ```</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">load</span><span class="op">&lt;</span><span class="ident">LB</span>: <span class="ident">IBackend</span> <span class="op">+</span> <span class="ident">LayerOps</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span> <span class="op">+</span> <span class="lifetime">&#39;static</span>, <span class="ident">P</span>: <span class="ident">AsRef</span><span class="op">&lt;</span><span class="ident">Path</span><span class="op">&gt;</span><span class="op">&gt;</span>(
        <span class="ident">backend</span>: <span class="ident">Rc</span><span class="op">&lt;</span><span class="ident">LB</span><span class="op">&gt;</span>,
        <span class="ident">path</span>: <span class="ident">P</span>,
    ) -&gt; <span class="ident">io::Result</span><span class="op">&lt;</span><span class="ident">Layer</span><span class="op">&lt;</span><span class="ident">LB</span><span class="op">&gt;</span><span class="op">&gt;</span> {
        <span class="kw">let</span> <span class="ident">path</span> <span class="op">=</span> <span class="ident">path</span>.<span class="ident">as_ref</span>();
        <span class="kw">let</span> <span class="kw-2">ref</span> <span class="kw-2">mut</span> <span class="ident">file</span> <span class="op">=</span> <span class="ident">File::open</span>(<span class="ident">path</span>)<span class="question-mark">?</span>;
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">reader</span> <span class="op">=</span> <span class="ident">BufReader::new</span>(<span class="ident">file</span>);

        <span class="kw">let</span> <span class="ident">message_reader</span> <span class="op">=</span>
            <span class="ident">::capnp::serialize_packed::read_message</span>(<span class="kw-2">&amp;mut</span> <span class="ident">reader</span>, <span class="ident">::capnp::message::ReaderOptions::new</span>()).<span class="ident">unwrap</span>();
        <span class="kw">let</span> <span class="ident">read_layer</span> <span class="op">=</span> <span class="ident">message_reader</span>.<span class="ident">get_root</span>::<span class="op">&lt;</span><span class="ident">capnp_layer::Reader</span><span class="op">&gt;</span>().<span class="ident">unwrap</span>();

        <span class="kw">let</span> <span class="ident">name</span> <span class="op">=</span> <span class="ident">read_layer</span>.<span class="ident">get_name</span>().<span class="ident">unwrap</span>().<span class="ident">to_owned</span>();
        <span class="kw">let</span> <span class="ident">layer_config</span> <span class="op">=</span> <span class="ident">LayerConfig::read_capnp</span>(<span class="ident">read_layer</span>.<span class="ident">get_config</span>().<span class="ident">unwrap</span>());
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">layer</span> <span class="op">=</span> <span class="ident">Layer::from_config</span>(<span class="ident">backend</span>, <span class="kw-2">&amp;</span><span class="ident">layer_config</span>);
        <span class="ident">layer</span>.<span class="ident">name</span> <span class="op">=</span> <span class="ident">name</span>;

        <span class="kw">let</span> <span class="ident">read_weights</span> <span class="op">=</span> <span class="ident">read_layer</span>.<span class="ident">get_weights_data</span>().<span class="ident">unwrap</span>();

        <span class="kw">let</span> <span class="ident">names</span> <span class="op">=</span> <span class="ident">layer</span>.<span class="ident">learnable_weights_names</span>();
        <span class="kw">let</span> <span class="ident">weights_data</span> <span class="op">=</span> <span class="ident">layer</span>.<span class="ident">learnable_weights_data</span>();

        <span class="kw">let</span> <span class="ident">native_backend</span> <span class="op">=</span> <span class="ident">Backend</span>::<span class="op">&lt;</span><span class="ident">Native</span><span class="op">&gt;</span><span class="ident">::default</span>().<span class="ident">unwrap</span>();
        <span class="kw">for</span> (<span class="ident">i</span>, (<span class="ident">name</span>, <span class="ident">weight</span>)) <span class="kw">in</span> <span class="ident">names</span>.<span class="ident">iter</span>().<span class="ident">zip</span>(<span class="ident">weights_data</span>).<span class="ident">enumerate</span>() {
            <span class="kw">for</span> <span class="ident">j</span> <span class="kw">in</span> <span class="number">0</span>..<span class="ident">read_weights</span>.<span class="ident">len</span>() {
                <span class="kw">let</span> <span class="ident">capnp_weight</span> <span class="op">=</span> <span class="ident">read_weights</span>.<span class="ident">get</span>(<span class="ident">i</span> <span class="kw">as</span> <span class="ident">u32</span>);
                <span class="kw">if</span> <span class="ident">capnp_weight</span>.<span class="ident">get_name</span>().<span class="ident">unwrap</span>() <span class="op">!</span><span class="op">=</span> <span class="ident">name</span> {
                    <span class="kw">continue</span>;
                }

                <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">weight_lock</span> <span class="op">=</span> <span class="ident">weight</span>.<span class="ident">write</span>().<span class="ident">unwrap</span>();

                <span class="kw">let</span> <span class="ident">capnp_tensor</span> <span class="op">=</span> <span class="ident">capnp_weight</span>.<span class="ident">get_tensor</span>().<span class="ident">unwrap</span>();
                <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">shape</span> <span class="op">=</span> <span class="ident">Vec::new</span>();
                <span class="kw">let</span> <span class="ident">capnp_shape</span> <span class="op">=</span> <span class="ident">capnp_tensor</span>.<span class="ident">get_shape</span>().<span class="ident">unwrap</span>();
                <span class="kw">for</span> <span class="ident">k</span> <span class="kw">in</span> <span class="number">0</span>..<span class="ident">capnp_shape</span>.<span class="ident">len</span>() {
                    <span class="ident">shape</span>.<span class="ident">push</span>(<span class="ident">capnp_shape</span>.<span class="ident">get</span>(<span class="ident">k</span>) <span class="kw">as</span> <span class="ident">usize</span>)
                }
                <span class="ident">weight_lock</span>.<span class="ident">reshape</span>(<span class="kw-2">&amp;</span><span class="ident">shape</span>).<span class="ident">unwrap</span>();

                <span class="kw">let</span> <span class="ident">native_slice</span> <span class="op">=</span> <span class="ident">weight_lock</span>
                    .<span class="ident">write_only</span>(<span class="ident">native_backend</span>.<span class="ident">device</span>())
                    .<span class="ident">unwrap</span>()
                    .<span class="ident">as_mut_slice</span>::<span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>();
                <span class="kw">let</span> <span class="ident">data</span> <span class="op">=</span> <span class="ident">capnp_tensor</span>.<span class="ident">get_data</span>().<span class="ident">unwrap</span>();
                <span class="kw">for</span> <span class="ident">k</span> <span class="kw">in</span> <span class="number">0</span>..<span class="ident">data</span>.<span class="ident">len</span>() {
                    <span class="ident">native_slice</span>[<span class="ident">k</span> <span class="kw">as</span> <span class="ident">usize</span>] <span class="op">=</span> <span class="ident">data</span>.<span class="ident">get</span>(<span class="ident">k</span>);
                }
            }
        }

        <span class="prelude-val">Ok</span>(<span class="ident">layer</span>)
    }

    <span class="doccomment">/// Sets whether the layer should compute gradients w.r.t. a</span>
    <span class="doccomment">/// weight at a particular index given by `weight_id`.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// See [`weight_propagate_down`][1]</span>
    <span class="doccomment">/// ./struct.Layer.html</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">set_weight_propagate_down</span>(<span class="kw-2">&amp;mut</span> <span class="self">self</span>, <span class="ident">weight_id</span>: <span class="ident">usize</span>, <span class="ident">value</span>: <span class="ident">bool</span>) {
        <span class="kw">if</span> <span class="self">self</span>.<span class="ident">weight_propagate_down</span>.<span class="ident">len</span>() <span class="op">&lt;</span><span class="op">=</span> <span class="ident">weight_id</span> {
            <span class="self">self</span>.<span class="ident">weight_propagate_down</span>.<span class="ident">resize</span>(<span class="ident">weight_id</span> <span class="op">+</span> <span class="number">1</span>, <span class="bool-val">true</span>);
        }
        <span class="self">self</span>.<span class="ident">weight_propagate_down</span>[<span class="ident">weight_id</span>] <span class="op">=</span> <span class="ident">value</span>;
    }

    <span class="doccomment">/// Returns `true` when the layer is using in-place computation.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// For a layer to use in-place computation it needs to support it via `compute_in_place`</span>
    <span class="doccomment">/// and the names of the first input and output tensor have to match.</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">is_using_in_place</span>(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; <span class="ident">bool</span> {
        <span class="self">self</span>.<span class="ident">worker</span>.<span class="ident">compute_in_place</span>()
            <span class="op">&amp;&amp;</span> <span class="self">self</span>.<span class="ident">input_blob_names</span>.<span class="ident">get</span>(<span class="number">0</span>).<span class="ident">is_some</span>()
            <span class="op">&amp;&amp;</span> <span class="self">self</span>.<span class="ident">output_blob_names</span>.<span class="ident">get</span>(<span class="number">0</span>).<span class="ident">is_some</span>()
            <span class="op">&amp;&amp;</span> <span class="self">self</span>.<span class="ident">input_blob_names</span>[<span class="number">0</span>] <span class="op">==</span> <span class="self">self</span>.<span class="ident">output_blob_names</span>[<span class="number">0</span>]
    }

    <span class="doccomment">/// Returns the names of all the input blobs.</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">input_blob_names</span>(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; <span class="kw-2">&amp;</span>[<span class="ident">String</span>] {
        <span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">input_blob_names</span>
    }

    <span class="doccomment">/// Returns the [loss weight][1] associated with the weight blob</span>
    <span class="doccomment">/// with id `weight_id`.</span>
    <span class="doccomment">/// [1]: http://caffe.berkeleyvision.org/tutorial/loss.html</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">loss</span>(<span class="kw-2">&amp;</span><span class="self">self</span>, <span class="ident">weight_id</span>: <span class="ident">usize</span>) -&gt; <span class="prelude-ty">Option</span><span class="op">&lt;</span><span class="kw-2">&amp;</span><span class="ident">f32</span><span class="op">&gt;</span> {
        <span class="self">self</span>.<span class="ident">loss</span>.<span class="ident">get</span>(<span class="ident">weight_id</span>)
    }

    <span class="doccomment">/// Returns all the learnable weights in the layer.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// If the layer is a container layer it will return all the weights of the</span>
    <span class="doccomment">/// layers inside it.</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">learnable_weights_data</span>(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span> {
        <span class="kw">if</span> <span class="kw">let</span> <span class="prelude-val">Some</span>(<span class="ident">weights</span>) <span class="op">=</span> <span class="self">self</span>.<span class="ident">worker</span>.<span class="ident">learnable_weights</span>() {
            <span class="ident">weights</span>
        } <span class="kw">else</span> {
            <span class="self">self</span>.<span class="ident">weights_data</span>.<span class="ident">clone</span>()
        }
    }

    <span class="doccomment">/// Returns the gradients for all the learnable weights in the layer.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// If the layer is a container layer it will return all the gradients of the</span>
    <span class="doccomment">/// layers inside it.</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">learnable_weights_gradients</span>(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span> {
        <span class="kw">if</span> <span class="kw">let</span> <span class="prelude-val">Some</span>(<span class="ident">gradients</span>) <span class="op">=</span> <span class="self">self</span>.<span class="ident">worker</span>.<span class="ident">learnable_weights_gradients</span>() {
            <span class="ident">gradients</span>
        } <span class="kw">else</span> {
            <span class="self">self</span>.<span class="ident">weights_gradient</span>.<span class="ident">clone</span>()
        }
    }

    <span class="doccomment">/// Returns the names of all the learnable weights in the layer.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// If the layer is a container layer it will return all the names of the</span>
    <span class="doccomment">/// layers inside it.</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">learnable_weights_names</span>(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">String</span><span class="op">&gt;</span> {
        <span class="kw">if</span> <span class="kw">let</span> <span class="prelude-val">Some</span>(<span class="ident">names</span>) <span class="op">=</span> <span class="self">self</span>.<span class="ident">worker</span>.<span class="ident">learnable_weights_names</span>() {
            <span class="ident">names</span>
        } <span class="kw">else</span> {
            <span class="self">self</span>.<span class="ident">weights_display_names</span>.<span class="ident">clone</span>()
        }
    }

    <span class="doccomment">/// Returns the learning rate for all the learnable weights in the layer.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// If the layer is a container layer it will return all learning rates of the</span>
    <span class="doccomment">/// layers inside it.</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">learnable_weights_lr</span>(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; <span class="ident">Vec</span><span class="op">&lt;</span><span class="prelude-ty">Option</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span> {
        <span class="kw">if</span> <span class="kw">let</span> <span class="prelude-val">Some</span>(<span class="ident">lr</span>) <span class="op">=</span> <span class="self">self</span>.<span class="ident">worker</span>.<span class="ident">learnable_weights_lr</span>() {
            <span class="ident">lr</span>
        }
        <span class="comment">// else { self.weights_lr.clone() }</span>
        <span class="kw">else</span> {
            <span class="self">self</span>.<span class="ident">learnable_weights_data</span>()
                .<span class="ident">iter</span>()
                .<span class="ident">map</span>(<span class="op">|</span><span class="kw">_</span><span class="op">|</span> <span class="prelude-val">Some</span>(<span class="number">1f32</span>))
                .<span class="ident">collect</span>::<span class="op">&lt;</span><span class="ident">Vec</span><span class="op">&lt;</span><span class="kw">_</span><span class="op">&gt;</span><span class="op">&gt;</span>()
        }
    }
}

<span class="attribute">#[<span class="ident">allow</span>(<span class="ident">unsafe_code</span>)]</span>
<span class="kw">unsafe</span> <span class="kw">impl</span><span class="op">&lt;</span><span class="ident">B</span>: <span class="ident">IBackend</span><span class="op">&gt;</span> <span class="ident">Send</span> <span class="kw">for</span> <span class="ident">Layer</span><span class="op">&lt;</span><span class="ident">B</span><span class="op">&gt;</span> {}

<span class="kw">impl</span><span class="op">&lt;</span><span class="lifetime">&#39;a</span>, <span class="ident">B</span>: <span class="ident">IBackend</span><span class="op">&gt;</span> <span class="ident">CapnpWrite</span><span class="op">&lt;</span><span class="lifetime">&#39;a</span><span class="op">&gt;</span> <span class="kw">for</span> <span class="ident">Layer</span><span class="op">&lt;</span><span class="ident">B</span><span class="op">&gt;</span> {
    <span class="kw">type</span> <span class="ident">Builder</span> <span class="op">=</span> <span class="ident">capnp_layer::Builder</span><span class="op">&lt;</span><span class="lifetime">&#39;a</span><span class="op">&gt;</span>;

    <span class="doccomment">/// Write the Layer into a capnp message.</span>
    <span class="kw">fn</span> <span class="ident">write_capnp</span>(<span class="kw-2">&amp;</span><span class="self">self</span>, <span class="ident">builder</span>: <span class="kw-2">&amp;mut</span> <span class="ident"><span class="self">Self</span>::Builder</span>) {
        <span class="ident">builder</span>.<span class="ident">set_name</span>(<span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">name</span>);
        {
            <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">layer_config</span> <span class="op">=</span> <span class="ident">builder</span>.<span class="ident">reborrow</span>().<span class="ident">init_config</span>();
            <span class="self">self</span>.<span class="ident">config</span>.<span class="ident">write_capnp</span>(<span class="kw-2">&amp;mut</span> <span class="ident">layer_config</span>);
        }
        {
            <span class="kw">let</span> <span class="ident">native_backend</span> <span class="op">=</span> <span class="ident">Backend</span>::<span class="op">&lt;</span><span class="ident">Native</span><span class="op">&gt;</span><span class="ident">::default</span>().<span class="ident">unwrap</span>();
            <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">weights</span> <span class="op">=</span> <span class="ident">builder</span>
                .<span class="ident">reborrow</span>()
                .<span class="ident">init_weights_data</span>(<span class="self">self</span>.<span class="ident">learnable_weights_names</span>().<span class="ident">len</span>() <span class="kw">as</span> <span class="ident">u32</span>);
            <span class="kw">let</span> <span class="ident">names</span> <span class="op">=</span> <span class="self">self</span>.<span class="ident">learnable_weights_names</span>();
            <span class="kw">let</span> <span class="ident">weights_data</span> <span class="op">=</span> <span class="self">self</span>.<span class="ident">learnable_weights_data</span>();

            <span class="kw">for</span> (<span class="ident">i</span>, (<span class="ident">name</span>, <span class="ident">weight</span>)) <span class="kw">in</span> <span class="ident">names</span>.<span class="ident">iter</span>().<span class="ident">zip</span>(<span class="ident">weights_data</span>).<span class="ident">enumerate</span>() {
                <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">capnp_weight</span> <span class="op">=</span> <span class="ident">weights</span>.<span class="ident">reborrow</span>().<span class="ident">get</span>(<span class="ident">i</span> <span class="kw">as</span> <span class="ident">u32</span>);
                <span class="ident">capnp_weight</span>.<span class="ident">set_name</span>(<span class="ident">name</span>);

                <span class="kw">let</span> <span class="ident">weight_lock</span> <span class="op">=</span> <span class="ident">weight</span>.<span class="ident">write</span>().<span class="ident">unwrap</span>();

                <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">tensor</span> <span class="op">=</span> <span class="ident">capnp_weight</span>.<span class="ident">init_tensor</span>();
                {
                    <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">tensor_shape</span> <span class="op">=</span> <span class="ident">tensor</span>.<span class="ident">reborrow</span>().<span class="ident">init_shape</span>(<span class="ident">weight_lock</span>.<span class="ident">desc</span>().<span class="ident">len</span>() <span class="kw">as</span> <span class="ident">u32</span>);
                    <span class="kw">for</span> (<span class="ident">i</span>, <span class="ident">dim</span>) <span class="kw">in</span> <span class="ident">weight_lock</span>.<span class="ident">desc</span>().<span class="ident">iter</span>().<span class="ident">enumerate</span>() {
                        <span class="ident">tensor_shape</span>.<span class="ident">set</span>(<span class="ident">i</span> <span class="kw">as</span> <span class="ident">u32</span>, <span class="kw-2">*</span><span class="ident">dim</span> <span class="kw">as</span> <span class="ident">u64</span>);
                    }
                }
                {
                    <span class="kw">let</span> <span class="ident">native_slice</span> <span class="op">=</span> <span class="ident">weight_lock</span>.<span class="ident">read</span>(<span class="ident">native_backend</span>.<span class="ident">device</span>()).<span class="ident">unwrap</span>().<span class="ident">as_slice</span>::<span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>();
                    <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">tensor_data</span> <span class="op">=</span> <span class="ident">tensor</span>.<span class="ident">reborrow</span>().<span class="ident">init_data</span>(<span class="ident">native_slice</span>.<span class="ident">len</span>() <span class="kw">as</span> <span class="ident">u32</span>);
                    <span class="kw">for</span> (<span class="ident">i</span>, <span class="ident">datum</span>) <span class="kw">in</span> <span class="ident">native_slice</span>.<span class="ident">iter</span>().<span class="ident">enumerate</span>() {
                        <span class="ident">tensor_data</span>.<span class="ident">set</span>(<span class="ident">i</span> <span class="kw">as</span> <span class="ident">u32</span>, <span class="kw-2">*</span><span class="ident">datum</span>);
                    }
                }
            }
        }
    }
}

<span class="kw">impl</span><span class="op">&lt;</span><span class="ident">B</span>: <span class="ident">IBackend</span> <span class="op">+</span> <span class="ident">LayerOps</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span> <span class="op">+</span> <span class="ident"><span class="kw">crate</span>::coblas::plugin::Copy</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span> <span class="op">+</span> <span class="lifetime">&#39;static</span><span class="op">&gt;</span> <span class="ident">Layer</span><span class="op">&lt;</span><span class="ident">B</span><span class="op">&gt;</span> {
    <span class="doccomment">/// Creates a new Layer from a [LayerConfig][1].</span>
    <span class="doccomment">/// [1]: ./struct.LayerConfig.html</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">from_config</span>(<span class="ident">backend</span>: <span class="ident">Rc</span><span class="op">&lt;</span><span class="ident">B</span><span class="op">&gt;</span>, <span class="ident">config</span>: <span class="kw-2">&amp;</span><span class="ident">LayerConfig</span>) -&gt; <span class="ident">Layer</span><span class="op">&lt;</span><span class="ident">B</span><span class="op">&gt;</span> {
        <span class="kw">let</span> <span class="ident">cl</span> <span class="op">=</span> <span class="ident">config</span>.<span class="ident">clone</span>();
        <span class="kw">let</span> <span class="ident">cfg</span> <span class="op">=</span> <span class="ident">Box</span>::<span class="op">&lt;</span><span class="ident">LayerConfig</span><span class="op">&gt;</span><span class="ident">::new</span>(<span class="ident">cl</span>);
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">layer</span> <span class="op">=</span> <span class="ident">Layer</span> {
            <span class="ident">name</span>: <span class="ident">cfg</span>.<span class="ident">name</span>.<span class="ident">clone</span>(),

            <span class="ident">needs_backward</span>: <span class="bool-val">true</span>,

            <span class="ident">weights_data</span>: <span class="ident">Vec::new</span>(),
            <span class="ident">weights_gradient</span>: <span class="ident">Vec::new</span>(),
            <span class="ident">learnable_weights</span>: <span class="ident">Vec::new</span>(),
            <span class="ident">weight_propagate_down</span>: <span class="ident">Vec::new</span>(),
            <span class="ident">weights_lr</span>: <span class="ident">Vec::new</span>(),
            <span class="ident">weights_weight_decay</span>: <span class="ident">Vec::new</span>(),
            <span class="ident">weights_display_names</span>: <span class="ident">Vec::new</span>(),

            <span class="ident">input_blobs_data</span>: <span class="ident">Vec::new</span>(),
            <span class="ident">input_blobs_gradient</span>: <span class="ident">Vec::new</span>(),
            <span class="ident">input_blob_names</span>: <span class="ident">Vec::new</span>(),
            <span class="ident">input_need_backwards</span>: <span class="ident">Vec::new</span>(),

            <span class="ident">output_blobs_data</span>: <span class="ident">Vec::new</span>(),
            <span class="ident">output_blobs_gradient</span>: <span class="ident">Vec::new</span>(),
            <span class="ident">output_blob_names</span>: <span class="ident">Vec::new</span>(),
            <span class="ident">loss</span>: <span class="macro">vec!</span>[<span class="number">1f32</span>, <span class="number">1f32</span>, <span class="number">1f32</span>],

            <span class="ident">blob_names</span>: <span class="ident">HashMap::new</span>(),

            <span class="ident">backend</span>: <span class="ident">backend</span>.<span class="ident">clone</span>(),

            <span class="ident">worker</span>: <span class="ident">Layer</span>::<span class="op">&lt;</span><span class="ident">B</span><span class="op">&gt;</span><span class="ident">::worker_from_config</span>(<span class="ident">backend</span>, <span class="kw-2">&amp;</span><span class="ident">cfg</span>),
            <span class="ident">config</span>: <span class="ident">cfg</span>,
        };
        <span class="ident">layer</span>.<span class="ident">expose_inputs</span>();
        <span class="ident">layer</span>.<span class="ident">expose_outputs</span>();

        <span class="ident">layer</span>
    }

    <span class="doccomment">/// Helper for [from_config] to match a [LayerType][2] to its [implementation][3].</span>
    <span class="doccomment">/// [1]: #method.from_config</span>
    <span class="doccomment">/// [2]: ./enum.LayerType.html</span>
    <span class="doccomment">/// [3]: ../layers/index.html</span>
    <span class="kw">fn</span> <span class="ident">worker_from_config</span>(<span class="ident">backend</span>: <span class="ident">Rc</span><span class="op">&lt;</span><span class="ident">B</span><span class="op">&gt;</span>, <span class="ident">config</span>: <span class="kw-2">&amp;</span><span class="ident">LayerConfig</span>) -&gt; <span class="ident">Box</span><span class="op">&lt;</span><span class="kw">dyn</span> <span class="ident">ILayer</span><span class="op">&lt;</span><span class="ident">B</span><span class="op">&gt;</span><span class="op">&gt;</span> {
        <span class="kw">match</span> <span class="ident">config</span>.<span class="ident">layer_type</span>.<span class="ident">clone</span>() {
            <span class="ident">LayerType::Convolution</span>(<span class="ident">layer_config</span>) =&gt; <span class="ident">Box::new</span>(<span class="ident">Convolution::from_config</span>(<span class="kw-2">&amp;</span><span class="ident">layer_config</span>)),
            <span class="ident">LayerType::Rnn</span>(<span class="ident">layer_config</span>) =&gt; <span class="ident">Box::new</span>(<span class="ident">Rnn::from_config</span>(<span class="kw-2">&amp;</span><span class="ident">layer_config</span>)),
            <span class="ident">LayerType::Linear</span>(<span class="ident">layer_config</span>) =&gt; <span class="ident">Box::new</span>(<span class="ident">Linear::from_config</span>(<span class="kw-2">&amp;</span><span class="ident">layer_config</span>)),
            <span class="ident">LayerType::LogSoftmax</span> =&gt; <span class="ident">Box::new</span>(<span class="ident">LogSoftmax::default</span>()),
            <span class="ident">LayerType::Pooling</span>(<span class="ident">layer_config</span>) =&gt; <span class="ident">Box::new</span>(<span class="ident">Pooling::from_config</span>(<span class="kw-2">&amp;</span><span class="ident">layer_config</span>)),
            <span class="ident">LayerType::Sequential</span>(<span class="ident">layer_config</span>) =&gt; <span class="ident">Box::new</span>(<span class="ident">Sequential::from_config</span>(<span class="ident">backend</span>, <span class="kw-2">&amp;</span><span class="ident">layer_config</span>)),
            <span class="ident">LayerType::Softmax</span> =&gt; <span class="ident">Box::new</span>(<span class="ident">Softmax::default</span>()),
            <span class="ident">LayerType::ReLU</span> =&gt; <span class="ident">Box::new</span>(<span class="ident">ReLU</span>),
            <span class="ident">LayerType::TanH</span> =&gt; <span class="ident">Box::new</span>(<span class="ident">TanH</span>),
            <span class="ident">LayerType::Sigmoid</span> =&gt; <span class="ident">Box::new</span>(<span class="ident">Sigmoid</span>),
            <span class="ident">LayerType::NegativeLogLikelihood</span>(<span class="ident">layer_config</span>) =&gt; {
                <span class="ident">Box::new</span>(<span class="ident">NegativeLogLikelihood::from_config</span>(<span class="kw-2">&amp;</span><span class="ident">layer_config</span>))
            }
            <span class="ident">LayerType::MeanSquaredError</span> =&gt; <span class="ident">Box::new</span>(<span class="ident">MeanSquaredError</span>),
            <span class="ident">LayerType::Reshape</span>(<span class="ident">layer_config</span>) =&gt; <span class="ident">Box::new</span>(<span class="ident">Reshape::from_config</span>(<span class="kw-2">&amp;</span><span class="ident">layer_config</span>)),
            <span class="ident">LayerType::Dropout</span>(<span class="ident">layer_config</span>) =&gt; <span class="ident">Box::new</span>(<span class="ident">Dropout::from_config</span>(<span class="kw-2">&amp;</span><span class="ident">layer_config</span>)),
        }
    }
}

<span class="doccomment">/// A Layer in a Neural Network that can handle forward and backward of a computation step.</span>
<span class="kw">pub</span> <span class="kw">trait</span> <span class="ident">ILayer</span><span class="op">&lt;</span><span class="ident">B</span>: <span class="ident">IBackend</span><span class="op">&gt;</span>:
    <span class="ident">ComputeOutput</span><span class="op">&lt;</span><span class="ident">f32</span>, <span class="ident">B</span><span class="op">&gt;</span> <span class="op">+</span> <span class="ident">ComputeInputGradient</span><span class="op">&lt;</span><span class="ident">f32</span>, <span class="ident">B</span><span class="op">&gt;</span> <span class="op">+</span> <span class="ident">ComputeParametersGradient</span><span class="op">&lt;</span><span class="ident">f32</span>, <span class="ident">B</span><span class="op">&gt;</span>
{
    <span class="doccomment">/// Initialize the layer for computation.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// Allows for layer-specific one time setup, e.g. precomputing constant values.</span>
    <span class="kw">fn</span> <span class="ident">init</span>(<span class="kw-2">&amp;mut</span> <span class="self">self</span>, <span class="ident">backend</span>: <span class="ident">Rc</span><span class="op">&lt;</span><span class="ident">B</span><span class="op">&gt;</span>) {}

    <span class="doccomment">/// Adjust to shapes of the output blobs to fit the shapes of the input blobs.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// Should be called during Layer initalization, after [init][2].</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// **Caution**: `input_data` should only be reshaped, but not resized.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// [2]: #method.init</span>
    <span class="kw">fn</span> <span class="ident">reshape</span>(
        <span class="kw-2">&amp;mut</span> <span class="self">self</span>,
        <span class="ident">backend</span>: <span class="ident">Rc</span><span class="op">&lt;</span><span class="ident">B</span><span class="op">&gt;</span>,
        <span class="ident">input_data</span>: <span class="kw-2">&amp;mut</span> <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span>,
        <span class="ident">input_gradient</span>: <span class="kw-2">&amp;mut</span> <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span>,
        <span class="ident">weights_data</span>: <span class="kw-2">&amp;mut</span> <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span>,
        <span class="ident">weights_gradient</span>: <span class="kw-2">&amp;mut</span> <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span>,
        <span class="ident">output_data</span>: <span class="kw-2">&amp;mut</span> <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span>,
        <span class="ident">output_gradient</span>: <span class="kw-2">&amp;mut</span> <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span>,
    ) {
    }

    <span class="doccomment">/// Adjust size of shared workspace.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// Is used by layers that need a workspace.</span>
    <span class="doccomment">/// The layer should either:</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// - leave the workspace as is if it bigger than required by this layer</span>
    <span class="doccomment">/// - resize the workspace to the required size if smaller</span>
    <span class="doccomment">/// - create the workspace if the `workspace` is `None`</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// The reference to the workspace should be saved in the layer.</span>
    <span class="kw">fn</span> <span class="ident">resize_shared_workspace</span>(
        <span class="kw-2">&amp;mut</span> <span class="self">self</span>,
        <span class="ident">backend</span>: <span class="ident">Rc</span><span class="op">&lt;</span><span class="ident">B</span><span class="op">&gt;</span>,
        <span class="ident">workspace</span>: <span class="prelude-ty">Option</span><span class="op">&lt;</span><span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">u8</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span>,
    ) -&gt; <span class="prelude-ty">Option</span><span class="op">&lt;</span><span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">u8</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span> {
        <span class="ident">workspace</span>
    }

    <span class="doccomment">/// Compute the [feedforward][1] layer output using the provided Backend.</span>
    <span class="doccomment">/// [1]: https://en.wikipedia.org/wiki/Feedforward_neural_network</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// Aquires read locks for the input tensors</span>
    <span class="doccomment">/// and write locks for the output tensors to ensure sequential computation,</span>
    <span class="doccomment">/// and then passes them to computation method specific function ([forward_cpu][4]).</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// [3]: #method.forward_cpu</span>
    <span class="attribute">#[<span class="ident">cfg_attr</span>(<span class="ident">lint</span>, <span class="ident">allow</span>(<span class="ident">map_clone</span>))]</span>
    <span class="kw">fn</span> <span class="ident">forward</span>(
        <span class="kw-2">&amp;</span><span class="self">self</span>,
        <span class="ident">backend</span>: <span class="kw-2">&amp;</span><span class="ident">B</span>,
        <span class="ident">input_data</span>: <span class="kw-2">&amp;</span>[<span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span>],
        <span class="ident">weights_data</span>: <span class="kw-2">&amp;</span>[<span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span>],
        <span class="ident">output_data</span>: <span class="kw-2">&amp;mut</span> [<span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span>],
    ) {
        <span class="comment">// aquire all the locks</span>
        <span class="kw">let</span> <span class="ident">inp</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="kw">_</span><span class="op">&gt;</span> <span class="op">=</span> <span class="ident">input_data</span>.<span class="ident">iter</span>().<span class="ident">map</span>(<span class="op">|</span><span class="ident">b</span><span class="op">|</span> <span class="ident">b</span>.<span class="ident">read</span>().<span class="ident">unwrap</span>()).<span class="ident">collect</span>();
        <span class="kw">let</span> <span class="ident">input_data_</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="kw-2">&amp;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span> <span class="op">=</span> <span class="ident">inp</span>.<span class="ident">iter</span>().<span class="ident">map</span>(<span class="op">|</span><span class="ident">val</span><span class="op">|</span> <span class="kw-2">&amp;</span><span class="kw-2">*</span><span class="kw-2">*</span><span class="ident">val</span>).<span class="ident">collect</span>();

        <span class="kw">let</span> <span class="ident">wgts</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="kw">_</span><span class="op">&gt;</span> <span class="op">=</span> <span class="ident">weights_data</span>.<span class="ident">iter</span>().<span class="ident">map</span>(<span class="op">|</span><span class="ident">w</span><span class="op">|</span> <span class="ident">w</span>.<span class="ident">read</span>().<span class="ident">unwrap</span>()).<span class="ident">collect</span>();
        <span class="kw">let</span> <span class="ident">weights_data_</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="kw-2">&amp;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span> <span class="op">=</span> <span class="ident">wgts</span>.<span class="ident">iter</span>().<span class="ident">map</span>(<span class="op">|</span><span class="ident">val</span><span class="op">|</span> <span class="kw-2">&amp;</span><span class="kw-2">*</span><span class="kw-2">*</span><span class="ident">val</span>).<span class="ident">collect</span>();

        <span class="kw">let</span> <span class="ident">out_ref</span> <span class="op">=</span> <span class="ident">output_data</span>.<span class="ident">iter</span>().<span class="ident">cloned</span>().<span class="ident">collect</span>::<span class="op">&lt;</span><span class="ident">Vec</span><span class="op">&lt;</span><span class="kw">_</span><span class="op">&gt;</span><span class="op">&gt;</span>();
        <span class="kw">let</span> <span class="ident">out</span> <span class="op">=</span> <span class="kw-2">&amp;mut</span> <span class="ident">out_ref</span>.<span class="ident">iter</span>().<span class="ident">map</span>(<span class="op">|</span><span class="ident">b</span><span class="op">|</span> <span class="ident">b</span>.<span class="ident">write</span>().<span class="ident">unwrap</span>()).<span class="ident">collect</span>::<span class="op">&lt;</span><span class="ident">Vec</span><span class="op">&lt;</span><span class="kw">_</span><span class="op">&gt;</span><span class="op">&gt;</span>();
        <span class="kw">let</span> <span class="ident">output_w</span> <span class="op">=</span> <span class="kw-2">&amp;mut</span> <span class="ident">out</span>.<span class="ident">iter_mut</span>().<span class="ident">map</span>(<span class="op">|</span><span class="ident">a</span><span class="op">|</span> <span class="ident">a</span>).<span class="ident">collect</span>::<span class="op">&lt;</span><span class="ident">Vec</span><span class="op">&lt;</span><span class="kw">_</span><span class="op">&gt;</span><span class="op">&gt;</span>();
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">output_data_</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="kw-2">&amp;mut</span> <span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span> <span class="op">=</span> <span class="ident">output_w</span>.<span class="ident">iter_mut</span>().<span class="ident">map</span>(<span class="op">|</span><span class="ident">val</span><span class="op">|</span> <span class="kw-2">&amp;mut</span> <span class="kw-2">*</span><span class="kw-2">*</span><span class="kw-2">*</span><span class="ident">val</span>).<span class="ident">collect</span>();

        <span class="self">self</span>.<span class="ident">compute_output</span>(<span class="ident">backend</span>, <span class="kw-2">&amp;</span><span class="ident">weights_data_</span>, <span class="kw-2">&amp;</span><span class="ident">input_data_</span>, <span class="kw-2">&amp;mut</span> <span class="ident">output_data_</span>);
    }

    <span class="doccomment">/// Compute the [backpropagation][1] input gradient using the provided backend.</span>
    <span class="doccomment">/// [1]: https://en.wikipedia.org/wiki/Backpropagation</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// Aquires write locks for the input blobs to ensure sequential computation,</span>
    <span class="doccomment">/// and then do a [compute_input_gradient][3].</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// [3]: ./trait.ComputeInputGradient.html#method.compute_input_gradient</span>
    <span class="attribute">#[<span class="ident">cfg_attr</span>(<span class="ident">lint</span>, <span class="ident">allow</span>(<span class="ident">map_clone</span>))]</span>
    <span class="kw">fn</span> <span class="ident">backward_input</span>(
        <span class="kw-2">&amp;</span><span class="self">self</span>,
        <span class="ident">backend</span>: <span class="kw-2">&amp;</span><span class="ident">B</span>,
        <span class="ident">weights_data</span>: <span class="kw-2">&amp;</span>[<span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span>],
        <span class="ident">output_data</span>: <span class="kw-2">&amp;</span>[<span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span>],
        <span class="ident">output_gradients</span>: <span class="kw-2">&amp;</span>[<span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span>],
        <span class="ident">input_data</span>: <span class="kw-2">&amp;</span>[<span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span>],
        <span class="ident">input_gradients</span>: <span class="kw-2">&amp;mut</span> [<span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span>],
    ) {
        <span class="kw">let</span> <span class="ident">wgts_data</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="kw">_</span><span class="op">&gt;</span> <span class="op">=</span> <span class="ident">weights_data</span>.<span class="ident">iter</span>().<span class="ident">map</span>(<span class="op">|</span><span class="ident">b</span><span class="op">|</span> <span class="ident">b</span>.<span class="ident">read</span>().<span class="ident">unwrap</span>()).<span class="ident">collect</span>();
        <span class="kw">let</span> <span class="ident">weights_data_</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="kw-2">&amp;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span> <span class="op">=</span> <span class="ident">wgts_data</span>.<span class="ident">iter</span>().<span class="ident">map</span>(<span class="op">|</span><span class="ident">val</span><span class="op">|</span> <span class="kw-2">&amp;</span><span class="kw-2">*</span><span class="kw-2">*</span><span class="ident">val</span>).<span class="ident">collect</span>();
        <span class="kw">let</span> <span class="ident">out_data</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="kw">_</span><span class="op">&gt;</span> <span class="op">=</span> <span class="ident">output_data</span>.<span class="ident">iter</span>().<span class="ident">map</span>(<span class="op">|</span><span class="ident">b</span><span class="op">|</span> <span class="ident">b</span>.<span class="ident">read</span>().<span class="ident">unwrap</span>()).<span class="ident">collect</span>();
        <span class="kw">let</span> <span class="ident">output_data_</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="kw-2">&amp;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span> <span class="op">=</span> <span class="ident">out_data</span>.<span class="ident">iter</span>().<span class="ident">map</span>(<span class="op">|</span><span class="ident">val</span><span class="op">|</span> <span class="kw-2">&amp;</span><span class="kw-2">*</span><span class="kw-2">*</span><span class="ident">val</span>).<span class="ident">collect</span>();
        <span class="kw">let</span> <span class="ident">out_gradient</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="kw">_</span><span class="op">&gt;</span> <span class="op">=</span> <span class="ident">output_gradients</span>.<span class="ident">iter</span>().<span class="ident">map</span>(<span class="op">|</span><span class="ident">b</span><span class="op">|</span> <span class="ident">b</span>.<span class="ident">read</span>().<span class="ident">unwrap</span>()).<span class="ident">collect</span>();
        <span class="kw">let</span> <span class="ident">output_gradients_</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="kw-2">&amp;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span> <span class="op">=</span> <span class="ident">out_gradient</span>.<span class="ident">iter</span>().<span class="ident">map</span>(<span class="op">|</span><span class="ident">val</span><span class="op">|</span> <span class="kw-2">&amp;</span><span class="kw-2">*</span><span class="kw-2">*</span><span class="ident">val</span>).<span class="ident">collect</span>();
        <span class="kw">let</span> <span class="ident">inp_data</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="kw">_</span><span class="op">&gt;</span> <span class="op">=</span> <span class="ident">input_data</span>.<span class="ident">iter</span>().<span class="ident">map</span>(<span class="op">|</span><span class="ident">b</span><span class="op">|</span> <span class="ident">b</span>.<span class="ident">read</span>().<span class="ident">unwrap</span>()).<span class="ident">collect</span>();
        <span class="kw">let</span> <span class="ident">input_data_</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="kw-2">&amp;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span> <span class="op">=</span> <span class="ident">inp_data</span>.<span class="ident">iter</span>().<span class="ident">map</span>(<span class="op">|</span><span class="ident">val</span><span class="op">|</span> <span class="kw-2">&amp;</span><span class="kw-2">*</span><span class="kw-2">*</span><span class="ident">val</span>).<span class="ident">collect</span>();
        <span class="kw">let</span> <span class="ident">btm_gradient_ref</span> <span class="op">=</span> <span class="ident">input_gradients</span>.<span class="ident">iter</span>().<span class="ident">cloned</span>().<span class="ident">collect</span>::<span class="op">&lt;</span><span class="ident">Vec</span><span class="op">&lt;</span><span class="kw">_</span><span class="op">&gt;</span><span class="op">&gt;</span>();
        <span class="kw">let</span> <span class="ident">btm_gradient</span> <span class="op">=</span> <span class="kw-2">&amp;mut</span> <span class="ident">btm_gradient_ref</span>.<span class="ident">iter</span>().<span class="ident">map</span>(<span class="op">|</span><span class="ident">b</span><span class="op">|</span> <span class="ident">b</span>.<span class="ident">write</span>().<span class="ident">unwrap</span>()).<span class="ident">collect</span>::<span class="op">&lt;</span><span class="ident">Vec</span><span class="op">&lt;</span><span class="kw">_</span><span class="op">&gt;</span><span class="op">&gt;</span>();
        <span class="kw">let</span> <span class="ident">input_gradient</span> <span class="op">=</span> <span class="kw-2">&amp;mut</span> <span class="ident">btm_gradient</span>.<span class="ident">iter_mut</span>().<span class="ident">map</span>(<span class="op">|</span><span class="ident">a</span><span class="op">|</span> <span class="ident">a</span>).<span class="ident">collect</span>::<span class="op">&lt;</span><span class="ident">Vec</span><span class="op">&lt;</span><span class="kw">_</span><span class="op">&gt;</span><span class="op">&gt;</span>();
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">input_gradients_</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="kw-2">&amp;mut</span> <span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span> <span class="op">=</span>
            <span class="ident">input_gradient</span>.<span class="ident">iter_mut</span>().<span class="ident">map</span>(<span class="op">|</span><span class="ident">val</span><span class="op">|</span> <span class="kw-2">&amp;mut</span> <span class="kw-2">*</span><span class="kw-2">*</span><span class="kw-2">*</span><span class="ident">val</span>).<span class="ident">collect</span>();

        <span class="self">self</span>.<span class="ident">compute_input_gradient</span>(
            <span class="ident">backend</span>,
            <span class="kw-2">&amp;</span><span class="ident">weights_data_</span>,
            <span class="kw-2">&amp;</span><span class="ident">output_data_</span>,
            <span class="kw-2">&amp;</span><span class="ident">output_gradients_</span>,
            <span class="kw-2">&amp;</span><span class="ident">input_data_</span>,
            <span class="kw-2">&amp;mut</span> <span class="ident">input_gradients_</span>,
        );
    }

    <span class="doccomment">/// Compute the [backpropagation][1] parameters gradient using the provided backend.</span>
    <span class="doccomment">/// [1]: https://en.wikipedia.org/wiki/Backpropagation</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// Aquires write locks for the input blobs to ensure sequential computation,</span>
    <span class="doccomment">/// and then do a [compute_parameters_gradient][4].</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// [4]: ./trait.ComputeParametersGradient.html#method.compute_parameters_gradient</span>
    <span class="attribute">#[<span class="ident">cfg_attr</span>(<span class="ident">lint</span>, <span class="ident">allow</span>(<span class="ident">map_clone</span>))]</span>
    <span class="kw">fn</span> <span class="ident">backward_parameters</span>(
        <span class="kw-2">&amp;</span><span class="self">self</span>,
        <span class="ident">backend</span>: <span class="kw-2">&amp;</span><span class="ident">B</span>,
        <span class="ident">output_data</span>: <span class="kw-2">&amp;</span>[<span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span>],
        <span class="ident">output_gradients</span>: <span class="kw-2">&amp;</span>[<span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span>],
        <span class="ident">input_data</span>: <span class="kw-2">&amp;</span>[<span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span>],
        <span class="ident">weights_gradients</span>: <span class="kw-2">&amp;mut</span> [<span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span>],
    ) {
        <span class="kw">let</span> <span class="ident">out_data</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="kw">_</span><span class="op">&gt;</span> <span class="op">=</span> <span class="ident">output_data</span>.<span class="ident">iter</span>().<span class="ident">map</span>(<span class="op">|</span><span class="ident">b</span><span class="op">|</span> <span class="ident">b</span>.<span class="ident">read</span>().<span class="ident">unwrap</span>()).<span class="ident">collect</span>();
        <span class="kw">let</span> <span class="ident">output_data_</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="kw-2">&amp;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span> <span class="op">=</span> <span class="ident">out_data</span>.<span class="ident">iter</span>().<span class="ident">map</span>(<span class="op">|</span><span class="ident">val</span><span class="op">|</span> <span class="kw-2">&amp;</span><span class="kw-2">*</span><span class="kw-2">*</span><span class="ident">val</span>).<span class="ident">collect</span>();
        <span class="kw">let</span> <span class="ident">out_gradients</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="kw">_</span><span class="op">&gt;</span> <span class="op">=</span> <span class="ident">output_gradients</span>.<span class="ident">iter</span>().<span class="ident">map</span>(<span class="op">|</span><span class="ident">b</span><span class="op">|</span> <span class="ident">b</span>.<span class="ident">read</span>().<span class="ident">unwrap</span>()).<span class="ident">collect</span>();
        <span class="kw">let</span> <span class="ident">output_gradients_</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="kw-2">&amp;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span> <span class="op">=</span> <span class="ident">out_gradients</span>.<span class="ident">iter</span>().<span class="ident">map</span>(<span class="op">|</span><span class="ident">val</span><span class="op">|</span> <span class="kw-2">&amp;</span><span class="kw-2">*</span><span class="kw-2">*</span><span class="ident">val</span>).<span class="ident">collect</span>();
        <span class="kw">let</span> <span class="ident">inp_data</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="kw">_</span><span class="op">&gt;</span> <span class="op">=</span> <span class="ident">input_data</span>.<span class="ident">iter</span>().<span class="ident">map</span>(<span class="op">|</span><span class="ident">b</span><span class="op">|</span> <span class="ident">b</span>.<span class="ident">read</span>().<span class="ident">unwrap</span>()).<span class="ident">collect</span>();
        <span class="kw">let</span> <span class="ident">input_data_</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="kw-2">&amp;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span> <span class="op">=</span> <span class="ident">inp_data</span>.<span class="ident">iter</span>().<span class="ident">map</span>(<span class="op">|</span><span class="ident">val</span><span class="op">|</span> <span class="kw-2">&amp;</span><span class="kw-2">*</span><span class="kw-2">*</span><span class="ident">val</span>).<span class="ident">collect</span>();
        <span class="kw">let</span> <span class="ident">wgt_gradient_ref</span> <span class="op">=</span> <span class="ident">weights_gradients</span>.<span class="ident">iter</span>().<span class="ident">cloned</span>().<span class="ident">collect</span>::<span class="op">&lt;</span><span class="ident">Vec</span><span class="op">&lt;</span><span class="kw">_</span><span class="op">&gt;</span><span class="op">&gt;</span>();
        <span class="kw">let</span> <span class="ident">wgt_gradient</span> <span class="op">=</span> <span class="kw-2">&amp;mut</span> <span class="ident">wgt_gradient_ref</span>.<span class="ident">iter</span>().<span class="ident">map</span>(<span class="op">|</span><span class="ident">b</span><span class="op">|</span> <span class="ident">b</span>.<span class="ident">write</span>().<span class="ident">unwrap</span>()).<span class="ident">collect</span>::<span class="op">&lt;</span><span class="ident">Vec</span><span class="op">&lt;</span><span class="kw">_</span><span class="op">&gt;</span><span class="op">&gt;</span>();
        <span class="kw">let</span> <span class="ident">weights_gradient</span> <span class="op">=</span> <span class="kw-2">&amp;mut</span> <span class="ident">wgt_gradient</span>.<span class="ident">iter_mut</span>().<span class="ident">map</span>(<span class="op">|</span><span class="ident">a</span><span class="op">|</span> <span class="ident">a</span>).<span class="ident">collect</span>::<span class="op">&lt;</span><span class="ident">Vec</span><span class="op">&lt;</span><span class="kw">_</span><span class="op">&gt;</span><span class="op">&gt;</span>();
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">weights_gradients_</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="kw-2">&amp;mut</span> <span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span> <span class="op">=</span>
            <span class="ident">weights_gradient</span>.<span class="ident">iter_mut</span>().<span class="ident">map</span>(<span class="op">|</span><span class="ident">val</span><span class="op">|</span> <span class="kw-2">&amp;mut</span> <span class="kw-2">*</span><span class="kw-2">*</span><span class="kw-2">*</span><span class="ident">val</span>).<span class="ident">collect</span>();

        <span class="self">self</span>.<span class="ident">compute_parameters_gradient</span>(
            <span class="ident">backend</span>,
            <span class="kw-2">&amp;</span><span class="ident">output_data_</span>,
            <span class="kw-2">&amp;</span><span class="ident">output_gradients_</span>,
            <span class="kw-2">&amp;</span><span class="ident">input_data_</span>,
            <span class="kw-2">&amp;mut</span> <span class="ident">weights_gradients_</span>,
        );
    }

    <span class="doccomment">/// Return whether &quot;anonymous&quot; output blobs are created automatically for the layer.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// If this method returns true, Network::init will create enough &quot;anonymous&quot; output</span>
    <span class="doccomment">/// blobs to fulfill the requirement specified by [exact_num_output_blobs][1] or</span>
    <span class="doccomment">/// [min_output_blobs][2].</span>
    <span class="doccomment">/// [1]: #method.exact_num_output_blobs</span>
    <span class="doccomment">/// [2]: #method.min_output_blobs</span>
    <span class="kw">fn</span> <span class="ident">auto_output_blobs</span>(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; <span class="ident">bool</span> {
        <span class="bool-val">false</span>
    }
    <span class="doccomment">/// Returns the minimum number of output blobs required by the layer,</span>
    <span class="doccomment">/// or 0 if no minimum number is required.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// This method should be overridden to return a positive value if your</span>
    <span class="doccomment">/// layer expects some minimum number of output blobs.</span>
    <span class="kw">fn</span> <span class="ident">min_output_blobs</span>(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; <span class="ident">usize</span> {
        <span class="number">0</span>
    }
    <span class="doccomment">/// Returns the exact number of output blobs required by the layer,</span>
    <span class="doccomment">/// or `None` if no exact number is required.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// This method should be overridden to return a positive value if your</span>
    <span class="doccomment">/// layer expects some exact number of output blobs.</span>
    <span class="kw">fn</span> <span class="ident">exact_num_output_blobs</span>(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; <span class="prelude-ty">Option</span><span class="op">&lt;</span><span class="ident">usize</span><span class="op">&gt;</span> {
        <span class="prelude-val">None</span>
    }
    <span class="doccomment">/// Return whether weight blobs are created automatically for the layer.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// If this method returns true, Network::init will create a weight blob</span>
    <span class="doccomment">/// for every output blob.</span>
    <span class="kw">fn</span> <span class="ident">auto_weight_blobs</span>(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; <span class="ident">bool</span> {
        <span class="bool-val">false</span>
    }
    <span class="doccomment">/// Returns the exact number of input blobs required by the layer,</span>
    <span class="doccomment">/// or `None` if no exact number is required.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// This method should be overridden to return a positive value if your</span>
    <span class="doccomment">/// layer expects some exact number of input blobs.</span>
    <span class="kw">fn</span> <span class="ident">exact_num_input_blobs</span>(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; <span class="prelude-ty">Option</span><span class="op">&lt;</span><span class="ident">usize</span><span class="op">&gt;</span> {
        <span class="prelude-val">None</span>
    }
    <span class="doccomment">/// Return whether to allow force_backward for a given input blob index.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// If allow_force_backward(i) == false, we will ignore the force_backward</span>
    <span class="doccomment">/// setting and backpropagate to blob i only if it needs gradient information</span>
    <span class="doccomment">/// (as is done when force_backward == false).</span>
    <span class="kw">fn</span> <span class="ident">allow_force_backward</span>(<span class="kw-2">&amp;</span><span class="self">self</span>, <span class="ident">input_id</span>: <span class="ident">usize</span>) -&gt; <span class="ident">bool</span> {
        <span class="bool-val">true</span>
    }
    <span class="doccomment">/// Return wether a simple native backend should be used to [sync][1] instead of the default backend.</span>
    <span class="doccomment">/// [1]: #method.sync</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// If `false` is returned the default backend will be used, otherwise a new native backend</span>
    <span class="doccomment">/// will be created and provided as argument to `sync`.</span>
    <span class="kw">fn</span> <span class="ident">sync_native</span>(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; <span class="ident">bool</span> {
        <span class="bool-val">false</span>
    }
    <span class="doccomment">/// Return wether the computations of a layer should be done in-place (the output will be written where the input was read from).</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// Doing computations in place reduces the memory required for layers.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// If `false` is returned the layer behaves as normal, otherwise</span>
    <span class="doccomment">/// if a layer is provided a identiacla &quot;input&quot; and &quot;output&quot;, it will only be supplied an</span>
    <span class="doccomment">/// &quot;output_data&quot; when doing a `compute_output`.</span>
    <span class="kw">fn</span> <span class="ident">compute_in_place</span>(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; <span class="ident">bool</span> {
        <span class="bool-val">false</span>
    }

    <span class="doccomment">/// Return wether the layer is a container.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// This turns of certain behaviour for containers which would lead to problems:</span>
    <span class="doccomment">/// - RwLocks will not be aquired for forward/backward since it would lead to deadlocks.</span>
    <span class="kw">fn</span> <span class="ident">is_container</span>(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; <span class="ident">bool</span> {
        <span class="bool-val">false</span>
    }

    <span class="doccomment">/// Return the associated loss weight for a given output blob index.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// If loss_weight(i) == `None`, no loss will be calculated for the output blob.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// This is usually overridden by loss layers.</span>
    <span class="kw">fn</span> <span class="ident">loss_weight</span>(<span class="kw-2">&amp;</span><span class="self">self</span>, <span class="ident">output_id</span>: <span class="ident">usize</span>) -&gt; <span class="prelude-ty">Option</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span> {
        <span class="prelude-val">None</span>
    }

    <span class="doccomment">/// Return the input tensors of the layer.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// This should only be overridden by container layers,</span>
    <span class="doccomment">/// where the tensors are not easily exposable.</span>
    <span class="kw">fn</span> <span class="ident">inputs_data</span>(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; <span class="prelude-ty">Option</span><span class="op">&lt;</span><span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span> {
        <span class="prelude-val">None</span>
    }

    <span class="doccomment">/// Return the gradients of the input tensors of the layer.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// This should only be overridden by container layers,</span>
    <span class="doccomment">/// where the tensors are not easily exposable.</span>
    <span class="kw">fn</span> <span class="ident">inputs_gradients</span>(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; <span class="prelude-ty">Option</span><span class="op">&lt;</span><span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span> {
        <span class="prelude-val">None</span>
    }

    <span class="doccomment">/// Return the output tensors of the layer.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// This should only be overridden by container layers,</span>
    <span class="doccomment">/// where the tensors are not easily exposable.</span>
    <span class="kw">fn</span> <span class="ident">outputs_data</span>(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; <span class="prelude-ty">Option</span><span class="op">&lt;</span><span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span> {
        <span class="prelude-val">None</span>
    }

    <span class="doccomment">/// Return the gradients of the output tensors of the layer.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// This should only be overridden by container layers,</span>
    <span class="doccomment">/// where the tensors are not easily exposable.</span>
    <span class="kw">fn</span> <span class="ident">outputs_gradients</span>(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; <span class="prelude-ty">Option</span><span class="op">&lt;</span><span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span> {
        <span class="prelude-val">None</span>
    }

    <span class="doccomment">/// Return the learnable weights inside the layer.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// This should only be overridden by container layers,</span>
    <span class="doccomment">/// where the weights are not easily exposable.</span>
    <span class="kw">fn</span> <span class="ident">learnable_weights</span>(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; <span class="prelude-ty">Option</span><span class="op">&lt;</span><span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span> {
        <span class="prelude-val">None</span>
    }

    <span class="doccomment">/// Return the gradients for the learnable weights inside the layer.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// This should only be overridden by container layers,</span>
    <span class="doccomment">/// where the weights are not easily exposable.</span>
    <span class="kw">fn</span> <span class="ident">learnable_weights_gradients</span>(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; <span class="prelude-ty">Option</span><span class="op">&lt;</span><span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span> {
        <span class="prelude-val">None</span>
    }

    <span class="doccomment">/// Return the names of the learnable weights inside the layer.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// This should only be overridden by container layers,</span>
    <span class="doccomment">/// where the weights are not easily exposable.</span>
    <span class="kw">fn</span> <span class="ident">learnable_weights_names</span>(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; <span class="prelude-ty">Option</span><span class="op">&lt;</span><span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">String</span><span class="op">&gt;</span><span class="op">&gt;</span> {
        <span class="prelude-val">None</span>
    }

    <span class="doccomment">/// Return the learning rates for the learnable weights inside the layer.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// This should only be overridden by container layers,</span>
    <span class="doccomment">/// where the weights are not easily exposable.</span>
    <span class="kw">fn</span> <span class="ident">learnable_weights_lr</span>(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; <span class="prelude-ty">Option</span><span class="op">&lt;</span><span class="ident">Vec</span><span class="op">&lt;</span><span class="prelude-ty">Option</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span> {
        <span class="prelude-val">None</span>
    }
}

<span class="doccomment">/// A Layer that can compute the output for a given input.</span>
<span class="kw">pub</span> <span class="kw">trait</span> <span class="ident">ComputeOutput</span><span class="op">&lt;</span><span class="ident">T</span>, <span class="ident">B</span>: <span class="ident">IBackend</span><span class="op">&gt;</span> {
    <span class="doccomment">/// Compute output for given input and write them into `output_data`.</span>
    <span class="kw">fn</span> <span class="ident">compute_output</span>(
        <span class="kw-2">&amp;</span><span class="self">self</span>,
        <span class="ident">backend</span>: <span class="kw-2">&amp;</span><span class="ident">B</span>,
        <span class="ident">weights_data</span>: <span class="kw-2">&amp;</span>[<span class="kw-2">&amp;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">T</span><span class="op">&gt;</span>],
        <span class="ident">input_data</span>: <span class="kw-2">&amp;</span>[<span class="kw-2">&amp;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">T</span><span class="op">&gt;</span>],
        <span class="ident">output_data</span>: <span class="kw-2">&amp;mut</span> [<span class="kw-2">&amp;mut</span> <span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">T</span><span class="op">&gt;</span>],
    );
}

<span class="doccomment">/// A Layer that can compute the gradient with respect to its input.</span>
<span class="kw">pub</span> <span class="kw">trait</span> <span class="ident">ComputeInputGradient</span><span class="op">&lt;</span><span class="ident">T</span>, <span class="ident">B</span>: <span class="ident">IBackend</span><span class="op">&gt;</span> {
    <span class="doccomment">/// Compute gradients with respect to the inputs and write them into `input_gradients`.</span>
    <span class="kw">fn</span> <span class="ident">compute_input_gradient</span>(
        <span class="kw-2">&amp;</span><span class="self">self</span>,
        <span class="ident">backend</span>: <span class="kw-2">&amp;</span><span class="ident">B</span>,
        <span class="ident">weights_data</span>: <span class="kw-2">&amp;</span>[<span class="kw-2">&amp;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">T</span><span class="op">&gt;</span>],
        <span class="ident">output_data</span>: <span class="kw-2">&amp;</span>[<span class="kw-2">&amp;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">T</span><span class="op">&gt;</span>],
        <span class="ident">output_gradients</span>: <span class="kw-2">&amp;</span>[<span class="kw-2">&amp;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">T</span><span class="op">&gt;</span>],
        <span class="ident">input_data</span>: <span class="kw-2">&amp;</span>[<span class="kw-2">&amp;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">T</span><span class="op">&gt;</span>],
        <span class="ident">input_gradients</span>: <span class="kw-2">&amp;mut</span> [<span class="kw-2">&amp;mut</span> <span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">T</span><span class="op">&gt;</span>],
    );
}

<span class="doccomment">/// A Layer that can compute the gradient with respect to its parameters (= weights, bias, etc.).</span>
<span class="kw">pub</span> <span class="kw">trait</span> <span class="ident">ComputeParametersGradient</span><span class="op">&lt;</span><span class="ident">T</span>, <span class="ident">B</span>: <span class="ident">IBackend</span><span class="op">&gt;</span> {
    <span class="doccomment">/// Compute gradients with respect to the parameters and write them into `parameters_gradients`.</span>
    <span class="kw">fn</span> <span class="ident">compute_parameters_gradient</span>(
        <span class="kw-2">&amp;</span><span class="self">self</span>,
        <span class="ident">backend</span>: <span class="kw-2">&amp;</span><span class="ident">B</span>,
        <span class="ident">output_data</span>: <span class="kw-2">&amp;</span>[<span class="kw-2">&amp;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">T</span><span class="op">&gt;</span>],
        <span class="ident">output_gradients</span>: <span class="kw-2">&amp;</span>[<span class="kw-2">&amp;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">T</span><span class="op">&gt;</span>],
        <span class="ident">input_data</span>: <span class="kw-2">&amp;</span>[<span class="kw-2">&amp;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">T</span><span class="op">&gt;</span>],
        <span class="ident">parameters_gradients</span>: <span class="kw-2">&amp;mut</span> [<span class="kw-2">&amp;mut</span> <span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">T</span><span class="op">&gt;</span>],
    ) {
    }
}

<span class="kw">impl</span><span class="op">&lt;</span><span class="ident">B</span>: <span class="ident">IBackend</span><span class="op">&gt;</span> <span class="ident">fmt::Debug</span> <span class="kw">for</span> <span class="kw">dyn</span> <span class="ident">ILayer</span><span class="op">&lt;</span><span class="ident">B</span><span class="op">&gt;</span> {
    <span class="kw">fn</span> <span class="ident">fmt</span>(<span class="kw-2">&amp;</span><span class="self">self</span>, <span class="ident">f</span>: <span class="kw-2">&amp;mut</span> <span class="ident">fmt::Formatter</span>) -&gt; <span class="ident">fmt::Result</span> {
        <span class="macro">write!</span>(<span class="ident">f</span>, <span class="string">&quot;({})&quot;</span>, <span class="string">&quot;ILayer&quot;</span>)
    }
}

<span class="attribute">#[<span class="ident">derive</span>(<span class="ident">Debug</span>, <span class="ident">Clone</span>)]</span>
<span class="doccomment">/// Layer Configuration Struct</span>
<span class="kw">pub</span> <span class="kw">struct</span> <span class="ident">LayerConfig</span> {
    <span class="doccomment">/// The name of the Layer</span>
    <span class="kw">pub</span> <span class="ident">name</span>: <span class="ident">String</span>,

    <span class="doccomment">/// The type of the Layer</span>
    <span class="kw">pub</span> <span class="ident">layer_type</span>: <span class="ident">LayerType</span>,

    <span class="doccomment">/// The name for each output Blob</span>
    <span class="kw">pub</span> <span class="ident">outputs</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">String</span><span class="op">&gt;</span>,

    <span class="doccomment">/// The name for each input Blob</span>
    <span class="kw">pub</span> <span class="ident">inputs</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">String</span><span class="op">&gt;</span>,

    <span class="doccomment">/// Specifies training configuration for each weight blob.</span>
    <span class="kw">pub</span> <span class="ident">params</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">WeightConfig</span><span class="op">&gt;</span>,

    <span class="doccomment">/// Specifies on which inputs the backpropagation should be skipped.</span>
    <span class="doccomment">/// The size must be either 0 or equal to the number of inputs.</span>
    <span class="kw">pub</span> <span class="ident">propagate_down</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">bool</span><span class="op">&gt;</span>,
}

<span class="attribute">#[<span class="ident">derive</span>(<span class="ident">Debug</span>, <span class="ident">Clone</span>)]</span>
<span class="doccomment">/// The Layer Types</span>
<span class="kw">pub</span> <span class="kw">enum</span> <span class="ident">LayerType</span> {
    <span class="comment">// Common layers</span>
    <span class="doccomment">/// Convolution Layer</span>
    <span class="ident">Convolution</span>(<span class="ident">ConvolutionConfig</span>),
    <span class="doccomment">/// RNN Layer</span>
    <span class="ident">Rnn</span>(<span class="ident">RnnConfig</span>),
    <span class="doccomment">/// Linear Layer</span>
    <span class="ident">Linear</span>(<span class="ident">LinearConfig</span>),
    <span class="doccomment">/// LogSoftmax Layer</span>
    <span class="ident">LogSoftmax</span>,
    <span class="doccomment">/// Pooling Layer</span>
    <span class="ident">Pooling</span>(<span class="ident">PoolingConfig</span>),
    <span class="doccomment">/// Sequential Layer</span>
    <span class="ident">Sequential</span>(<span class="ident">SequentialConfig</span>),
    <span class="doccomment">/// Softmax Layer</span>
    <span class="ident">Softmax</span>,
    <span class="doccomment">/// Dropout</span>
    <span class="ident">Dropout</span>(<span class="ident">DropoutConfig</span>),
    <span class="comment">// Activation layers</span>
    <span class="doccomment">/// ReLU Layer</span>
    <span class="ident">ReLU</span>,
    <span class="doccomment">/// TanH Layer</span>
    <span class="ident">TanH</span>,
    <span class="doccomment">/// Sigmoid Layer</span>
    <span class="ident">Sigmoid</span>,
    <span class="comment">// Loss layers</span>
    <span class="doccomment">/// NegativeLogLikelihood Layer</span>
    <span class="ident">NegativeLogLikelihood</span>(<span class="ident">NegativeLogLikelihoodConfig</span>),
    <span class="doccomment">/// MeanSquaredError Layer</span>
    <span class="ident">MeanSquaredError</span>,
    <span class="comment">// Utility layers</span>
    <span class="doccomment">/// Reshape Layer</span>
    <span class="ident">Reshape</span>(<span class="ident">ReshapeConfig</span>),
}

<span class="comment">// TODO get rid of this, each implementation has to state if this</span>
<span class="comment">// TODO an in place operation or not, this thing here makes no sense whatsoever</span>
<span class="kw">impl</span> <span class="ident">LayerType</span> {
    <span class="doccomment">/// Returns wether the LayerType supports in-place operations.</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">supports_in_place</span>(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; <span class="ident">bool</span> {
        <span class="kw">match</span> <span class="kw-2">*</span><span class="self">self</span> {
            <span class="ident">LayerType::Linear</span>(<span class="kw">_</span>) =&gt; <span class="bool-val">false</span>,
            <span class="ident">LayerType::LogSoftmax</span> =&gt; <span class="bool-val">false</span>,
            <span class="ident">LayerType::Sequential</span>(<span class="kw">_</span>) =&gt; <span class="bool-val">false</span>,
            <span class="ident">LayerType::Softmax</span> =&gt; <span class="bool-val">false</span>,
            <span class="ident">LayerType::ReLU</span> =&gt; <span class="bool-val">true</span>,
            <span class="ident">LayerType::TanH</span> =&gt; <span class="bool-val">true</span>,
            <span class="ident">LayerType::Sigmoid</span> =&gt; <span class="bool-val">true</span>,
            <span class="ident">LayerType::NegativeLogLikelihood</span>(<span class="kw">_</span>) =&gt; <span class="bool-val">false</span>,
            <span class="ident">LayerType::MeanSquaredError</span> =&gt; <span class="bool-val">false</span>,
            <span class="ident">LayerType::Reshape</span>(<span class="kw">_</span>) =&gt; <span class="bool-val">true</span>,
            <span class="ident">LayerType::Convolution</span>(<span class="kw">_</span>) =&gt; <span class="bool-val">false</span>,
            <span class="ident">LayerType::Rnn</span>(<span class="kw">_</span>) =&gt; <span class="bool-val">false</span>,
            <span class="ident">LayerType::Pooling</span>(<span class="kw">_</span>) =&gt; <span class="bool-val">false</span>,
            <span class="ident">LayerType::Dropout</span>(<span class="kw">_</span>) =&gt; <span class="bool-val">false</span>,
        }
    }
}

<span class="kw">impl</span><span class="op">&lt;</span><span class="lifetime">&#39;a</span><span class="op">&gt;</span> <span class="ident">CapnpWrite</span><span class="op">&lt;</span><span class="lifetime">&#39;a</span><span class="op">&gt;</span> <span class="kw">for</span> <span class="ident">LayerType</span> {
    <span class="kw">type</span> <span class="ident">Builder</span> <span class="op">=</span> <span class="ident">capnp_layer_type::Builder</span><span class="op">&lt;</span><span class="lifetime">&#39;a</span><span class="op">&gt;</span>;

    <span class="doccomment">/// Write the LayerType into a capnp message.</span>
    <span class="kw">fn</span> <span class="ident">write_capnp</span>(<span class="kw-2">&amp;</span><span class="self">self</span>, <span class="ident">builder</span>: <span class="kw-2">&amp;mut</span> <span class="ident"><span class="self">Self</span>::Builder</span>) {
        <span class="kw">match</span> <span class="self">self</span> {
            <span class="kw-2">&amp;</span><span class="ident">LayerType::Linear</span>(<span class="kw-2">ref</span> <span class="ident">cfg</span>) =&gt; {
                <span class="kw">let</span> <span class="kw-2">ref</span> <span class="kw-2">mut</span> <span class="ident">config</span> <span class="op">=</span> <span class="ident">builder</span>.<span class="ident">reborrow</span>().<span class="ident">init_linear</span>();
                <span class="ident">cfg</span>.<span class="ident">write_capnp</span>(<span class="ident">config</span>);
            }
            <span class="kw-2">&amp;</span><span class="ident">LayerType::LogSoftmax</span> =&gt; <span class="ident">builder</span>.<span class="ident">set_log_softmax</span>(()),
            <span class="kw-2">&amp;</span><span class="ident">LayerType::Sequential</span>(<span class="kw-2">ref</span> <span class="ident">cfg</span>) =&gt; {
                <span class="kw">let</span> <span class="kw-2">ref</span> <span class="kw-2">mut</span> <span class="ident">config</span> <span class="op">=</span> <span class="ident">builder</span>.<span class="ident">reborrow</span>().<span class="ident">init_sequential</span>();
                <span class="ident">cfg</span>.<span class="ident">write_capnp</span>(<span class="ident">config</span>);
            }
            <span class="kw-2">&amp;</span><span class="ident">LayerType::Softmax</span> =&gt; <span class="ident">builder</span>.<span class="ident">set_softmax</span>(()),
            <span class="kw-2">&amp;</span><span class="ident">LayerType::ReLU</span> =&gt; <span class="ident">builder</span>.<span class="ident">set_relu</span>(()),
            <span class="kw-2">&amp;</span><span class="ident">LayerType::TanH</span> =&gt; <span class="ident">builder</span>.<span class="ident">set_tanh</span>(()),
            <span class="kw-2">&amp;</span><span class="ident">LayerType::Sigmoid</span> =&gt; <span class="ident">builder</span>.<span class="ident">set_sigmoid</span>(()),
            <span class="kw-2">&amp;</span><span class="ident">LayerType::NegativeLogLikelihood</span>(<span class="kw-2">ref</span> <span class="ident">cfg</span>) =&gt; {
                <span class="kw">let</span> <span class="kw-2">ref</span> <span class="kw-2">mut</span> <span class="ident">config</span> <span class="op">=</span> <span class="ident">builder</span>.<span class="ident">reborrow</span>().<span class="ident">init_negative_log_likelihood</span>();
                <span class="ident">cfg</span>.<span class="ident">write_capnp</span>(<span class="ident">config</span>);
            }
            <span class="kw-2">&amp;</span><span class="ident">LayerType::MeanSquaredError</span> =&gt; <span class="ident">builder</span>.<span class="ident">set_mean_squared_error</span>(()),
            <span class="kw-2">&amp;</span><span class="ident">LayerType::Reshape</span>(<span class="kw-2">ref</span> <span class="ident">cfg</span>) =&gt; {
                <span class="kw">let</span> <span class="kw-2">ref</span> <span class="kw-2">mut</span> <span class="ident">config</span> <span class="op">=</span> <span class="ident">builder</span>.<span class="ident">reborrow</span>().<span class="ident">init_reshape</span>();
                <span class="ident">cfg</span>.<span class="ident">write_capnp</span>(<span class="ident">config</span>);
            }
            <span class="kw-2">&amp;</span><span class="ident">LayerType::Convolution</span>(<span class="kw-2">ref</span> <span class="ident">cfg</span>) =&gt; {
                <span class="kw">let</span> <span class="kw-2">ref</span> <span class="kw-2">mut</span> <span class="ident">config</span> <span class="op">=</span> <span class="ident">builder</span>.<span class="ident">reborrow</span>().<span class="ident">init_convolution</span>();
                <span class="ident">cfg</span>.<span class="ident">write_capnp</span>(<span class="ident">config</span>);
            }
            <span class="kw-2">&amp;</span><span class="ident">LayerType::Rnn</span>(<span class="kw-2">ref</span> <span class="ident">cfg</span>) =&gt; {
                <span class="kw">let</span> <span class="kw-2">ref</span> <span class="kw-2">mut</span> <span class="ident">config</span> <span class="op">=</span> <span class="ident">builder</span>.<span class="ident">reborrow</span>().<span class="ident">init_rnn</span>();
                <span class="ident">cfg</span>.<span class="ident">write_capnp</span>(<span class="ident">config</span>);
            }
            <span class="kw-2">&amp;</span><span class="ident">LayerType::Pooling</span>(<span class="kw-2">ref</span> <span class="ident">cfg</span>) =&gt; {
                <span class="kw">let</span> <span class="kw-2">ref</span> <span class="kw-2">mut</span> <span class="ident">config</span> <span class="op">=</span> <span class="ident">builder</span>.<span class="ident">reborrow</span>().<span class="ident">init_pooling</span>();
                <span class="ident">cfg</span>.<span class="ident">write_capnp</span>(<span class="ident">config</span>);
            }
            <span class="kw-2">&amp;</span><span class="ident">LayerType::Dropout</span>(<span class="kw-2">ref</span> <span class="ident">cfg</span>) =&gt; {
                <span class="kw">let</span> <span class="kw-2">ref</span> <span class="kw-2">mut</span> <span class="ident">config</span> <span class="op">=</span> <span class="ident">builder</span>.<span class="ident">reborrow</span>().<span class="ident">init_dropout</span>();
                <span class="ident">cfg</span>.<span class="ident">write_capnp</span>(<span class="ident">config</span>);
            }
        }
    }
}

<span class="kw">impl</span><span class="op">&lt;</span><span class="lifetime">&#39;a</span><span class="op">&gt;</span> <span class="ident">CapnpRead</span><span class="op">&lt;</span><span class="lifetime">&#39;a</span><span class="op">&gt;</span> <span class="kw">for</span> <span class="ident">LayerType</span> {
    <span class="kw">type</span> <span class="ident">Reader</span> <span class="op">=</span> <span class="ident">capnp_layer_type::Reader</span><span class="op">&lt;</span><span class="lifetime">&#39;a</span><span class="op">&gt;</span>;

    <span class="kw">fn</span> <span class="ident">read_capnp</span>(<span class="ident">reader</span>: <span class="ident"><span class="self">Self</span>::Reader</span>) -&gt; <span class="self">Self</span> {
        <span class="kw">match</span> <span class="ident">reader</span>.<span class="ident">which</span>().<span class="ident">unwrap</span>() {
            <span class="ident">capnp_layer_type::Which::Linear</span>(<span class="ident">read_config</span>) =&gt; {
                <span class="kw">let</span> <span class="ident">config</span> <span class="op">=</span> <span class="ident">LinearConfig::read_capnp</span>(<span class="ident">read_config</span>.<span class="ident">unwrap</span>());
                <span class="ident">LayerType::Linear</span>(<span class="ident">config</span>)
            }
            <span class="ident">capnp_layer_type::Which::LogSoftmax</span>(<span class="ident">read_config</span>) =&gt; <span class="ident">LayerType::LogSoftmax</span>,
            <span class="ident">capnp_layer_type::Which::Sequential</span>(<span class="ident">read_config</span>) =&gt; {
                <span class="kw">let</span> <span class="ident">config</span> <span class="op">=</span> <span class="ident">SequentialConfig::read_capnp</span>(<span class="ident">read_config</span>.<span class="ident">unwrap</span>());
                <span class="ident">LayerType::Sequential</span>(<span class="ident">config</span>)
            }
            <span class="ident">capnp_layer_type::Which::Softmax</span>(<span class="kw">_</span>) =&gt; <span class="ident">LayerType::Softmax</span>,
            <span class="ident">capnp_layer_type::Which::Relu</span>(<span class="kw">_</span>) =&gt; <span class="ident">LayerType::ReLU</span>,
            <span class="ident">capnp_layer_type::Which::Tanh</span>(<span class="kw">_</span>) =&gt; <span class="ident">LayerType::TanH</span>,
            <span class="ident">capnp_layer_type::Which::Sigmoid</span>(<span class="kw">_</span>) =&gt; <span class="ident">LayerType::Sigmoid</span>,
            <span class="ident">capnp_layer_type::Which::NegativeLogLikelihood</span>(<span class="ident">read_config</span>) =&gt; {
                <span class="kw">let</span> <span class="ident">config</span> <span class="op">=</span> <span class="ident">NegativeLogLikelihoodConfig::read_capnp</span>(<span class="ident">read_config</span>.<span class="ident">unwrap</span>());
                <span class="ident">LayerType::NegativeLogLikelihood</span>(<span class="ident">config</span>)
            }
            <span class="ident">capnp_layer_type::Which::MeanSquaredError</span>(<span class="kw">_</span>) =&gt; <span class="ident">LayerType::MeanSquaredError</span>,
            <span class="ident">capnp_layer_type::Which::Reshape</span>(<span class="ident">read_config</span>) =&gt; {
                <span class="kw">let</span> <span class="ident">config</span> <span class="op">=</span> <span class="ident">ReshapeConfig::read_capnp</span>(<span class="ident">read_config</span>.<span class="ident">unwrap</span>());
                <span class="ident">LayerType::Reshape</span>(<span class="ident">config</span>)
            }
            <span class="ident">capnp_layer_type::Which::Pooling</span>(<span class="ident">read_config</span>) =&gt; {
                <span class="kw">let</span> <span class="ident">config</span> <span class="op">=</span> <span class="ident">PoolingConfig::read_capnp</span>(<span class="ident">read_config</span>.<span class="ident">unwrap</span>());
                <span class="ident">LayerType::Pooling</span>(<span class="ident">config</span>)
            }
            <span class="ident">capnp_layer_type::Which::Convolution</span>(<span class="ident">read_config</span>) =&gt; {
                <span class="kw">let</span> <span class="ident">config</span> <span class="op">=</span> <span class="ident">ConvolutionConfig::read_capnp</span>(<span class="ident">read_config</span>.<span class="ident">unwrap</span>());
                <span class="ident">LayerType::Convolution</span>(<span class="ident">config</span>)
            }
            <span class="ident">capnp_layer_type::Which::Rnn</span>(<span class="ident">read_config</span>) =&gt; {
                <span class="kw">let</span> <span class="ident">config</span> <span class="op">=</span> <span class="ident">RnnConfig::read_capnp</span>(<span class="ident">read_config</span>.<span class="ident">unwrap</span>());
                <span class="ident">LayerType::Rnn</span>(<span class="ident">config</span>)
            }
            <span class="ident">capnp_layer_type::Which::Dropout</span>(<span class="ident">read_config</span>) =&gt; {
                <span class="kw">let</span> <span class="ident">config</span> <span class="op">=</span> <span class="ident">DropoutConfig::read_capnp</span>(<span class="ident">read_config</span>.<span class="ident">unwrap</span>());
                <span class="ident">LayerType::Dropout</span>(<span class="ident">config</span>)
            }
        }
    }
}

<span class="kw">impl</span> <span class="ident">LayerConfig</span> {
    <span class="doccomment">/// Creates a new LayerConfig</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">new</span><span class="op">&lt;</span><span class="ident">L</span>: <span class="ident">Into</span><span class="op">&lt;</span><span class="ident">LayerType</span><span class="op">&gt;</span><span class="op">&gt;</span>(<span class="ident">name</span>: <span class="kw-2">&amp;</span><span class="ident">str</span>, <span class="ident">layer_type</span>: <span class="ident">L</span>) -&gt; <span class="ident">LayerConfig</span> {
        <span class="ident">LayerConfig</span> {
            <span class="ident">name</span>: <span class="ident">name</span>.<span class="ident">to_owned</span>(),
            <span class="ident">layer_type</span>: <span class="ident">layer_type</span>.<span class="ident">into</span>(),

            <span class="ident">outputs</span>: <span class="ident">Vec::new</span>(),
            <span class="ident">inputs</span>: <span class="ident">Vec::new</span>(),

            <span class="ident">params</span>: <span class="ident">Vec::new</span>(),
            <span class="ident">propagate_down</span>: <span class="ident">Vec::new</span>(),
        }
    }

    <span class="doccomment">/// Returns the Name of the requested output Blob</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">output</span>(<span class="kw-2">&amp;</span><span class="self">self</span>, <span class="ident">output_id</span>: <span class="ident">usize</span>) -&gt; <span class="prelude-ty">Option</span><span class="op">&lt;</span><span class="kw-2">&amp;</span><span class="ident">String</span><span class="op">&gt;</span> {
        <span class="self">self</span>.<span class="ident">outputs</span>.<span class="ident">get</span>(<span class="ident">output_id</span>)
    }

    <span class="doccomment">/// Returns the number of output Blobs</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">outputs_len</span>(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; <span class="ident">usize</span> {
        <span class="self">self</span>.<span class="ident">outputs</span>.<span class="ident">len</span>()
    }

    <span class="doccomment">/// Add a output by name</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">add_output</span>(<span class="kw-2">&amp;mut</span> <span class="self">self</span>, <span class="ident">output_name</span>: <span class="kw-2">&amp;</span><span class="ident">str</span>) {
        <span class="self">self</span>.<span class="ident">outputs</span>.<span class="ident">push</span>(<span class="ident">output_name</span>.<span class="ident">to_owned</span>());
    }

    <span class="doccomment">/// Returns the Name of the requested input Blob</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">input</span>(<span class="kw-2">&amp;</span><span class="self">self</span>, <span class="ident">input_id</span>: <span class="ident">usize</span>) -&gt; <span class="prelude-ty">Option</span><span class="op">&lt;</span><span class="kw-2">&amp;</span><span class="ident">String</span><span class="op">&gt;</span> {
        <span class="self">self</span>.<span class="ident">inputs</span>.<span class="ident">get</span>(<span class="ident">input_id</span>)
    }

    <span class="doccomment">/// Returns the number of input Blobs</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">inputs_len</span>(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; <span class="ident">usize</span> {
        <span class="self">self</span>.<span class="ident">inputs</span>.<span class="ident">len</span>()
    }

    <span class="doccomment">/// Add a input by name</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">add_input</span>(<span class="kw-2">&amp;mut</span> <span class="self">self</span>, <span class="ident">input_name</span>: <span class="kw-2">&amp;</span><span class="ident">str</span>) {
        <span class="self">self</span>.<span class="ident">inputs</span>.<span class="ident">push</span>(<span class="ident">input_name</span>.<span class="ident">to_owned</span>());
    }

    <span class="doccomment">/// Returns the requested WeightConfig</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">param</span>(<span class="kw-2">&amp;</span><span class="self">self</span>, <span class="ident">param_id</span>: <span class="ident">usize</span>) -&gt; <span class="prelude-ty">Option</span><span class="op">&lt;</span><span class="kw-2">&amp;</span><span class="ident">WeightConfig</span><span class="op">&gt;</span> {
        <span class="self">self</span>.<span class="ident">params</span>.<span class="ident">get</span>(<span class="ident">param_id</span>)
    }

    <span class="doccomment">/// Returns the number of params</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">params_len</span>(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; <span class="ident">usize</span> {
        <span class="self">self</span>.<span class="ident">params</span>.<span class="ident">len</span>()
    }

    <span class="doccomment">/// Check if the configured parameters make sense.</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">validate</span>(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; <span class="prelude-ty">Result</span><span class="op">&lt;</span>(), <span class="kw-2">&amp;</span><span class="lifetime">&#39;static</span> <span class="ident">str</span><span class="op">&gt;</span> {
        <span class="self">self</span>.<span class="ident">validate_propagate_down_len</span>()<span class="question-mark">?</span>;
        <span class="prelude-val">Ok</span>(())
    }

    <span class="doccomment">/// Checks if propagate down length makes sense.</span>
    <span class="kw">fn</span> <span class="ident">validate_propagate_down_len</span>(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; <span class="prelude-ty">Result</span><span class="op">&lt;</span>(), <span class="kw-2">&amp;</span><span class="lifetime">&#39;static</span> <span class="ident">str</span><span class="op">&gt;</span> {
        <span class="kw">if</span> <span class="self">self</span>.<span class="ident">propagate_down</span>.<span class="ident">is_empty</span>() <span class="op">|</span><span class="op">|</span> <span class="self">self</span>.<span class="ident">propagate_down</span>.<span class="ident">len</span>() <span class="op">==</span> <span class="self">self</span>.<span class="ident">inputs_len</span>() {
            <span class="prelude-val">Ok</span>(())
        } <span class="kw">else</span> {
            <span class="prelude-val">Err</span>(<span class="string">&quot;propagate_down config must be specified either 0 or inputs_len times&quot;</span>)
        }
    }
}

<span class="kw">impl</span><span class="op">&lt;</span><span class="lifetime">&#39;a</span><span class="op">&gt;</span> <span class="ident">CapnpWrite</span><span class="op">&lt;</span><span class="lifetime">&#39;a</span><span class="op">&gt;</span> <span class="kw">for</span> <span class="ident">LayerConfig</span> {
    <span class="kw">type</span> <span class="ident">Builder</span> <span class="op">=</span> <span class="ident">capnp_layer_config::Builder</span><span class="op">&lt;</span><span class="lifetime">&#39;a</span><span class="op">&gt;</span>;

    <span class="doccomment">/// Write the LayerConfig into a capnp message.</span>
    <span class="kw">fn</span> <span class="ident">write_capnp</span>(<span class="kw-2">&amp;</span><span class="self">self</span>, <span class="ident">builder</span>: <span class="kw-2">&amp;mut</span> <span class="ident"><span class="self">Self</span>::Builder</span>) {
        <span class="ident">builder</span>.<span class="ident">set_name</span>(<span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">name</span>);
        {
            <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">layer_type</span> <span class="op">=</span> <span class="ident">builder</span>.<span class="ident">reborrow</span>().<span class="ident">init_layer_type</span>();
            <span class="self">self</span>.<span class="ident">layer_type</span>.<span class="ident">write_capnp</span>(<span class="kw-2">&amp;mut</span> <span class="ident">layer_type</span>);
        }
        {
            <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">outputs</span> <span class="op">=</span> <span class="ident">builder</span>.<span class="ident">reborrow</span>().<span class="ident">init_outputs</span>(<span class="self">self</span>.<span class="ident">outputs</span>.<span class="ident">len</span>() <span class="kw">as</span> <span class="ident">u32</span>);
            <span class="kw">for</span> (<span class="ident">i</span>, <span class="ident">output</span>) <span class="kw">in</span> <span class="self">self</span>.<span class="ident">outputs</span>.<span class="ident">iter</span>().<span class="ident">enumerate</span>() {
                <span class="ident">outputs</span>.<span class="ident">set</span>(<span class="ident">i</span> <span class="kw">as</span> <span class="ident">u32</span>, <span class="kw-2">&amp;</span><span class="ident">output</span>);
            }
        }
        {
            <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">inputs</span> <span class="op">=</span> <span class="ident">builder</span>.<span class="ident">reborrow</span>().<span class="ident">init_inputs</span>(<span class="self">self</span>.<span class="ident">inputs</span>.<span class="ident">len</span>() <span class="kw">as</span> <span class="ident">u32</span>);
            <span class="kw">for</span> (<span class="ident">i</span>, <span class="ident">input</span>) <span class="kw">in</span> <span class="self">self</span>.<span class="ident">inputs</span>.<span class="ident">iter</span>().<span class="ident">enumerate</span>() {
                <span class="ident">inputs</span>.<span class="ident">set</span>(<span class="ident">i</span> <span class="kw">as</span> <span class="ident">u32</span>, <span class="kw-2">&amp;</span><span class="ident">input</span>);
            }
        }
        {
            <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">params</span> <span class="op">=</span> <span class="ident">builder</span>.<span class="ident">reborrow</span>().<span class="ident">init_params</span>(<span class="self">self</span>.<span class="ident">params</span>.<span class="ident">len</span>() <span class="kw">as</span> <span class="ident">u32</span>);
            <span class="kw">for</span> (<span class="ident">i</span>, <span class="ident">param</span>) <span class="kw">in</span> <span class="self">self</span>.<span class="ident">params</span>.<span class="ident">iter</span>().<span class="ident">enumerate</span>() {
                <span class="kw">let</span> <span class="kw-2">ref</span> <span class="kw-2">mut</span> <span class="ident">capnp_param</span> <span class="op">=</span> <span class="ident">params</span>.<span class="ident">reborrow</span>().<span class="ident">get</span>(<span class="ident">i</span> <span class="kw">as</span> <span class="ident">u32</span>);
                <span class="ident">param</span>.<span class="ident">write_capnp</span>(<span class="ident">capnp_param</span>);
            }
        }
        {
            <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">propagate_down</span> <span class="op">=</span> <span class="ident">builder</span>.<span class="ident">reborrow</span>().<span class="ident">init_propagate_down</span>(<span class="self">self</span>.<span class="ident">propagate_down</span>.<span class="ident">len</span>() <span class="kw">as</span> <span class="ident">u32</span>);
            <span class="kw">for</span> (<span class="ident">i</span>, <span class="ident">input</span>) <span class="kw">in</span> <span class="self">self</span>.<span class="ident">propagate_down</span>.<span class="ident">iter</span>().<span class="ident">enumerate</span>() {
                <span class="ident">propagate_down</span>.<span class="ident">set</span>(<span class="ident">i</span> <span class="kw">as</span> <span class="ident">u32</span>, <span class="kw-2">*</span><span class="ident">input</span>);
            }
        }
    }
}

<span class="kw">impl</span><span class="op">&lt;</span><span class="lifetime">&#39;a</span><span class="op">&gt;</span> <span class="ident">CapnpRead</span><span class="op">&lt;</span><span class="lifetime">&#39;a</span><span class="op">&gt;</span> <span class="kw">for</span> <span class="ident">LayerConfig</span> {
    <span class="kw">type</span> <span class="ident">Reader</span> <span class="op">=</span> <span class="ident">capnp_layer_config::Reader</span><span class="op">&lt;</span><span class="lifetime">&#39;a</span><span class="op">&gt;</span>;

    <span class="kw">fn</span> <span class="ident">read_capnp</span>(<span class="ident">reader</span>: <span class="ident"><span class="self">Self</span>::Reader</span>) -&gt; <span class="self">Self</span> {
        <span class="kw">let</span> <span class="ident">name</span> <span class="op">=</span> <span class="ident">reader</span>.<span class="ident">get_name</span>().<span class="ident">unwrap</span>().<span class="ident">to_owned</span>();
        <span class="kw">let</span> <span class="ident">layer_type</span> <span class="op">=</span> <span class="ident">LayerType::read_capnp</span>(<span class="ident">reader</span>.<span class="ident">get_layer_type</span>());

        <span class="kw">let</span> <span class="ident">read_outputs</span> <span class="op">=</span> <span class="ident">reader</span>.<span class="ident">get_outputs</span>().<span class="ident">unwrap</span>();
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">outputs</span> <span class="op">=</span> <span class="ident">Vec::new</span>();
        <span class="kw">for</span> <span class="ident">i</span> <span class="kw">in</span> <span class="number">0</span>..<span class="ident">read_outputs</span>.<span class="ident">len</span>() {
            <span class="ident">outputs</span>.<span class="ident">push</span>(<span class="ident">read_outputs</span>.<span class="ident">get</span>(<span class="ident">i</span>).<span class="ident">unwrap</span>().<span class="ident">to_owned</span>())
        }
        <span class="kw">let</span> <span class="ident">read_inputs</span> <span class="op">=</span> <span class="ident">reader</span>.<span class="ident">get_inputs</span>().<span class="ident">unwrap</span>();
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">inputs</span> <span class="op">=</span> <span class="ident">Vec::new</span>();
        <span class="kw">for</span> <span class="ident">i</span> <span class="kw">in</span> <span class="number">0</span>..<span class="ident">read_inputs</span>.<span class="ident">len</span>() {
            <span class="ident">inputs</span>.<span class="ident">push</span>(<span class="ident">read_inputs</span>.<span class="ident">get</span>(<span class="ident">i</span>).<span class="ident">unwrap</span>().<span class="ident">to_owned</span>())
        }

        <span class="kw">let</span> <span class="ident">read_params</span> <span class="op">=</span> <span class="ident">reader</span>.<span class="ident">get_params</span>().<span class="ident">unwrap</span>();
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">params</span> <span class="op">=</span> <span class="ident">Vec::new</span>();
        <span class="kw">for</span> <span class="ident">i</span> <span class="kw">in</span> <span class="number">0</span>..<span class="ident">read_params</span>.<span class="ident">len</span>() {
            <span class="ident">params</span>.<span class="ident">push</span>(<span class="ident">WeightConfig::read_capnp</span>(<span class="ident">read_params</span>.<span class="ident">get</span>(<span class="ident">i</span>)))
        }

        <span class="kw">let</span> <span class="ident">read_propagate_down</span> <span class="op">=</span> <span class="ident">reader</span>.<span class="ident">get_propagate_down</span>().<span class="ident">unwrap</span>();
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">propagate_down</span> <span class="op">=</span> <span class="ident">Vec::new</span>();
        <span class="kw">for</span> <span class="ident">i</span> <span class="kw">in</span> <span class="number">0</span>..<span class="ident">read_propagate_down</span>.<span class="ident">len</span>() {
            <span class="ident">propagate_down</span>.<span class="ident">push</span>(<span class="ident">read_propagate_down</span>.<span class="ident">get</span>(<span class="ident">i</span>))
        }

        <span class="ident">LayerConfig</span> {
            <span class="ident">name</span>: <span class="ident">name</span>,
            <span class="ident">layer_type</span>: <span class="ident">layer_type</span>,
            <span class="ident">outputs</span>: <span class="ident">outputs</span>,
            <span class="ident">inputs</span>: <span class="ident">inputs</span>,
            <span class="ident">params</span>: <span class="ident">params</span>,
            <span class="ident">propagate_down</span>: <span class="ident">propagate_down</span>,
        }
    }
}
</code></pre></div>
</section></div></main><div id="rustdoc-vars" data-root-path="../../" data-current-crate="juice" data-themes="ayu,dark,light" data-resource-suffix="" data-rustdoc-version="1.63.0 (4b91a6ea7 2022-08-08)" ></div>
</body></html>