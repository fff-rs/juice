<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="rustdoc"><meta name="description" content="Source to the Rust file `juice/src/layers/common/linear.rs`."><meta name="keywords" content="rust, rustlang, rust-lang"><title>linear.rs.html -- source</title><link rel="stylesheet" type="text/css" href="../../../../normalize.css"><link rel="stylesheet" type="text/css" href="../../../../rustdoc.css" id="mainThemeStyle"><link rel="stylesheet" type="text/css" href="../../../../dark.css"><link rel="stylesheet" type="text/css" href="../../../../light.css" id="themeStyle"><script src="../../../../storage.js"></script><noscript><link rel="stylesheet" href="../../../../noscript.css"></noscript><link rel="shortcut icon" href="../../../../favicon.ico"><style type="text/css">#crate-search{background-image:url("../../../../down-arrow.svg");}</style></head><body class="rustdoc source"><!--[if lte IE 8]><div class="warning">This old browser is unsupported and will most likely display funky things.</div><![endif]--><nav class="sidebar"><div class="sidebar-menu">&#9776;</div><a href='../../../../juice/index.html'><div class='logo-container'><img src='../../../../rust-logo.png' alt='logo'></div></a></nav><div class="theme-picker"><button id="theme-picker" aria-label="Pick another theme!"><img src="../../../../brush.svg" width="18" alt="Pick another theme!"></button><div id="theme-choices"></div></div><script src="../../../../theme.js"></script><nav class="sub"><form class="search-form js-only"><div class="search-container"><div><select id="crate-search"><option value="All crates">All crates</option></select><input class="search-input" name="search" autocomplete="off" spellcheck="false" placeholder="Click or press ‘S’ to search, ‘?’ for more options…" type="search"></div><a id="settings-menu" href="../../../../settings.html"><img src="../../../../wheel.svg" width="18" alt="Change settings"></a></div></form></nav><section id="main" class="content"><pre class="line-numbers"><span id="1">  1</span>
<span id="2">  2</span>
<span id="3">  3</span>
<span id="4">  4</span>
<span id="5">  5</span>
<span id="6">  6</span>
<span id="7">  7</span>
<span id="8">  8</span>
<span id="9">  9</span>
<span id="10"> 10</span>
<span id="11"> 11</span>
<span id="12"> 12</span>
<span id="13"> 13</span>
<span id="14"> 14</span>
<span id="15"> 15</span>
<span id="16"> 16</span>
<span id="17"> 17</span>
<span id="18"> 18</span>
<span id="19"> 19</span>
<span id="20"> 20</span>
<span id="21"> 21</span>
<span id="22"> 22</span>
<span id="23"> 23</span>
<span id="24"> 24</span>
<span id="25"> 25</span>
<span id="26"> 26</span>
<span id="27"> 27</span>
<span id="28"> 28</span>
<span id="29"> 29</span>
<span id="30"> 30</span>
<span id="31"> 31</span>
<span id="32"> 32</span>
<span id="33"> 33</span>
<span id="34"> 34</span>
<span id="35"> 35</span>
<span id="36"> 36</span>
<span id="37"> 37</span>
<span id="38"> 38</span>
<span id="39"> 39</span>
<span id="40"> 40</span>
<span id="41"> 41</span>
<span id="42"> 42</span>
<span id="43"> 43</span>
<span id="44"> 44</span>
<span id="45"> 45</span>
<span id="46"> 46</span>
<span id="47"> 47</span>
<span id="48"> 48</span>
<span id="49"> 49</span>
<span id="50"> 50</span>
<span id="51"> 51</span>
<span id="52"> 52</span>
<span id="53"> 53</span>
<span id="54"> 54</span>
<span id="55"> 55</span>
<span id="56"> 56</span>
<span id="57"> 57</span>
<span id="58"> 58</span>
<span id="59"> 59</span>
<span id="60"> 60</span>
<span id="61"> 61</span>
<span id="62"> 62</span>
<span id="63"> 63</span>
<span id="64"> 64</span>
<span id="65"> 65</span>
<span id="66"> 66</span>
<span id="67"> 67</span>
<span id="68"> 68</span>
<span id="69"> 69</span>
<span id="70"> 70</span>
<span id="71"> 71</span>
<span id="72"> 72</span>
<span id="73"> 73</span>
<span id="74"> 74</span>
<span id="75"> 75</span>
<span id="76"> 76</span>
<span id="77"> 77</span>
<span id="78"> 78</span>
<span id="79"> 79</span>
<span id="80"> 80</span>
<span id="81"> 81</span>
<span id="82"> 82</span>
<span id="83"> 83</span>
<span id="84"> 84</span>
<span id="85"> 85</span>
<span id="86"> 86</span>
<span id="87"> 87</span>
<span id="88"> 88</span>
<span id="89"> 89</span>
<span id="90"> 90</span>
<span id="91"> 91</span>
<span id="92"> 92</span>
<span id="93"> 93</span>
<span id="94"> 94</span>
<span id="95"> 95</span>
<span id="96"> 96</span>
<span id="97"> 97</span>
<span id="98"> 98</span>
<span id="99"> 99</span>
<span id="100">100</span>
<span id="101">101</span>
<span id="102">102</span>
<span id="103">103</span>
<span id="104">104</span>
<span id="105">105</span>
<span id="106">106</span>
<span id="107">107</span>
<span id="108">108</span>
<span id="109">109</span>
<span id="110">110</span>
<span id="111">111</span>
<span id="112">112</span>
<span id="113">113</span>
<span id="114">114</span>
<span id="115">115</span>
<span id="116">116</span>
<span id="117">117</span>
<span id="118">118</span>
<span id="119">119</span>
<span id="120">120</span>
<span id="121">121</span>
<span id="122">122</span>
<span id="123">123</span>
<span id="124">124</span>
<span id="125">125</span>
<span id="126">126</span>
<span id="127">127</span>
<span id="128">128</span>
<span id="129">129</span>
<span id="130">130</span>
<span id="131">131</span>
<span id="132">132</span>
<span id="133">133</span>
<span id="134">134</span>
<span id="135">135</span>
<span id="136">136</span>
<span id="137">137</span>
<span id="138">138</span>
<span id="139">139</span>
<span id="140">140</span>
<span id="141">141</span>
<span id="142">142</span>
<span id="143">143</span>
<span id="144">144</span>
<span id="145">145</span>
<span id="146">146</span>
<span id="147">147</span>
<span id="148">148</span>
<span id="149">149</span>
<span id="150">150</span>
<span id="151">151</span>
<span id="152">152</span>
<span id="153">153</span>
<span id="154">154</span>
<span id="155">155</span>
<span id="156">156</span>
<span id="157">157</span>
<span id="158">158</span>
<span id="159">159</span>
<span id="160">160</span>
<span id="161">161</span>
<span id="162">162</span>
<span id="163">163</span>
<span id="164">164</span>
<span id="165">165</span>
<span id="166">166</span>
<span id="167">167</span>
<span id="168">168</span>
<span id="169">169</span>
<span id="170">170</span>
<span id="171">171</span>
<span id="172">172</span>
<span id="173">173</span>
<span id="174">174</span>
<span id="175">175</span>
<span id="176">176</span>
<span id="177">177</span>
<span id="178">178</span>
<span id="179">179</span>
<span id="180">180</span>
<span id="181">181</span>
<span id="182">182</span>
<span id="183">183</span>
<span id="184">184</span>
<span id="185">185</span>
<span id="186">186</span>
<span id="187">187</span>
<span id="188">188</span>
<span id="189">189</span>
<span id="190">190</span>
<span id="191">191</span>
<span id="192">192</span>
<span id="193">193</span>
<span id="194">194</span>
<span id="195">195</span>
<span id="196">196</span>
<span id="197">197</span>
<span id="198">198</span>
<span id="199">199</span>
<span id="200">200</span>
<span id="201">201</span>
<span id="202">202</span>
<span id="203">203</span>
<span id="204">204</span>
<span id="205">205</span>
<span id="206">206</span>
<span id="207">207</span>
<span id="208">208</span>
<span id="209">209</span>
<span id="210">210</span>
<span id="211">211</span>
<span id="212">212</span>
<span id="213">213</span>
<span id="214">214</span>
<span id="215">215</span>
<span id="216">216</span>
<span id="217">217</span>
<span id="218">218</span>
<span id="219">219</span>
<span id="220">220</span>
<span id="221">221</span>
<span id="222">222</span>
<span id="223">223</span>
<span id="224">224</span>
<span id="225">225</span>
<span id="226">226</span>
<span id="227">227</span>
<span id="228">228</span>
<span id="229">229</span>
<span id="230">230</span>
<span id="231">231</span>
<span id="232">232</span>
<span id="233">233</span>
<span id="234">234</span>
<span id="235">235</span>
<span id="236">236</span>
<span id="237">237</span>
<span id="238">238</span>
<span id="239">239</span>
<span id="240">240</span>
<span id="241">241</span>
<span id="242">242</span>
<span id="243">243</span>
<span id="244">244</span>
<span id="245">245</span>
<span id="246">246</span>
<span id="247">247</span>
<span id="248">248</span>
<span id="249">249</span>
<span id="250">250</span>
<span id="251">251</span>
<span id="252">252</span>
<span id="253">253</span>
<span id="254">254</span>
<span id="255">255</span>
<span id="256">256</span>
<span id="257">257</span>
<span id="258">258</span>
<span id="259">259</span>
<span id="260">260</span>
<span id="261">261</span>
<span id="262">262</span>
<span id="263">263</span>
<span id="264">264</span>
<span id="265">265</span>
<span id="266">266</span>
<span id="267">267</span>
<span id="268">268</span>
<span id="269">269</span>
<span id="270">270</span>
<span id="271">271</span>
<span id="272">272</span>
<span id="273">273</span>
<span id="274">274</span>
<span id="275">275</span>
<span id="276">276</span>
<span id="277">277</span>
<span id="278">278</span>
<span id="279">279</span>
<span id="280">280</span>
<span id="281">281</span>
<span id="282">282</span>
<span id="283">283</span>
<span id="284">284</span>
<span id="285">285</span>
<span id="286">286</span>
<span id="287">287</span>
<span id="288">288</span>
<span id="289">289</span>
<span id="290">290</span>
<span id="291">291</span>
<span id="292">292</span>
<span id="293">293</span>
<span id="294">294</span>
<span id="295">295</span>
<span id="296">296</span>
<span id="297">297</span>
<span id="298">298</span>
<span id="299">299</span>
<span id="300">300</span>
<span id="301">301</span>
<span id="302">302</span>
<span id="303">303</span>
<span id="304">304</span>
<span id="305">305</span>
<span id="306">306</span>
<span id="307">307</span>
<span id="308">308</span>
<span id="309">309</span>
<span id="310">310</span>
<span id="311">311</span>
<span id="312">312</span>
<span id="313">313</span>
<span id="314">314</span>
<span id="315">315</span>
<span id="316">316</span>
<span id="317">317</span>
<span id="318">318</span>
<span id="319">319</span>
<span id="320">320</span>
<span id="321">321</span>
<span id="322">322</span>
<span id="323">323</span>
<span id="324">324</span>
<span id="325">325</span>
<span id="326">326</span>
<span id="327">327</span>
<span id="328">328</span>
<span id="329">329</span>
<span id="330">330</span>
<span id="331">331</span>
<span id="332">332</span>
<span id="333">333</span>
<span id="334">334</span>
<span id="335">335</span>
<span id="336">336</span>
<span id="337">337</span>
<span id="338">338</span>
<span id="339">339</span>
<span id="340">340</span>
<span id="341">341</span>
<span id="342">342</span>
<span id="343">343</span>
<span id="344">344</span>
<span id="345">345</span>
<span id="346">346</span>
<span id="347">347</span>
<span id="348">348</span>
<span id="349">349</span>
<span id="350">350</span>
<span id="351">351</span>
<span id="352">352</span>
<span id="353">353</span>
<span id="354">354</span>
<span id="355">355</span>
<span id="356">356</span>
<span id="357">357</span>
<span id="358">358</span>
<span id="359">359</span>
<span id="360">360</span>
<span id="361">361</span>
<span id="362">362</span>
<span id="363">363</span>
<span id="364">364</span>
<span id="365">365</span>
<span id="366">366</span>
<span id="367">367</span>
<span id="368">368</span>
<span id="369">369</span>
<span id="370">370</span>
<span id="371">371</span>
<span id="372">372</span>
<span id="373">373</span>
<span id="374">374</span>
<span id="375">375</span>
<span id="376">376</span>
<span id="377">377</span>
<span id="378">378</span>
<span id="379">379</span>
<span id="380">380</span>
<span id="381">381</span>
</pre><div class="example-wrap"><pre class="rust ">
<span class="doccomment">//! Applies a linear transformation to the input data `y = a * x + b`</span>
<span class="doccomment">//!</span>
<span class="doccomment">//! The variables are:</span>
<span class="doccomment">//!</span>
<span class="doccomment">//! - `y`: output value</span>
<span class="doccomment">//! - `a`: weight (a trainable weight in a neural network)</span>
<span class="doccomment">//! - `x`: input value</span>
<span class="doccomment">//! - `b`: bias (only for Backends with the `coblas::plugin::Copy trait`)</span>
<span class="doccomment">//!</span>
<span class="doccomment">//! ## Input Data</span>
<span class="doccomment">//!</span>
<span class="doccomment">//! The input can either have one or two dimensions:</span>
<span class="doccomment">//!</span>
<span class="doccomment">//! - If the input has one dimension the transformation will just be applied to the input data.</span>
<span class="doccomment">//! - If the input has two dimensions **the first dimension is treated as batch size** (`N`)</span>
<span class="doccomment">//!   and the transformation will be applied to every vector in the second dimension, using the</span>
<span class="doccomment">//!   same weights and biases.</span>
<span class="doccomment">//!</span>
<span class="doccomment">//! In the context of convolutional neural networks this layer is also</span>
<span class="doccomment">//! called a &quot;fully-connected layer&quot; if it is used at the end of the network.</span>

<span class="kw">use</span> <span class="kw">crate</span>::<span class="ident">capnp_util</span>::<span class="kw-2">*</span>;
<span class="kw">use</span> <span class="kw">crate</span>::<span class="ident">co</span>::<span class="ident">backend</span>::<span class="ident">IBackend</span>;
<span class="kw">use</span> <span class="kw">crate</span>::<span class="ident">co</span>::<span class="ident">tensor</span>::<span class="ident">SharedTensor</span>;
<span class="kw">use</span> <span class="kw">crate</span>::<span class="ident">coblas</span>::<span class="ident">transpose</span>::<span class="ident">Transpose</span>;
<span class="kw">use</span> <span class="kw">crate</span>::<span class="ident">layer</span>::<span class="kw-2">*</span>;
<span class="kw">use</span> <span class="kw">crate</span>::<span class="ident">juice_capnp</span>::<span class="ident">linear_config</span> <span class="kw">as</span> <span class="ident">capnp_config</span>;
<span class="kw">use</span> <span class="kw">crate</span>::<span class="ident">util</span>::{<span class="ident">ArcLock</span>, <span class="ident">native_scalar</span>, <span class="ident">LayerOps</span>};
<span class="kw">use</span> <span class="kw">crate</span>::<span class="ident">weight</span>::<span class="ident">FillerType</span>;

<span class="attribute">#[<span class="ident">derive</span>(<span class="ident">Debug</span>)]</span>
<span class="doccomment">/// Linear Layer</span>
<span class="kw">pub</span> <span class="kw">struct</span> <span class="ident">Linear</span> {
    <span class="ident">output_size</span>: <span class="ident">usize</span>,

    <span class="ident">one</span>: <span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>,
    <span class="ident">zero</span>: <span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>,
}

<span class="kw">impl</span> <span class="ident">Linear</span> {
    <span class="doccomment">/// Create a Linear layer from a LinearConfig.</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">from_config</span>(<span class="ident">config</span>: <span class="kw-2">&amp;</span><span class="ident">LinearConfig</span>) <span class="op">-</span><span class="op">&gt;</span> <span class="ident">Linear</span> {
        <span class="kw">let</span> <span class="ident">one</span> <span class="op">=</span> <span class="ident">native_scalar</span>(<span class="number">1f32</span>);
        <span class="kw">let</span> <span class="ident">zero</span> <span class="op">=</span> <span class="ident">native_scalar</span>(<span class="number">0f32</span>);

        <span class="ident">Linear</span> {
            <span class="ident">output_size</span>: <span class="ident">config</span>.<span class="ident">output_size</span>,

            <span class="ident">one</span>,
            <span class="ident">zero</span>,
        }
    }

    <span class="comment">// Calculates the input size by skipping the batch size.</span>
    <span class="kw">fn</span> <span class="ident">calculate_input_size</span>(<span class="ident">input_shape</span>: <span class="kw-2">&amp;</span>[<span class="ident">usize</span>]) <span class="op">-</span><span class="op">&gt;</span> <span class="ident">usize</span> {
        <span class="ident">input_shape</span>.<span class="ident">iter</span>().<span class="ident">skip</span>(<span class="number">1</span>).<span class="ident">fold</span>(<span class="number">1</span>, <span class="op">|</span><span class="ident">prod</span>, <span class="ident">i</span><span class="op">|</span> <span class="ident">prod</span> <span class="op">*</span> <span class="ident">i</span>)
    }

    <span class="kw">fn</span> <span class="ident">calculate_output_shape</span>(<span class="kw-2">&amp;</span><span class="self">self</span>, <span class="ident">input_shape</span>: <span class="kw-2">&amp;</span>[<span class="ident">usize</span>]) <span class="op">-</span><span class="op">&gt;</span> <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">usize</span><span class="op">&gt;</span> {
        <span class="kw">let</span> <span class="ident">n</span> <span class="op">=</span> <span class="ident">input_shape</span>[<span class="number">0</span>]; <span class="comment">// batch size</span>
        <span class="macro">vec</span><span class="macro">!</span>[<span class="ident">n</span>, <span class="self">self</span>.<span class="ident">output_size</span>]
    }

    <span class="kw">fn</span> <span class="ident">calculate_weight_shape</span>(<span class="kw-2">&amp;</span><span class="self">self</span>, <span class="ident">input_shape</span>: <span class="kw-2">&amp;</span>[<span class="ident">usize</span>]) <span class="op">-</span><span class="op">&gt;</span> <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">usize</span><span class="op">&gt;</span> {
        <span class="kw">let</span> <span class="ident">m</span> <span class="op">=</span> <span class="self">Self</span>::<span class="ident">calculate_input_size</span>(<span class="ident">input_shape</span>);
        <span class="macro">vec</span><span class="macro">!</span>[<span class="self">self</span>.<span class="ident">output_size</span>, <span class="ident">m</span>]
    }
}

<span class="kw">impl</span><span class="op">&lt;</span><span class="ident">B</span>: <span class="ident">IBackend</span> <span class="op">+</span> <span class="ident">LayerOps</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span> <span class="ident">ILayer</span><span class="op">&lt;</span><span class="ident">B</span><span class="op">&gt;</span> <span class="kw">for</span> <span class="ident">Linear</span> {

    <span class="kw">fn</span> <span class="ident">auto_weight_blobs</span>(<span class="kw-2">&amp;</span><span class="self">self</span>) <span class="op">-</span><span class="op">&gt;</span> <span class="ident">bool</span> {
        <span class="bool-val">true</span>
    }

    <span class="kw">fn</span> <span class="ident">reshape</span>(<span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="self">self</span>,
               <span class="ident">backend</span>: ::<span class="ident">std</span>::<span class="ident">rc</span>::<span class="ident">Rc</span><span class="op">&lt;</span><span class="ident">B</span><span class="op">&gt;</span>,
               <span class="ident">input_data</span>: <span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span>,
               <span class="ident">input_gradient</span>: <span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span>,
               <span class="ident">weights_data</span>: <span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span>,
               <span class="ident">weights_gradient</span>: <span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span>,
               <span class="ident">output_data</span>: <span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span>,
               <span class="ident">output_gradient</span>: <span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span>) {
        <span class="kw">let</span> <span class="ident">input</span> <span class="op">=</span> <span class="ident">input_data</span>[<span class="number">0</span>].<span class="ident">read</span>().<span class="ident">unwrap</span>();
        <span class="comment">// reshape top</span>
        <span class="kw">let</span> <span class="ident">output_shape</span> <span class="op">=</span> <span class="self">self</span>.<span class="ident">calculate_output_shape</span>(<span class="ident">input</span>.<span class="ident">desc</span>());
        <span class="ident">output_data</span>[<span class="number">0</span>].<span class="ident">write</span>().<span class="ident">unwrap</span>().<span class="ident">resize</span>(<span class="kw-2">&amp;</span><span class="ident">output_shape</span>).<span class="ident">unwrap</span>();
        <span class="ident">output_gradient</span>[<span class="number">0</span>].<span class="ident">write</span>().<span class="ident">unwrap</span>().<span class="ident">resize</span>(<span class="kw-2">&amp;</span><span class="ident">output_shape</span>).<span class="ident">unwrap</span>();
        <span class="comment">// reshape weight</span>
        <span class="kw">let</span> <span class="ident">weight_shape</span> <span class="op">=</span> <span class="self">self</span>.<span class="ident">calculate_weight_shape</span>(<span class="ident">input</span>.<span class="ident">desc</span>());
        <span class="comment">// TODO: change weight creation to not require this</span>
        <span class="kw">if</span> <span class="kw">let</span> <span class="prelude-val">Some</span>(<span class="ident">weight</span>) <span class="op">=</span> <span class="ident">weights_data</span>.<span class="ident">get</span>(<span class="number">0</span>) {
            <span class="ident">weight</span>.<span class="ident">write</span>().<span class="ident">unwrap</span>().<span class="ident">resize</span>(<span class="kw-2">&amp;</span><span class="ident">weight_shape</span>).<span class="ident">unwrap</span>();
            <span class="kw">let</span> <span class="ident">filler</span> <span class="op">=</span> <span class="ident">FillerType</span>::<span class="ident">Glorot</span> {
                <span class="ident">input_size</span>: <span class="self">Self</span>::<span class="ident">calculate_input_size</span>(<span class="ident">input</span>.<span class="ident">desc</span>()),
                <span class="ident">output_size</span>: <span class="self">self</span>.<span class="ident">output_size</span>,
            };
            <span class="ident">filler</span>.<span class="ident">fill</span>(<span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="ident">weight</span>.<span class="ident">write</span>().<span class="ident">unwrap</span>());
        }
        <span class="kw">if</span> <span class="kw">let</span> <span class="prelude-val">Some</span>(<span class="ident">weight</span>) <span class="op">=</span> <span class="ident">weights_gradient</span>.<span class="ident">get</span>(<span class="number">0</span>) {
            <span class="ident">weight</span>.<span class="ident">write</span>().<span class="ident">unwrap</span>().<span class="ident">resize</span>(<span class="kw-2">&amp;</span><span class="ident">weight_shape</span>).<span class="ident">unwrap</span>();
        }

        <span class="comment">// Fill the bias</span>
        <span class="kw">if</span> <span class="kw">let</span> <span class="prelude-val">Some</span>(<span class="ident">weight</span>) <span class="op">=</span> <span class="ident">weights_data</span>.<span class="ident">get</span>(<span class="number">1</span>) {
            <span class="ident">weight</span>.<span class="ident">write</span>().<span class="ident">unwrap</span>().<span class="ident">resize</span>(<span class="kw-2">&amp;</span>(<span class="number">1</span>, <span class="self">self</span>.<span class="ident">output_size</span>)).<span class="ident">unwrap</span>();
            <span class="kw">let</span> <span class="ident">filler</span> <span class="op">=</span> <span class="ident">FillerType</span>::<span class="ident">Glorot</span> {
                <span class="ident">input_size</span>: <span class="number">1</span>,
                <span class="ident">output_size</span>: <span class="self">self</span>.<span class="ident">output_size</span>,
            };
            <span class="ident">filler</span>.<span class="ident">fill</span>(<span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="ident">weight</span>.<span class="ident">write</span>().<span class="ident">unwrap</span>());
        }
        <span class="kw">if</span> <span class="kw">let</span> <span class="prelude-val">Some</span>(<span class="ident">weight</span>) <span class="op">=</span> <span class="ident">weights_gradient</span>.<span class="ident">get</span>(<span class="number">1</span>) {
            <span class="ident">weight</span>.<span class="ident">write</span>().<span class="ident">unwrap</span>().<span class="ident">resize</span>(<span class="kw-2">&amp;</span>(<span class="number">1</span>, <span class="self">self</span>.<span class="ident">output_size</span>)).<span class="ident">unwrap</span>();
        }
    }

    <span class="kw">fn</span> <span class="ident">exact_num_output_blobs</span>(<span class="kw-2">&amp;</span><span class="self">self</span>) <span class="op">-</span><span class="op">&gt;</span> <span class="prelude-ty">Option</span><span class="op">&lt;</span><span class="ident">usize</span><span class="op">&gt;</span> {
        <span class="prelude-val">Some</span>(<span class="number">1</span>)
    }
}

<span class="kw">impl</span><span class="op">&lt;</span><span class="ident">B</span>: <span class="ident">IBackend</span> <span class="op">+</span> <span class="ident">LayerOps</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span> <span class="ident">ComputeOutput</span><span class="op">&lt;</span><span class="ident">f32</span>, <span class="ident">B</span><span class="op">&gt;</span> <span class="kw">for</span> <span class="ident">Linear</span> {
    <span class="doccomment">/// Basically, x has the shape (k, n) where k is the batch size. Given W with shape (m, n) where</span>
    <span class="doccomment">/// m is output vector length, we compute the output with the formula xW^T which will give us a</span>
    <span class="doccomment">/// matrix of size (k, m) with the outputs.</span>
    <span class="kw">fn</span> <span class="ident">compute_output</span>(<span class="kw-2">&amp;</span><span class="self">self</span>,
                      <span class="ident">backend</span>: <span class="kw-2">&amp;</span><span class="ident">B</span>,
                      <span class="ident">weights</span>: <span class="kw-2">&amp;</span>[<span class="kw-2">&amp;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>],
                      <span class="ident">input_data</span>: <span class="kw-2">&amp;</span>[<span class="kw-2">&amp;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>],
                      <span class="ident">output_data</span>: <span class="kw-2">&amp;</span><span class="kw-2">mut</span> [<span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>]) {

        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">ones_tensor</span> <span class="op">=</span> <span class="ident">SharedTensor</span>::<span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>::<span class="ident">new</span>(<span class="kw-2">&amp;</span>[<span class="ident">input_data</span>[<span class="number">0</span>].<span class="ident">desc</span>().<span class="ident">as_slice</span>()[<span class="number">0</span>], <span class="number">1</span>]);
        <span class="ident">FillerType</span>::<span class="ident">fill_constant</span>(<span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="ident">ones_tensor</span>, <span class="number">1f32</span>);
        <span class="ident">backend</span>.<span class="ident">gemm</span>(<span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">one</span>,
                     <span class="ident">Transpose</span>::<span class="ident">NoTrans</span>,
                     <span class="kw-2">&amp;</span><span class="ident">ones_tensor</span>,
                     <span class="ident">Transpose</span>::<span class="ident">NoTrans</span>,
                     <span class="ident">weights</span>[<span class="number">1</span>],
                     <span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">zero</span>,
                     <span class="ident">output_data</span>[<span class="number">0</span>])
            .<span class="ident">unwrap</span>();

        <span class="ident">backend</span>.<span class="ident">gemm</span>(<span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">one</span>,
                  <span class="ident">Transpose</span>::<span class="ident">NoTrans</span>,
                  <span class="ident">input_data</span>[<span class="number">0</span>],
                  <span class="ident">Transpose</span>::<span class="ident">Trans</span>,
                  <span class="ident">weights</span>[<span class="number">0</span>],
                  <span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">one</span>,
                  <span class="ident">output_data</span>[<span class="number">0</span>])
            .<span class="ident">unwrap</span>();
    }
}

<span class="kw">impl</span><span class="op">&lt;</span><span class="ident">B</span>: <span class="ident">IBackend</span> <span class="op">+</span> <span class="ident">LayerOps</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span> <span class="ident">ComputeInputGradient</span><span class="op">&lt;</span><span class="ident">f32</span>, <span class="ident">B</span><span class="op">&gt;</span> <span class="kw">for</span> <span class="ident">Linear</span> {
    <span class="doccomment">/// Since we have row vectors instead of columns, xW^T = (Wx^T)^T. Take the derivative with</span>
    <span class="doccomment">/// respect to x^T (gives us a column vector of dimension (n, 1)), we get d((Wx^T)^T)/d(x^T) =</span>
    <span class="doccomment">/// W^T of dims (n, m). In backpropagation with column vectors, we would take W^T * output_grad,</span>
    <span class="doccomment">/// and in terms of row vectors, that would be output_grad^T * W which produces a vector of</span>
    <span class="doccomment">/// dims (1, n)</span>
    <span class="kw">fn</span> <span class="ident">compute_input_gradient</span>(<span class="kw-2">&amp;</span><span class="self">self</span>,
                              <span class="ident">backend</span>: <span class="kw-2">&amp;</span><span class="ident">B</span>,
                              <span class="ident">weights_data</span>: <span class="kw-2">&amp;</span>[<span class="kw-2">&amp;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>],
                              <span class="ident">output_data</span>: <span class="kw-2">&amp;</span>[<span class="kw-2">&amp;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>],
                              <span class="ident">output_gradients</span>: <span class="kw-2">&amp;</span>[<span class="kw-2">&amp;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>],
                              <span class="ident">input_data</span>: <span class="kw-2">&amp;</span>[<span class="kw-2">&amp;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>],
                              <span class="ident">input_gradients</span>: <span class="kw-2">&amp;</span><span class="kw-2">mut</span> [<span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>]) {
        <span class="comment">// Gradient with respect to input data</span>
        <span class="ident">backend</span>.<span class="ident">gemm</span>(<span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">one</span>,
                  <span class="ident">Transpose</span>::<span class="ident">NoTrans</span>,
                  <span class="ident">output_gradients</span>[<span class="number">0</span>],
                  <span class="ident">Transpose</span>::<span class="ident">NoTrans</span>,
                  <span class="ident">weights_data</span>[<span class="number">0</span>],
                  <span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">zero</span>,
                  <span class="ident">input_gradients</span>[<span class="number">0</span>])
            .<span class="ident">unwrap</span>();
    }
}

<span class="kw">impl</span><span class="op">&lt;</span><span class="ident">B</span>: <span class="ident">IBackend</span> <span class="op">+</span> <span class="ident">LayerOps</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span> <span class="ident">ComputeParametersGradient</span><span class="op">&lt;</span><span class="ident">f32</span>, <span class="ident">B</span><span class="op">&gt;</span> <span class="kw">for</span> <span class="ident">Linear</span> {
    <span class="kw">fn</span> <span class="ident">compute_parameters_gradient</span>(<span class="kw-2">&amp;</span><span class="self">self</span>,
                                   <span class="ident">backend</span>: <span class="kw-2">&amp;</span><span class="ident">B</span>,
                                   <span class="ident">output_data</span>: <span class="kw-2">&amp;</span>[<span class="kw-2">&amp;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>],
                                   <span class="ident">output_gradients</span>: <span class="kw-2">&amp;</span>[<span class="kw-2">&amp;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>],
                                   <span class="ident">input_data</span>: <span class="kw-2">&amp;</span>[<span class="kw-2">&amp;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>],
                                   <span class="ident">parameters_gradients</span>: <span class="kw-2">&amp;</span><span class="kw-2">mut</span> [<span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>]) {
        <span class="comment">// gradient w.r.t. weights</span>
        <span class="ident">backend</span>.<span class="ident">gemm</span>(<span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">one</span>,
                  <span class="ident">Transpose</span>::<span class="ident">Trans</span>,
                  <span class="ident">output_gradients</span>[<span class="number">0</span>],
                  <span class="ident">Transpose</span>::<span class="ident">NoTrans</span>,
                  <span class="ident">input_data</span>[<span class="number">0</span>],
                  <span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">zero</span>,
                  <span class="ident">parameters_gradients</span>[<span class="number">0</span>])
            .<span class="ident">unwrap</span>();

        <span class="comment">// gradient w.r.t bias</span>
        <span class="comment">// Technically, the gradient of vector b of length n to itself is the I_n identity matrix,</span>
        <span class="comment">// so instead we&#39;ll just copy the output_gradient[0] vector into</span>
        <span class="ident">backend</span>.<span class="ident">copy</span>(<span class="kw-2">&amp;</span><span class="ident">output_gradients</span>[<span class="number">0</span>], <span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="ident">parameters_gradients</span>[<span class="number">1</span>]).<span class="ident">unwrap</span>();

    }
}

<span class="kw">impl</span> ::<span class="ident">std</span>::<span class="ident">default</span>::<span class="ident">Default</span> <span class="kw">for</span> <span class="ident">Linear</span> {
    <span class="kw">fn</span> <span class="ident">default</span>() <span class="op">-</span><span class="op">&gt;</span> <span class="ident">Linear</span> {
        <span class="kw">let</span> <span class="ident">config</span> <span class="op">=</span> <span class="ident">LinearConfig</span> { <span class="ident">output_size</span>: <span class="number">10</span> };

        <span class="self">Self</span>::<span class="ident">from_config</span>(<span class="kw-2">&amp;</span><span class="ident">config</span>)
    }
}


<span class="attribute">#[<span class="ident">derive</span>(<span class="ident">Debug</span>, <span class="ident">Clone</span>)]</span>
<span class="attribute">#[<span class="ident">allow</span>(<span class="ident">missing_copy_implementations</span>)]</span>
<span class="doccomment">/// Specifies configuration parameters for a Linear Layer.</span>
<span class="kw">pub</span> <span class="kw">struct</span> <span class="ident">LinearConfig</span> {
    <span class="doccomment">/// The number of output values</span>
    <span class="kw">pub</span> <span class="ident">output_size</span>: <span class="ident">usize</span>,
}

<span class="kw">impl</span><span class="op">&lt;</span><span class="lifetime">&#39;a</span><span class="op">&gt;</span> <span class="ident">CapnpWrite</span><span class="op">&lt;</span><span class="lifetime">&#39;a</span><span class="op">&gt;</span> <span class="kw">for</span> <span class="ident">LinearConfig</span> {
    <span class="kw">type</span> <span class="ident">Builder</span> <span class="op">=</span> <span class="ident">capnp_config</span>::<span class="ident">Builder</span><span class="op">&lt;</span><span class="lifetime">&#39;a</span><span class="op">&gt;</span>;

    <span class="doccomment">/// Write the LinearConfig into a capnp message.</span>
    <span class="kw">fn</span> <span class="ident">write_capnp</span>(<span class="kw-2">&amp;</span><span class="self">self</span>, <span class="ident">builder</span>: <span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="self">Self</span>::<span class="ident">Builder</span>) {
        <span class="ident">builder</span>.<span class="ident">reborrow</span>().<span class="ident">set_output_size</span>(<span class="self">self</span>.<span class="ident">output_size</span> <span class="kw">as</span> <span class="ident">u64</span>);
    }
}

<span class="kw">impl</span><span class="op">&lt;</span><span class="lifetime">&#39;a</span><span class="op">&gt;</span> <span class="ident">CapnpRead</span><span class="op">&lt;</span><span class="lifetime">&#39;a</span><span class="op">&gt;</span> <span class="kw">for</span> <span class="ident">LinearConfig</span> {
    <span class="kw">type</span> <span class="ident">Reader</span> <span class="op">=</span> <span class="ident">capnp_config</span>::<span class="ident">Reader</span><span class="op">&lt;</span><span class="lifetime">&#39;a</span><span class="op">&gt;</span>;

    <span class="kw">fn</span> <span class="ident">read_capnp</span>(<span class="ident">reader</span>: <span class="self">Self</span>::<span class="ident">Reader</span>) <span class="op">-</span><span class="op">&gt;</span> <span class="self">Self</span> {
        <span class="kw">let</span> <span class="ident">output_size</span> <span class="op">=</span> <span class="ident">reader</span>.<span class="ident">get_output_size</span>() <span class="kw">as</span> <span class="ident">usize</span>;

        <span class="ident">LinearConfig</span> { <span class="ident">output_size</span>: <span class="ident">output_size</span> }
    }
}

<span class="kw">impl</span> <span class="ident">Into</span><span class="op">&lt;</span><span class="ident">LayerType</span><span class="op">&gt;</span> <span class="kw">for</span> <span class="ident">LinearConfig</span> {
    <span class="kw">fn</span> <span class="ident">into</span>(<span class="self">self</span>) <span class="op">-</span><span class="op">&gt;</span> <span class="ident">LayerType</span> {
        <span class="ident">LayerType</span>::<span class="ident">Linear</span>(<span class="self">self</span>)
    }
}

<span class="attribute">#[<span class="ident">cfg</span>(<span class="ident">test</span>)]</span>
<span class="kw">mod</span> <span class="ident">tests</span> {
    <span class="kw">use</span> <span class="kw">crate</span>::<span class="ident">layers</span>::{<span class="ident">LinearConfig</span>, <span class="ident">Linear</span>};
    <span class="kw">use</span> <span class="kw">crate</span>::<span class="ident">co</span>::<span class="ident">tensor</span>::<span class="ident">SharedTensor</span>;
    <span class="kw">use</span> <span class="kw">crate</span>::<span class="ident">layer</span>::{<span class="ident">ComputeOutput</span>, <span class="ident">ComputeParametersGradient</span>, <span class="ident">ComputeInputGradient</span>};
    <span class="kw">use</span> <span class="ident">util</span>::<span class="ident">native_backend</span>;


    <span class="kw">fn</span> <span class="ident">get_sample_w</span>() <span class="op">-</span><span class="op">&gt;</span> <span class="kw-2">&amp;</span><span class="lifetime">&#39;static</span> [<span class="ident">f32</span>] {
        [
            <span class="number">1f32</span>, <span class="number">0f32</span>, <span class="number">3f32</span>, <span class="number">0f32</span>,
            <span class="number">1.5f32</span>, <span class="number">4f32</span>, <span class="number">2f32</span>, <span class="number">0f32</span>,
            <span class="number">0f32</span>, <span class="number">2f32</span>, <span class="number">1.5f32</span>, <span class="number">4f32</span>,
        ].<span class="ident">as_ref</span>()
    }

    <span class="kw">fn</span> <span class="ident">get_sample_x</span>() <span class="op">-</span><span class="op">&gt;</span> <span class="kw-2">&amp;</span><span class="lifetime">&#39;static</span> [<span class="ident">f32</span>] {
        [<span class="number">1f32</span>, <span class="number">2f32</span>, <span class="number">3f32</span>, <span class="number">4f32</span>].<span class="ident">as_ref</span>()
    }

    <span class="kw">fn</span> <span class="ident">get_sample_b</span>() <span class="op">-</span><span class="op">&gt;</span> <span class="kw-2">&amp;</span><span class="lifetime">&#39;static</span> [<span class="ident">f32</span>] {
        [<span class="op">-</span><span class="number">1f32</span>, <span class="number">1f32</span>, <span class="number">0f32</span>].<span class="ident">as_ref</span>()
    }

    <span class="kw">fn</span> <span class="ident">get_sample_output_gradient</span>() <span class="op">-</span><span class="op">&gt;</span> <span class="kw-2">&amp;</span><span class="lifetime">&#39;static</span> [<span class="ident">f32</span>] {
        [<span class="op">-</span><span class="number">1f32</span>, <span class="number">0.5f32</span>, <span class="number">0.2f32</span>].<span class="ident">as_ref</span>()
    }

    <span class="attribute">#[<span class="ident">test</span>]</span>
    <span class="kw">fn</span> <span class="ident">forward_pass_test</span>() {
        <span class="kw">let</span> <span class="kw-2">ref</span> <span class="ident">config</span> <span class="op">=</span> <span class="ident">LinearConfig</span> { <span class="ident">output_size</span>: <span class="number">3</span> };
        <span class="kw">let</span> <span class="ident">layer</span> <span class="op">=</span> <span class="ident">Linear</span>::<span class="ident">from_config</span>(<span class="ident">config</span>);
        <span class="kw">let</span> <span class="ident">backend</span> <span class="op">=</span> <span class="ident">native_backend</span>();

        <span class="kw">let</span> <span class="kw-2">ref</span> <span class="ident">w_shape</span> <span class="op">=</span> (<span class="number">3</span>, <span class="number">4</span>);
        <span class="kw">let</span> <span class="kw-2">ref</span> <span class="ident">x_shape</span> <span class="op">=</span> (<span class="number">1</span>, <span class="number">4</span>);
        <span class="kw">let</span> <span class="kw-2">ref</span> <span class="ident">output_shape</span> <span class="op">=</span> (<span class="number">1</span>, <span class="number">3</span>);
        <span class="kw">let</span> <span class="ident">b_shape</span> <span class="op">=</span> <span class="ident">output_shape</span>;

        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">w</span> <span class="op">=</span> <span class="ident">SharedTensor</span>::<span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>::<span class="ident">new</span>(<span class="ident">w_shape</span>);
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">x</span> <span class="op">=</span> <span class="ident">SharedTensor</span>::<span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>::<span class="ident">new</span>(<span class="ident">x_shape</span>);
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">b</span> <span class="op">=</span> <span class="ident">SharedTensor</span>::<span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>::<span class="ident">new</span>(<span class="ident">b_shape</span>);

        <span class="ident">w</span>.<span class="ident">write_only</span>(<span class="ident">backend</span>.<span class="ident">device</span>()).<span class="ident">unwrap</span>().<span class="ident">as_mut_slice</span>().<span class="ident">copy_from_slice</span>(<span class="ident">get_sample_w</span>());
        <span class="ident">x</span>.<span class="ident">write_only</span>(<span class="ident">backend</span>.<span class="ident">device</span>()).<span class="ident">unwrap</span>().<span class="ident">as_mut_slice</span>().<span class="ident">copy_from_slice</span>(<span class="ident">get_sample_x</span>());
        <span class="ident">b</span>.<span class="ident">write_only</span>(<span class="ident">backend</span>.<span class="ident">device</span>()).<span class="ident">unwrap</span>().<span class="ident">as_mut_slice</span>().<span class="ident">copy_from_slice</span>(<span class="ident">get_sample_b</span>());

        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">output</span> <span class="op">=</span> <span class="ident">SharedTensor</span>::<span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>::<span class="ident">new</span>(<span class="ident">output_shape</span>);

        <span class="ident">layer</span>.<span class="ident">compute_output</span>(<span class="kw-2">&amp;</span><span class="ident">backend</span>, <span class="kw-2">&amp;</span>[<span class="kw-2">&amp;</span><span class="ident">w</span>, <span class="kw-2">&amp;</span><span class="ident">b</span>], <span class="kw-2">&amp;</span>[<span class="kw-2">&amp;</span><span class="ident">x</span>], <span class="kw-2">&amp;</span><span class="kw-2">mut</span> [<span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="ident">output</span>]);

        <span class="kw">let</span> <span class="ident">result_slice</span>: <span class="kw-2">&amp;</span>[<span class="ident">f32</span>] <span class="op">=</span> <span class="ident">output</span>.<span class="ident">read</span>(<span class="ident">backend</span>.<span class="ident">device</span>()).<span class="ident">unwrap</span>().<span class="ident">as_slice</span>();
        <span class="macro">assert_eq</span><span class="macro">!</span>(<span class="ident">result_slice</span>, <span class="kw-2">&amp;</span>[<span class="number">9f32</span>, <span class="number">16.5f32</span>, <span class="number">24.5f32</span>])
    }

    <span class="attribute">#[<span class="ident">test</span>]</span>
    <span class="kw">fn</span> <span class="ident">input_gradient_test</span>() {
        <span class="kw">let</span> <span class="kw-2">ref</span> <span class="ident">config</span> <span class="op">=</span> <span class="ident">LinearConfig</span> { <span class="ident">output_size</span>: <span class="number">3</span> };
        <span class="kw">let</span> <span class="ident">layer</span> <span class="op">=</span> <span class="ident">Linear</span>::<span class="ident">from_config</span>(<span class="ident">config</span>);
        <span class="kw">let</span> <span class="ident">backend</span> <span class="op">=</span> <span class="ident">native_backend</span>();

        <span class="kw">let</span> <span class="kw-2">ref</span> <span class="ident">w_shape</span> <span class="op">=</span> (<span class="number">3</span>, <span class="number">4</span>);
        <span class="kw">let</span> <span class="kw-2">ref</span> <span class="ident">x_shape</span> <span class="op">=</span> (<span class="number">1</span>, <span class="number">4</span>);
        <span class="kw">let</span> <span class="kw-2">ref</span> <span class="ident">output_shape</span> <span class="op">=</span> (<span class="number">1</span>, <span class="number">3</span>);
        <span class="kw">let</span> <span class="ident">b_shape</span> <span class="op">=</span> <span class="ident">output_shape</span>;

        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">w</span> <span class="op">=</span> <span class="ident">SharedTensor</span>::<span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>::<span class="ident">new</span>(<span class="ident">w_shape</span>);
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">x</span> <span class="op">=</span> <span class="ident">SharedTensor</span>::<span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>::<span class="ident">new</span>(<span class="ident">x_shape</span>);
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">b</span> <span class="op">=</span> <span class="ident">SharedTensor</span>::<span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>::<span class="ident">new</span>(<span class="ident">b_shape</span>);

        <span class="ident">w</span>.<span class="ident">write_only</span>(<span class="ident">backend</span>.<span class="ident">device</span>()).<span class="ident">unwrap</span>().<span class="ident">as_mut_slice</span>().<span class="ident">copy_from_slice</span>(<span class="ident">get_sample_w</span>());
        <span class="ident">x</span>.<span class="ident">write_only</span>(<span class="ident">backend</span>.<span class="ident">device</span>()).<span class="ident">unwrap</span>().<span class="ident">as_mut_slice</span>().<span class="ident">copy_from_slice</span>(<span class="ident">get_sample_x</span>());
        <span class="ident">b</span>.<span class="ident">write_only</span>(<span class="ident">backend</span>.<span class="ident">device</span>()).<span class="ident">unwrap</span>().<span class="ident">as_mut_slice</span>().<span class="ident">copy_from_slice</span>(<span class="ident">get_sample_b</span>());

        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">input_gradient</span> <span class="op">=</span> <span class="ident">SharedTensor</span>::<span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>::<span class="ident">new</span>(<span class="ident">x_shape</span>);
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">output_gradient</span> <span class="op">=</span> <span class="ident">SharedTensor</span>::<span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>::<span class="ident">new</span>(<span class="ident">output_shape</span>);
        <span class="ident">output_gradient</span>.<span class="ident">write_only</span>(<span class="ident">backend</span>.<span class="ident">device</span>()).<span class="ident">unwrap</span>().<span class="ident">as_mut_slice</span>().<span class="ident">copy_from_slice</span>(<span class="ident">get_sample_output_gradient</span>());
        <span class="comment">// The output_data tensor doesn&#39;t really matter since it&#39;s not used.</span>
        <span class="kw">let</span> <span class="ident">output_data</span> <span class="op">=</span> <span class="ident">SharedTensor</span>::<span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>::<span class="ident">new</span>(<span class="kw-2">&amp;</span>(<span class="number">1</span>, <span class="number">1</span>));

        <span class="ident">layer</span>.<span class="ident">compute_input_gradient</span>(
            <span class="kw-2">&amp;</span><span class="ident">backend</span>,
            <span class="kw-2">&amp;</span>[<span class="kw-2">&amp;</span><span class="ident">w</span>, <span class="kw-2">&amp;</span><span class="ident">b</span>],
            <span class="kw-2">&amp;</span>[<span class="kw-2">&amp;</span><span class="ident">output_data</span>],
            <span class="kw-2">&amp;</span>[<span class="kw-2">&amp;</span><span class="ident">output_gradient</span>],
            <span class="kw-2">&amp;</span>[<span class="kw-2">&amp;</span><span class="ident">x</span>],
            <span class="kw-2">&amp;</span><span class="kw-2">mut</span> [<span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="ident">input_gradient</span>]
        );

        <span class="kw">let</span> <span class="ident">result_slice</span>: <span class="kw-2">&amp;</span>[<span class="ident">f32</span>] <span class="op">=</span> <span class="ident">input_gradient</span>.<span class="ident">read</span>(<span class="ident">backend</span>.<span class="ident">device</span>()).<span class="ident">unwrap</span>().<span class="ident">as_slice</span>();
        <span class="macro">assert_eq</span><span class="macro">!</span>(<span class="ident">result_slice</span>, <span class="kw-2">&amp;</span>[<span class="op">-</span><span class="number">0.25f32</span>, <span class="number">2.4f32</span>, <span class="op">-</span><span class="number">1.7f32</span>, <span class="number">0.8f32</span>]);
    }

    <span class="attribute">#[<span class="ident">test</span>]</span>
    <span class="kw">fn</span> <span class="ident">parameter_gradient_test</span>() {
        <span class="kw">let</span> <span class="kw-2">ref</span> <span class="ident">config</span> <span class="op">=</span> <span class="ident">LinearConfig</span> { <span class="ident">output_size</span>: <span class="number">3</span> };
        <span class="kw">let</span> <span class="ident">layer</span> <span class="op">=</span> <span class="ident">Linear</span>::<span class="ident">from_config</span>(<span class="ident">config</span>);
        <span class="kw">let</span> <span class="ident">backend</span> <span class="op">=</span> <span class="ident">native_backend</span>();

        <span class="kw">let</span> <span class="kw-2">ref</span> <span class="ident">w_shape</span> <span class="op">=</span> (<span class="number">3</span>, <span class="number">4</span>);
        <span class="kw">let</span> <span class="kw-2">ref</span> <span class="ident">x_shape</span> <span class="op">=</span> (<span class="number">1</span>, <span class="number">4</span>);
        <span class="kw">let</span> <span class="kw-2">ref</span> <span class="ident">output_shape</span> <span class="op">=</span> (<span class="number">1</span>, <span class="number">3</span>);
        <span class="kw">let</span> <span class="ident">b_shape</span> <span class="op">=</span> <span class="ident">output_shape</span>;

        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">w_grad</span> <span class="op">=</span> <span class="ident">SharedTensor</span>::<span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>::<span class="ident">new</span>(<span class="ident">w_shape</span>);
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">x</span> <span class="op">=</span> <span class="ident">SharedTensor</span>::<span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>::<span class="ident">new</span>(<span class="ident">x_shape</span>);
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">b_grad</span> <span class="op">=</span> <span class="ident">SharedTensor</span>::<span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>::<span class="ident">new</span>(<span class="ident">b_shape</span>);

        <span class="ident">x</span>.<span class="ident">write_only</span>(<span class="ident">backend</span>.<span class="ident">device</span>()).<span class="ident">unwrap</span>().<span class="ident">as_mut_slice</span>().<span class="ident">copy_from_slice</span>(<span class="ident">get_sample_x</span>());

        <span class="kw">let</span> <span class="ident">input_gradient</span> <span class="op">=</span> <span class="ident">SharedTensor</span>::<span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>::<span class="ident">new</span>(<span class="ident">x_shape</span>);
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">output_gradient</span> <span class="op">=</span> <span class="ident">SharedTensor</span>::<span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>::<span class="ident">new</span>(<span class="ident">output_shape</span>);
        <span class="ident">output_gradient</span>.<span class="ident">write_only</span>(<span class="ident">backend</span>.<span class="ident">device</span>()).<span class="ident">unwrap</span>().<span class="ident">as_mut_slice</span>().<span class="ident">copy_from_slice</span>(<span class="ident">get_sample_output_gradient</span>());
        <span class="comment">// The output_data tensor doesn&#39;t really matter since it&#39;s not used.</span>
        <span class="kw">let</span> <span class="ident">output_data</span> <span class="op">=</span> <span class="ident">SharedTensor</span>::<span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>::<span class="ident">new</span>(<span class="kw-2">&amp;</span>(<span class="number">1</span>, <span class="number">1</span>));

        <span class="ident">layer</span>.<span class="ident">compute_parameters_gradient</span>(
            <span class="kw-2">&amp;</span><span class="ident">backend</span>,
            <span class="kw-2">&amp;</span>[<span class="kw-2">&amp;</span><span class="ident">output_data</span>],
            <span class="kw-2">&amp;</span>[<span class="kw-2">&amp;</span><span class="ident">output_gradient</span>],
            <span class="kw-2">&amp;</span>[<span class="kw-2">&amp;</span><span class="ident">x</span>],
            <span class="kw-2">&amp;</span><span class="kw-2">mut</span> [<span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="ident">w_grad</span>, <span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="ident">b_grad</span>]
        );

        <span class="kw">let</span> <span class="ident">w_grad_result</span>: <span class="kw-2">&amp;</span>[<span class="ident">f32</span>] <span class="op">=</span> <span class="ident">w_grad</span>.<span class="ident">read</span>(<span class="ident">backend</span>.<span class="ident">device</span>()).<span class="ident">unwrap</span>().<span class="ident">as_slice</span>();
        <span class="kw">let</span> <span class="ident">b_grad_result</span>: <span class="kw-2">&amp;</span>[<span class="ident">f32</span>] <span class="op">=</span> <span class="ident">b_grad</span>.<span class="ident">read</span>(<span class="ident">backend</span>.<span class="ident">device</span>()).<span class="ident">unwrap</span>().<span class="ident">as_slice</span>();

        <span class="macro">assert_eq</span><span class="macro">!</span>(<span class="ident">w_grad_result</span>, <span class="kw-2">&amp;</span>[
            <span class="op">-</span><span class="number">1f32</span>, <span class="op">-</span><span class="number">2f32</span>, <span class="op">-</span><span class="number">3f32</span>, <span class="op">-</span><span class="number">4f32</span>,
            <span class="number">0.5f32</span>, <span class="number">1f32</span>, <span class="number">1.5f32</span>, <span class="number">2f32</span>,
            <span class="number">0.2f32</span>, <span class="number">0.4f32</span>, <span class="number">0.6f32</span>, <span class="number">0.8f32</span>
        ]);
        <span class="macro">assert_eq</span><span class="macro">!</span>(<span class="ident">b_grad_result</span>, <span class="kw-2">&amp;</span>[<span class="op">-</span><span class="number">1f32</span>, <span class="number">0.5f32</span>, <span class="number">0.2f32</span>]);
    }
}</pre></div>
</section><section id="search" class="content hidden"></section><section class="footer"></section><script>window.rootPath = "../../../../";window.currentCrate = "juice";</script><script src="../../../../aliases.js"></script><script src="../../../../main.js"></script><script src="../../../../source-script.js"></script><script src="../../../../source-files.js"></script><script defer src="../../../../search-index.js"></script></body></html>