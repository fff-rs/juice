<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="rustdoc"><meta name="description" content="Source of the Rust file `juice/src/layers/activation/relu.rs`."><meta name="keywords" content="rust, rustlang, rust-lang"><title>relu.rs - source</title><link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../SourceSerif4-Regular.ttf.woff2"><link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../FiraSans-Regular.woff2"><link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../FiraSans-Medium.woff2"><link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../SourceCodePro-Regular.ttf.woff2"><link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../SourceSerif4-Bold.ttf.woff2"><link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../SourceCodePro-Semibold.ttf.woff2"><link rel="stylesheet" type="text/css" href="../../../../normalize.css"><link rel="stylesheet" type="text/css" href="../../../../rustdoc.css" id="mainThemeStyle"><link rel="stylesheet" type="text/css" href="../../../../ayu.css" disabled><link rel="stylesheet" type="text/css" href="../../../../dark.css" disabled><link rel="stylesheet" type="text/css" href="../../../../light.css" id="themeStyle"><script id="default-settings" ></script><script src="../../../../storage.js"></script><script defer src="../../../../source-script.js"></script><script defer src="../../../../source-files.js"></script><script defer src="../../../../main.js"></script><noscript><link rel="stylesheet" href="../../../../noscript.css"></noscript><link rel="alternate icon" type="image/png" href="../../../../favicon-16x16.png"><link rel="alternate icon" type="image/png" href="../../../../favicon-32x32.png"><link rel="icon" type="image/svg+xml" href="../../../../favicon.svg"></head><body class="rustdoc source"><!--[if lte IE 11]><div class="warning">This old browser is unsupported and will most likely display funky things.</div><![endif]--><nav class="mobile-topbar"><button class="sidebar-menu-toggle">&#9776;</button><a class="sidebar-logo" href="../../../../juice/index.html"><div class="logo-container"><img class="rust-logo" src="../../../../rust-logo.svg" alt="logo"></div>
        </a><h2 class="location"></h2>
    </nav>
    <nav class="sidebar"><a class="sidebar-logo" href="../../../../juice/index.html"><div class="logo-container"><img class="rust-logo" src="../../../../rust-logo.svg" alt="logo"></div>
        </a></nav><main><div class="width-limiter"><div class="sub-container"><a class="sub-logo-container" href="../../../../juice/index.html"><img class="rust-logo" src="../../../../rust-logo.svg" alt="logo"></a><nav class="sub"><form class="search-form"><div class="search-container"><span></span><input class="search-input" name="search" autocomplete="off" spellcheck="false" placeholder="Click or press ‘S’ to search, ‘?’ for more options…" type="search"><button type="button" id="help-button" title="help">?</button><div id="settings-menu" tabindex="-1">
                                <a href="../../../../settings.html" title="settings"><img width="22" height="22" alt="Change settings" src="../../../../wheel.svg"></a></div>
                        </div></form></nav></div><section id="main-content" class="content"><div class="example-wrap"><pre class="line-numbers"><span id="1">1</span>
<span id="2">2</span>
<span id="3">3</span>
<span id="4">4</span>
<span id="5">5</span>
<span id="6">6</span>
<span id="7">7</span>
<span id="8">8</span>
<span id="9">9</span>
<span id="10">10</span>
<span id="11">11</span>
<span id="12">12</span>
<span id="13">13</span>
<span id="14">14</span>
<span id="15">15</span>
<span id="16">16</span>
<span id="17">17</span>
<span id="18">18</span>
<span id="19">19</span>
<span id="20">20</span>
<span id="21">21</span>
<span id="22">22</span>
<span id="23">23</span>
<span id="24">24</span>
<span id="25">25</span>
<span id="26">26</span>
<span id="27">27</span>
<span id="28">28</span>
<span id="29">29</span>
<span id="30">30</span>
<span id="31">31</span>
<span id="32">32</span>
<span id="33">33</span>
<span id="34">34</span>
<span id="35">35</span>
<span id="36">36</span>
<span id="37">37</span>
<span id="38">38</span>
<span id="39">39</span>
<span id="40">40</span>
<span id="41">41</span>
<span id="42">42</span>
<span id="43">43</span>
<span id="44">44</span>
<span id="45">45</span>
<span id="46">46</span>
<span id="47">47</span>
<span id="48">48</span>
<span id="49">49</span>
<span id="50">50</span>
<span id="51">51</span>
<span id="52">52</span>
<span id="53">53</span>
<span id="54">54</span>
<span id="55">55</span>
<span id="56">56</span>
<span id="57">57</span>
<span id="58">58</span>
<span id="59">59</span>
<span id="60">60</span>
<span id="61">61</span>
<span id="62">62</span>
<span id="63">63</span>
<span id="64">64</span>
<span id="65">65</span>
<span id="66">66</span>
<span id="67">67</span>
<span id="68">68</span>
<span id="69">69</span>
<span id="70">70</span>
<span id="71">71</span>
<span id="72">72</span>
<span id="73">73</span>
<span id="74">74</span>
<span id="75">75</span>
<span id="76">76</span>
<span id="77">77</span>
<span id="78">78</span>
<span id="79">79</span>
<span id="80">80</span>
<span id="81">81</span>
<span id="82">82</span>
<span id="83">83</span>
<span id="84">84</span>
</pre><pre class="rust"><code><span class="doccomment">//! Applies the nonlinear Rectified Linear Unit.</span>
<span class="doccomment">//!</span>
<span class="doccomment">//! Non-linearity activation function: y = max(0, x)</span>
<span class="doccomment">//!</span>
<span class="doccomment">//! This is generally the preferred choice over Sigmod or TanH.</span>
<span class="doccomment">//! The max function used in ReLU is usually faster to compute than the exponentiation</span>
<span class="doccomment">//! needed in a Sigmoid layer.</span>

<span class="kw">use</span> <span class="ident"><span class="kw">crate</span>::co</span>::{<span class="ident">IBackend</span>, <span class="ident">SharedTensor</span>};
<span class="kw">use</span> <span class="ident"><span class="kw">crate</span>::conn::Relu</span>;
<span class="kw">use</span> <span class="ident"><span class="kw">crate</span>::conn::ReluPointwise</span>;
<span class="kw">use</span> <span class="ident"><span class="kw">crate</span>::layer</span>::<span class="kw-2">*</span>;
<span class="kw">use</span> <span class="ident"><span class="kw">crate</span>::util::ArcLock</span>;

<span class="attribute">#[<span class="ident">derive</span>(<span class="ident">Debug</span>, <span class="ident">Clone</span>)]</span>
<span class="attribute">#[<span class="ident">allow</span>(<span class="ident">missing_copy_implementations</span>)]</span>
<span class="doccomment">/// ReLU Activation Layer</span>
<span class="kw">pub</span> <span class="kw">struct</span> <span class="ident">ReLU</span>;

<span class="comment">//</span>
<span class="comment">// ReLU + ReLUPointwise</span>
<span class="comment">//</span>
<span class="kw">impl</span><span class="op">&lt;</span><span class="ident">B</span>: <span class="ident">IBackend</span> <span class="op">+</span> <span class="ident">Relu</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span> <span class="op">+</span> <span class="ident">ReluPointwise</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span> <span class="ident">ILayer</span><span class="op">&lt;</span><span class="ident">B</span><span class="op">&gt;</span> <span class="kw">for</span> <span class="ident">ReLU</span> {
    <span class="macro">impl_ilayer_activation!</span>();

    <span class="kw">fn</span> <span class="ident">compute_in_place</span>(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; <span class="ident">bool</span> {
        <span class="bool-val">true</span>
    }

    <span class="kw">fn</span> <span class="ident">reshape</span>(
        <span class="kw-2">&amp;mut</span> <span class="self">self</span>,
        <span class="ident">backend</span>: <span class="ident">::std::rc::Rc</span><span class="op">&lt;</span><span class="ident">B</span><span class="op">&gt;</span>,
        <span class="ident">input_data</span>: <span class="kw-2">&amp;mut</span> <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span>,
        <span class="ident">input_gradient</span>: <span class="kw-2">&amp;mut</span> <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span>,
        <span class="ident">weights_data</span>: <span class="kw-2">&amp;mut</span> <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span>,
        <span class="ident">weights_gradient</span>: <span class="kw-2">&amp;mut</span> <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span>,
        <span class="ident">output_data</span>: <span class="kw-2">&amp;mut</span> <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span>,
        <span class="ident">output_gradient</span>: <span class="kw-2">&amp;mut</span> <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">ArcLock</span><span class="op">&lt;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span><span class="op">&gt;</span>,
    ) {
        <span class="kw">if</span> <span class="kw">let</span> <span class="prelude-val">Some</span>(<span class="ident">inp</span>) <span class="op">=</span> <span class="ident">input_data</span>.<span class="ident">get</span>(<span class="number">0</span>) {
            <span class="kw">let</span> <span class="ident">read_inp</span> <span class="op">=</span> <span class="ident">inp</span>.<span class="ident">read</span>().<span class="ident">unwrap</span>();
            <span class="kw">let</span> <span class="ident">input_desc</span> <span class="op">=</span> <span class="ident">read_inp</span>.<span class="ident">desc</span>();
            <span class="ident">input_gradient</span>[<span class="number">0</span>].<span class="ident">write</span>().<span class="ident">unwrap</span>().<span class="ident">resize</span>(<span class="ident">input_desc</span>).<span class="ident">unwrap</span>();
            <span class="ident">output_data</span>[<span class="number">0</span>].<span class="ident">write</span>().<span class="ident">unwrap</span>().<span class="ident">resize</span>(<span class="ident">input_desc</span>).<span class="ident">unwrap</span>();
            <span class="ident">output_gradient</span>[<span class="number">0</span>].<span class="ident">write</span>().<span class="ident">unwrap</span>().<span class="ident">resize</span>(<span class="ident">input_desc</span>).<span class="ident">unwrap</span>();
        }
    }
}

<span class="kw">impl</span><span class="op">&lt;</span><span class="ident">B</span>: <span class="ident">IBackend</span> <span class="op">+</span> <span class="ident">Relu</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span> <span class="op">+</span> <span class="ident">ReluPointwise</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span> <span class="ident">ComputeOutput</span><span class="op">&lt;</span><span class="ident">f32</span>, <span class="ident">B</span><span class="op">&gt;</span> <span class="kw">for</span> <span class="ident">ReLU</span> {
    <span class="kw">fn</span> <span class="ident">compute_output</span>(
        <span class="kw-2">&amp;</span><span class="self">self</span>,
        <span class="ident">backend</span>: <span class="kw-2">&amp;</span><span class="ident">B</span>,
        <span class="ident">_weights</span>: <span class="kw-2">&amp;</span>[<span class="kw-2">&amp;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>],
        <span class="ident">input_data</span>: <span class="kw-2">&amp;</span>[<span class="kw-2">&amp;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>],
        <span class="ident">output_data</span>: <span class="kw-2">&amp;mut</span> [<span class="kw-2">&amp;mut</span> <span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>],
    ) {
        <span class="kw">match</span> <span class="ident">input_data</span>.<span class="ident">get</span>(<span class="number">0</span>) {
            <span class="prelude-val">Some</span>(<span class="ident">input</span>) =&gt; <span class="ident">backend</span>.<span class="ident">relu</span>(<span class="ident">input</span>, <span class="ident">output_data</span>[<span class="number">0</span>]).<span class="ident">unwrap</span>(),
            <span class="prelude-val">None</span> =&gt; <span class="ident">backend</span>.<span class="ident">relu_pointwise</span>(<span class="ident">output_data</span>[<span class="number">0</span>]).<span class="ident">unwrap</span>(),
        }
    }
}

<span class="kw">impl</span><span class="op">&lt;</span><span class="ident">B</span>: <span class="ident">IBackend</span> <span class="op">+</span> <span class="ident">Relu</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span> <span class="op">+</span> <span class="ident">ReluPointwise</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span> <span class="ident">ComputeInputGradient</span><span class="op">&lt;</span><span class="ident">f32</span>, <span class="ident">B</span><span class="op">&gt;</span> <span class="kw">for</span> <span class="ident">ReLU</span> {
    <span class="kw">fn</span> <span class="ident">compute_input_gradient</span>(
        <span class="kw-2">&amp;</span><span class="self">self</span>,
        <span class="ident">backend</span>: <span class="kw-2">&amp;</span><span class="ident">B</span>,
        <span class="ident">weights_data</span>: <span class="kw-2">&amp;</span>[<span class="kw-2">&amp;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>],
        <span class="ident">output_data</span>: <span class="kw-2">&amp;</span>[<span class="kw-2">&amp;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>],
        <span class="ident">output_gradients</span>: <span class="kw-2">&amp;</span>[<span class="kw-2">&amp;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>],
        <span class="ident">input_data</span>: <span class="kw-2">&amp;</span>[<span class="kw-2">&amp;</span><span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>],
        <span class="ident">input_gradients</span>: <span class="kw-2">&amp;mut</span> [<span class="kw-2">&amp;mut</span> <span class="ident">SharedTensor</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span>],
    ) {
        <span class="kw">match</span> <span class="ident">output_data</span>.<span class="ident">get</span>(<span class="number">0</span>) {
            <span class="prelude-val">Some</span>(<span class="kw">_</span>) =&gt; <span class="ident">backend</span>
                .<span class="ident">relu_grad</span>(<span class="ident">output_data</span>[<span class="number">0</span>], <span class="ident">output_gradients</span>[<span class="number">0</span>], <span class="ident">input_data</span>[<span class="number">0</span>], <span class="ident">input_gradients</span>[<span class="number">0</span>])
                .<span class="ident">unwrap</span>(),
            <span class="prelude-val">None</span> =&gt; <span class="ident">backend</span>.<span class="ident">relu_pointwise_grad</span>(<span class="ident">input_data</span>[<span class="number">0</span>], <span class="ident">input_gradients</span>[<span class="number">0</span>]).<span class="ident">unwrap</span>(),
        }
    }
}

<span class="kw">impl</span><span class="op">&lt;</span><span class="ident">B</span>: <span class="ident">IBackend</span> <span class="op">+</span> <span class="ident">Relu</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span> <span class="op">+</span> <span class="ident">ReluPointwise</span><span class="op">&lt;</span><span class="ident">f32</span><span class="op">&gt;</span><span class="op">&gt;</span> <span class="ident">ComputeParametersGradient</span><span class="op">&lt;</span><span class="ident">f32</span>, <span class="ident">B</span><span class="op">&gt;</span> <span class="kw">for</span> <span class="ident">ReLU</span> {}
</code></pre></div>
</section></div></main><div id="rustdoc-vars" data-root-path="../../../../" data-current-crate="juice" data-themes="ayu,dark,light" data-resource-suffix="" data-rustdoc-version="1.63.0 (4b91a6ea7 2022-08-08)" ></div>
</body></html>