//! Defines the Foreign Function Interface for the CUDA cuDNN API.
#![allow(non_camel_case_types)]
#![allow(non_snake_case)]
#![allow(non_upper_case_globals)]

extern crate libc;

/* automatically generated by rust-bindgen */
#[repr(C)]
pub struct __BindgenUnionField<T>(::std::marker::PhantomData<T>);
impl <T> __BindgenUnionField<T> {
    #[inline]
    pub fn new() -> Self { __BindgenUnionField(::std::marker::PhantomData) }
    #[inline]
    pub unsafe fn as_ref(&self) -> &T { ::std::mem::transmute(self) }
    #[inline]
    pub unsafe fn as_mut(&mut self) -> &mut T { ::std::mem::transmute(self) }
}
impl <T> ::std::default::Default for __BindgenUnionField<T> {
    #[inline]
    fn default() -> Self { Self::new() }
}
impl <T> ::std::clone::Clone for __BindgenUnionField<T> {
    #[inline]
    fn clone(&self) -> Self { Self::new() }
}
impl <T> ::std::marker::Copy for __BindgenUnionField<T> { }
impl <T> ::std::fmt::Debug for __BindgenUnionField<T> {
    fn fmt(&self, fmt: &mut ::std::fmt::Formatter) -> ::std::fmt::Result {
        fmt.write_str("__BindgenUnionField")
    }
}
pub const CUDNN_MAJOR: ::libc::c_uint = 5;
pub const CUDNN_MINOR: ::libc::c_uint = 1;
pub const CUDNN_PATCHLEVEL: ::libc::c_uint = 5;
pub const CUDNN_VERSION: ::libc::c_uint = 5105;
pub const cudaHostAllocDefault: ::libc::c_uint = 0;
pub const cudaHostAllocPortable: ::libc::c_uint = 1;
pub const cudaHostAllocMapped: ::libc::c_uint = 2;
pub const cudaHostAllocWriteCombined: ::libc::c_uint = 4;
pub const cudaHostRegisterDefault: ::libc::c_uint = 0;
pub const cudaHostRegisterPortable: ::libc::c_uint = 1;
pub const cudaHostRegisterMapped: ::libc::c_uint = 2;
pub const cudaHostRegisterIoMemory: ::libc::c_uint = 4;
pub const cudaPeerAccessDefault: ::libc::c_uint = 0;
pub const cudaStreamDefault: ::libc::c_uint = 0;
pub const cudaStreamNonBlocking: ::libc::c_uint = 1;
pub const cudaEventDefault: ::libc::c_uint = 0;
pub const cudaEventBlockingSync: ::libc::c_uint = 1;
pub const cudaEventDisableTiming: ::libc::c_uint = 2;
pub const cudaEventInterprocess: ::libc::c_uint = 4;
pub const cudaDeviceScheduleAuto: ::libc::c_uint = 0;
pub const cudaDeviceScheduleSpin: ::libc::c_uint = 1;
pub const cudaDeviceScheduleYield: ::libc::c_uint = 2;
pub const cudaDeviceScheduleBlockingSync: ::libc::c_uint = 4;
pub const cudaDeviceBlockingSync: ::libc::c_uint = 4;
pub const cudaDeviceScheduleMask: ::libc::c_uint = 7;
pub const cudaDeviceMapHost: ::libc::c_uint = 8;
pub const cudaDeviceLmemResizeToMax: ::libc::c_uint = 16;
pub const cudaDeviceMask: ::libc::c_uint = 31;
pub const cudaArrayDefault: ::libc::c_uint = 0;
pub const cudaArrayLayered: ::libc::c_uint = 1;
pub const cudaArraySurfaceLoadStore: ::libc::c_uint = 2;
pub const cudaArrayCubemap: ::libc::c_uint = 4;
pub const cudaArrayTextureGather: ::libc::c_uint = 8;
pub const cudaIpcMemLazyEnablePeerAccess: ::libc::c_uint = 1;
pub const cudaMemAttachGlobal: ::libc::c_uint = 1;
pub const cudaMemAttachHost: ::libc::c_uint = 2;
pub const cudaMemAttachSingle: ::libc::c_uint = 4;
pub const cudaOccupancyDefault: ::libc::c_uint = 0;
pub const cudaOccupancyDisableCachingOverride: ::libc::c_uint = 1;
pub const CUDA_IPC_HANDLE_SIZE: ::libc::c_uint = 64;
pub const cudaSurfaceType1D: ::libc::c_uint = 1;
pub const cudaSurfaceType2D: ::libc::c_uint = 2;
pub const cudaSurfaceType3D: ::libc::c_uint = 3;
pub const cudaSurfaceTypeCubemap: ::libc::c_uint = 12;
pub const cudaSurfaceType1DLayered: ::libc::c_uint = 241;
pub const cudaSurfaceType2DLayered: ::libc::c_uint = 242;
pub const cudaSurfaceTypeCubemapLayered: ::libc::c_uint = 252;
pub const cudaTextureType1D: ::libc::c_uint = 1;
pub const cudaTextureType2D: ::libc::c_uint = 2;
pub const cudaTextureType3D: ::libc::c_uint = 3;
pub const cudaTextureTypeCubemap: ::libc::c_uint = 12;
pub const cudaTextureType1DLayered: ::libc::c_uint = 241;
pub const cudaTextureType2DLayered: ::libc::c_uint = 242;
pub const cudaTextureTypeCubemapLayered: ::libc::c_uint = 252;
pub const CUDART_VERSION: ::libc::c_uint = 8000;
pub const CUDNN_DIM_MAX: ::libc::c_uint = 8;
pub const CUDNN_LRN_MIN_N: ::libc::c_uint = 1;
pub const CUDNN_LRN_MAX_N: ::libc::c_uint = 16;
pub const CUDNN_LRN_MIN_K: f32 = 0.00001;
pub const CUDNN_LRN_MIN_BETA: f32 = 0.01;
pub const CUDNN_BN_MIN_EPSILON: f32 = 0.00001;
pub type wchar_t = ::libc::c_int;
#[repr(C)]
#[derive(Debug, Copy)]
pub struct _bindgen_ty_1 {
    pub __clang_max_align_nonce1: ::libc::c_longlong,
    pub __clang_max_align_nonce2: f64,
}
impl Clone for _bindgen_ty_1 {
    fn clone(&self) -> Self { *self }
}
pub type max_align_t = _bindgen_ty_1;
#[repr(u32)]
/**
 * CUDA error types
 */
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum cudaError {
    cudaSuccess = 0,
    cudaErrorMissingConfiguration = 1,
    cudaErrorMemoryAllocation = 2,
    cudaErrorInitializationError = 3,
    cudaErrorLaunchFailure = 4,
    cudaErrorPriorLaunchFailure = 5,
    cudaErrorLaunchTimeout = 6,
    cudaErrorLaunchOutOfResources = 7,
    cudaErrorInvalidDeviceFunction = 8,
    cudaErrorInvalidConfiguration = 9,
    cudaErrorInvalidDevice = 10,
    cudaErrorInvalidValue = 11,
    cudaErrorInvalidPitchValue = 12,
    cudaErrorInvalidSymbol = 13,
    cudaErrorMapBufferObjectFailed = 14,
    cudaErrorUnmapBufferObjectFailed = 15,
    cudaErrorInvalidHostPointer = 16,
    cudaErrorInvalidDevicePointer = 17,
    cudaErrorInvalidTexture = 18,
    cudaErrorInvalidTextureBinding = 19,
    cudaErrorInvalidChannelDescriptor = 20,
    cudaErrorInvalidMemcpyDirection = 21,
    cudaErrorAddressOfConstant = 22,
    cudaErrorTextureFetchFailed = 23,
    cudaErrorTextureNotBound = 24,
    cudaErrorSynchronizationError = 25,
    cudaErrorInvalidFilterSetting = 26,
    cudaErrorInvalidNormSetting = 27,
    cudaErrorMixedDeviceExecution = 28,
    cudaErrorCudartUnloading = 29,
    cudaErrorUnknown = 30,
    cudaErrorNotYetImplemented = 31,
    cudaErrorMemoryValueTooLarge = 32,
    cudaErrorInvalidResourceHandle = 33,
    cudaErrorNotReady = 34,
    cudaErrorInsufficientDriver = 35,
    cudaErrorSetOnActiveProcess = 36,
    cudaErrorInvalidSurface = 37,
    cudaErrorNoDevice = 38,
    cudaErrorECCUncorrectable = 39,
    cudaErrorSharedObjectSymbolNotFound = 40,
    cudaErrorSharedObjectInitFailed = 41,
    cudaErrorUnsupportedLimit = 42,
    cudaErrorDuplicateVariableName = 43,
    cudaErrorDuplicateTextureName = 44,
    cudaErrorDuplicateSurfaceName = 45,
    cudaErrorDevicesUnavailable = 46,
    cudaErrorInvalidKernelImage = 47,
    cudaErrorNoKernelImageForDevice = 48,
    cudaErrorIncompatibleDriverContext = 49,
    cudaErrorPeerAccessAlreadyEnabled = 50,
    cudaErrorPeerAccessNotEnabled = 51,
    cudaErrorDeviceAlreadyInUse = 54,
    cudaErrorProfilerDisabled = 55,
    cudaErrorProfilerNotInitialized = 56,
    cudaErrorProfilerAlreadyStarted = 57,
    cudaErrorProfilerAlreadyStopped = 58,
    cudaErrorAssert = 59,
    cudaErrorTooManyPeers = 60,
    cudaErrorHostMemoryAlreadyRegistered = 61,
    cudaErrorHostMemoryNotRegistered = 62,
    cudaErrorOperatingSystem = 63,
    cudaErrorPeerAccessUnsupported = 64,
    cudaErrorLaunchMaxDepthExceeded = 65,
    cudaErrorLaunchFileScopedTex = 66,
    cudaErrorLaunchFileScopedSurf = 67,
    cudaErrorSyncDepthExceeded = 68,
    cudaErrorLaunchPendingCountExceeded = 69,
    cudaErrorNotPermitted = 70,
    cudaErrorNotSupported = 71,
    cudaErrorHardwareStackError = 72,
    cudaErrorIllegalInstruction = 73,
    cudaErrorMisalignedAddress = 74,
    cudaErrorInvalidAddressSpace = 75,
    cudaErrorInvalidPc = 76,
    cudaErrorIllegalAddress = 77,
    cudaErrorInvalidPtx = 78,
    cudaErrorInvalidGraphicsContext = 79,
    cudaErrorNvlinkUncorrectable = 80,
    cudaErrorStartupFailure = 127,
    cudaErrorApiFailureBase = 10000,
}
#[repr(u32)]
/**
 * Channel format kind
 */
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum cudaChannelFormatKind {
    cudaChannelFormatKindSigned = 0,
    cudaChannelFormatKindUnsigned = 1,
    cudaChannelFormatKindFloat = 2,
    cudaChannelFormatKindNone = 3,
}
/**
 * CUDA Channel format descriptor
 */
#[repr(C)]
#[derive(Debug, Copy)]
pub struct cudaChannelFormatDesc {
    /**< x */
    pub x: ::libc::c_int,
    /**< y */
    pub y: ::libc::c_int,
    /**< z */
    pub z: ::libc::c_int,
    /**< w */
    pub w: ::libc::c_int,
    /**< Channel format kind */
    pub f: cudaChannelFormatKind,
}
#[test]
fn bindgen_test_layout_cudaChannelFormatDesc() {
    assert_eq!(::std::mem::size_of::<cudaChannelFormatDesc>() , 20usize);
    assert_eq!(::std::mem::align_of::<cudaChannelFormatDesc>() , 4usize);
}
impl Clone for cudaChannelFormatDesc {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudaArray([u8; 0]);
/**
 * CUDA array
 */
pub type cudaArray_t = *mut cudaArray;
/**
 * CUDA array (as source copy argument)
 */
pub type cudaArray_const_t = *const cudaArray;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudaMipmappedArray([u8; 0]);
/**
 * CUDA mipmapped array
 */
pub type cudaMipmappedArray_t = *mut cudaMipmappedArray;
/**
 * CUDA mipmapped array (as source argument)
 */
pub type cudaMipmappedArray_const_t = *const cudaMipmappedArray;
#[repr(u32)]
/**
 * CUDA memory types
 */
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum cudaMemoryType { cudaMemoryTypeHost = 1, cudaMemoryTypeDevice = 2, }
#[repr(u32)]
/**
 * CUDA memory copy types
 */
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum cudaMemcpyKind {
    cudaMemcpyHostToHost = 0,
    cudaMemcpyHostToDevice = 1,
    cudaMemcpyDeviceToHost = 2,
    cudaMemcpyDeviceToDevice = 3,
    cudaMemcpyDefault = 4,
}
/**
 * CUDA Pitched memory pointer
 *
 * \sa ::make_cudaPitchedPtr
 */
#[repr(C)]
#[derive(Debug, Copy)]
pub struct cudaPitchedPtr {
    /**< Pointer to allocated memory */
    pub ptr: *mut ::libc::c_void,
    /**< Pitch of allocated memory in bytes */
    pub pitch: usize,
    /**< Logical width of allocation in elements */
    pub xsize: usize,
    /**< Logical height of allocation in elements */
    pub ysize: usize,
}
#[test]
fn bindgen_test_layout_cudaPitchedPtr() {
    assert_eq!(::std::mem::size_of::<cudaPitchedPtr>() , 32usize);
    assert_eq!(::std::mem::align_of::<cudaPitchedPtr>() , 8usize);
}
impl Clone for cudaPitchedPtr {
    fn clone(&self) -> Self { *self }
}
/**
 * CUDA extent
 *
 * \sa ::make_cudaExtent
 */
#[repr(C)]
#[derive(Debug, Copy)]
pub struct cudaExtent {
    /**< Width in elements when referring to array memory, in bytes when referring to linear memory */
    pub width: usize,
    /**< Height in elements */
    pub height: usize,
    /**< Depth in elements */
    pub depth: usize,
}
#[test]
fn bindgen_test_layout_cudaExtent() {
    assert_eq!(::std::mem::size_of::<cudaExtent>() , 24usize);
    assert_eq!(::std::mem::align_of::<cudaExtent>() , 8usize);
}
impl Clone for cudaExtent {
    fn clone(&self) -> Self { *self }
}
/**
 * CUDA 3D position
 *
 * \sa ::make_cudaPos
 */
#[repr(C)]
#[derive(Debug, Copy)]
pub struct cudaPos {
    /**< x */
    pub x: usize,
    /**< y */
    pub y: usize,
    /**< z */
    pub z: usize,
}
#[test]
fn bindgen_test_layout_cudaPos() {
    assert_eq!(::std::mem::size_of::<cudaPos>() , 24usize);
    assert_eq!(::std::mem::align_of::<cudaPos>() , 8usize);
}
impl Clone for cudaPos {
    fn clone(&self) -> Self { *self }
}
/**
 * CUDA 3D memory copying parameters
 */
#[repr(C)]
#[derive(Debug, Copy)]
pub struct cudaMemcpy3DParms {
    /**< Source memory address */
    pub srcArray: cudaArray_t,
    /**< Source position offset */
    pub srcPos: cudaPos,
    /**< Pitched source memory address */
    pub srcPtr: cudaPitchedPtr,
    /**< Destination memory address */
    pub dstArray: cudaArray_t,
    /**< Destination position offset */
    pub dstPos: cudaPos,
    /**< Pitched destination memory address */
    pub dstPtr: cudaPitchedPtr,
    /**< Requested memory copy size */
    pub extent: cudaExtent,
    /**< Type of transfer */
    pub kind: cudaMemcpyKind,
}
#[test]
fn bindgen_test_layout_cudaMemcpy3DParms() {
    assert_eq!(::std::mem::size_of::<cudaMemcpy3DParms>() , 160usize);
    assert_eq!(::std::mem::align_of::<cudaMemcpy3DParms>() , 8usize);
}
impl Clone for cudaMemcpy3DParms {
    fn clone(&self) -> Self { *self }
}
/**
 * CUDA 3D cross-device memory copying parameters
 */
#[repr(C)]
#[derive(Debug, Copy)]
pub struct cudaMemcpy3DPeerParms {
    /**< Source memory address */
    pub srcArray: cudaArray_t,
    /**< Source position offset */
    pub srcPos: cudaPos,
    /**< Pitched source memory address */
    pub srcPtr: cudaPitchedPtr,
    /**< Source device */
    pub srcDevice: ::libc::c_int,
    /**< Destination memory address */
    pub dstArray: cudaArray_t,
    /**< Destination position offset */
    pub dstPos: cudaPos,
    /**< Pitched destination memory address */
    pub dstPtr: cudaPitchedPtr,
    /**< Destination device */
    pub dstDevice: ::libc::c_int,
    /**< Requested memory copy size */
    pub extent: cudaExtent,
}
#[test]
fn bindgen_test_layout_cudaMemcpy3DPeerParms() {
    assert_eq!(::std::mem::size_of::<cudaMemcpy3DPeerParms>() , 168usize);
    assert_eq!(::std::mem::align_of::<cudaMemcpy3DPeerParms>() , 8usize);
}
impl Clone for cudaMemcpy3DPeerParms {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudaGraphicsResource([u8; 0]);
#[repr(u32)]
/**
 * CUDA graphics interop register flags
 */
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum cudaGraphicsRegisterFlags {
    cudaGraphicsRegisterFlagsNone = 0,
    cudaGraphicsRegisterFlagsReadOnly = 1,
    cudaGraphicsRegisterFlagsWriteDiscard = 2,
    cudaGraphicsRegisterFlagsSurfaceLoadStore = 4,
    cudaGraphicsRegisterFlagsTextureGather = 8,
}
#[repr(u32)]
/**
 * CUDA graphics interop map flags
 */
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum cudaGraphicsMapFlags {
    cudaGraphicsMapFlagsNone = 0,
    cudaGraphicsMapFlagsReadOnly = 1,
    cudaGraphicsMapFlagsWriteDiscard = 2,
}
#[repr(u32)]
/**
 * CUDA graphics interop array indices for cube maps
 */
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum cudaGraphicsCubeFace {
    cudaGraphicsCubeFacePositiveX = 0,
    cudaGraphicsCubeFaceNegativeX = 1,
    cudaGraphicsCubeFacePositiveY = 2,
    cudaGraphicsCubeFaceNegativeY = 3,
    cudaGraphicsCubeFacePositiveZ = 4,
    cudaGraphicsCubeFaceNegativeZ = 5,
}
#[repr(u32)]
/**
 * CUDA resource types
 */
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum cudaResourceType {
    cudaResourceTypeArray = 0,
    cudaResourceTypeMipmappedArray = 1,
    cudaResourceTypeLinear = 2,
    cudaResourceTypePitch2D = 3,
}
#[repr(u32)]
/**
 * CUDA texture resource view formats
 */
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum cudaResourceViewFormat {
    cudaResViewFormatNone = 0,
    cudaResViewFormatUnsignedChar1 = 1,
    cudaResViewFormatUnsignedChar2 = 2,
    cudaResViewFormatUnsignedChar4 = 3,
    cudaResViewFormatSignedChar1 = 4,
    cudaResViewFormatSignedChar2 = 5,
    cudaResViewFormatSignedChar4 = 6,
    cudaResViewFormatUnsignedShort1 = 7,
    cudaResViewFormatUnsignedShort2 = 8,
    cudaResViewFormatUnsignedShort4 = 9,
    cudaResViewFormatSignedShort1 = 10,
    cudaResViewFormatSignedShort2 = 11,
    cudaResViewFormatSignedShort4 = 12,
    cudaResViewFormatUnsignedInt1 = 13,
    cudaResViewFormatUnsignedInt2 = 14,
    cudaResViewFormatUnsignedInt4 = 15,
    cudaResViewFormatSignedInt1 = 16,
    cudaResViewFormatSignedInt2 = 17,
    cudaResViewFormatSignedInt4 = 18,
    cudaResViewFormatHalf1 = 19,
    cudaResViewFormatHalf2 = 20,
    cudaResViewFormatHalf4 = 21,
    cudaResViewFormatFloat1 = 22,
    cudaResViewFormatFloat2 = 23,
    cudaResViewFormatFloat4 = 24,
    cudaResViewFormatUnsignedBlockCompressed1 = 25,
    cudaResViewFormatUnsignedBlockCompressed2 = 26,
    cudaResViewFormatUnsignedBlockCompressed3 = 27,
    cudaResViewFormatUnsignedBlockCompressed4 = 28,
    cudaResViewFormatSignedBlockCompressed4 = 29,
    cudaResViewFormatUnsignedBlockCompressed5 = 30,
    cudaResViewFormatSignedBlockCompressed5 = 31,
    cudaResViewFormatUnsignedBlockCompressed6H = 32,
    cudaResViewFormatSignedBlockCompressed6H = 33,
    cudaResViewFormatUnsignedBlockCompressed7 = 34,
}
/**
 * CUDA resource descriptor
 */
#[repr(C)]
#[derive(Debug, Copy)]
pub struct cudaResourceDesc {
    /**< Resource type */
    pub resType: cudaResourceType,
    pub res: cudaResourceDesc__bindgen_ty_1,
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct cudaResourceDesc__bindgen_ty_1 {
    pub array: __BindgenUnionField<cudaResourceDesc__bindgen_ty_1__bindgen_ty_1>,
    pub mipmap: __BindgenUnionField<cudaResourceDesc__bindgen_ty_1__bindgen_ty_2>,
    pub linear: __BindgenUnionField<cudaResourceDesc__bindgen_ty_1__bindgen_ty_3>,
    pub pitch2D: __BindgenUnionField<cudaResourceDesc__bindgen_ty_1__bindgen_ty_4>,
    pub bindgen_union_field: [u64; 7usize],
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct cudaResourceDesc__bindgen_ty_1__bindgen_ty_1 {
    /**< CUDA array */
    pub array: cudaArray_t,
}
#[test]
fn bindgen_test_layout_cudaResourceDesc__bindgen_ty_1__bindgen_ty_1() {
    assert_eq!(::std::mem::size_of::<cudaResourceDesc__bindgen_ty_1__bindgen_ty_1>()
               , 8usize);
    assert_eq!(::std::mem::align_of::<cudaResourceDesc__bindgen_ty_1__bindgen_ty_1>()
               , 8usize);
}
impl Clone for cudaResourceDesc__bindgen_ty_1__bindgen_ty_1 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct cudaResourceDesc__bindgen_ty_1__bindgen_ty_2 {
    /**< CUDA mipmapped array */
    pub mipmap: cudaMipmappedArray_t,
}
#[test]
fn bindgen_test_layout_cudaResourceDesc__bindgen_ty_1__bindgen_ty_2() {
    assert_eq!(::std::mem::size_of::<cudaResourceDesc__bindgen_ty_1__bindgen_ty_2>()
               , 8usize);
    assert_eq!(::std::mem::align_of::<cudaResourceDesc__bindgen_ty_1__bindgen_ty_2>()
               , 8usize);
}
impl Clone for cudaResourceDesc__bindgen_ty_1__bindgen_ty_2 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct cudaResourceDesc__bindgen_ty_1__bindgen_ty_3 {
    /**< Device pointer */
    pub devPtr: *mut ::libc::c_void,
    /**< Channel descriptor */
    pub desc: cudaChannelFormatDesc,
    /**< Size in bytes */
    pub sizeInBytes: usize,
}
#[test]
fn bindgen_test_layout_cudaResourceDesc__bindgen_ty_1__bindgen_ty_3() {
    assert_eq!(::std::mem::size_of::<cudaResourceDesc__bindgen_ty_1__bindgen_ty_3>()
               , 40usize);
    assert_eq!(::std::mem::align_of::<cudaResourceDesc__bindgen_ty_1__bindgen_ty_3>()
               , 8usize);
}
impl Clone for cudaResourceDesc__bindgen_ty_1__bindgen_ty_3 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct cudaResourceDesc__bindgen_ty_1__bindgen_ty_4 {
    /**< Device pointer */
    pub devPtr: *mut ::libc::c_void,
    /**< Channel descriptor */
    pub desc: cudaChannelFormatDesc,
    /**< Width of the array in elements */
    pub width: usize,
    /**< Height of the array in elements */
    pub height: usize,
    /**< Pitch between two rows in bytes */
    pub pitchInBytes: usize,
}
#[test]
fn bindgen_test_layout_cudaResourceDesc__bindgen_ty_1__bindgen_ty_4() {
    assert_eq!(::std::mem::size_of::<cudaResourceDesc__bindgen_ty_1__bindgen_ty_4>()
               , 56usize);
    assert_eq!(::std::mem::align_of::<cudaResourceDesc__bindgen_ty_1__bindgen_ty_4>()
               , 8usize);
}
impl Clone for cudaResourceDesc__bindgen_ty_1__bindgen_ty_4 {
    fn clone(&self) -> Self { *self }
}
#[test]
fn bindgen_test_layout_cudaResourceDesc__bindgen_ty_1() {
    assert_eq!(::std::mem::size_of::<cudaResourceDesc__bindgen_ty_1>() ,
               56usize);
    assert_eq!(::std::mem::align_of::<cudaResourceDesc__bindgen_ty_1>() ,
               8usize);
}
impl Clone for cudaResourceDesc__bindgen_ty_1 {
    fn clone(&self) -> Self { *self }
}
#[test]
fn bindgen_test_layout_cudaResourceDesc() {
    assert_eq!(::std::mem::size_of::<cudaResourceDesc>() , 64usize);
    assert_eq!(::std::mem::align_of::<cudaResourceDesc>() , 8usize);
}
impl Clone for cudaResourceDesc {
    fn clone(&self) -> Self { *self }
}
/**
 * CUDA resource view descriptor
 */
#[repr(C)]
#[derive(Debug, Copy)]
pub struct cudaResourceViewDesc {
    /**< Resource view format */
    pub format: cudaResourceViewFormat,
    /**< Width of the resource view */
    pub width: usize,
    /**< Height of the resource view */
    pub height: usize,
    /**< Depth of the resource view */
    pub depth: usize,
    /**< First defined mipmap level */
    pub firstMipmapLevel: ::libc::c_uint,
    /**< Last defined mipmap level */
    pub lastMipmapLevel: ::libc::c_uint,
    /**< First layer index */
    pub firstLayer: ::libc::c_uint,
    /**< Last layer index */
    pub lastLayer: ::libc::c_uint,
}
#[test]
fn bindgen_test_layout_cudaResourceViewDesc() {
    assert_eq!(::std::mem::size_of::<cudaResourceViewDesc>() , 48usize);
    assert_eq!(::std::mem::align_of::<cudaResourceViewDesc>() , 8usize);
}
impl Clone for cudaResourceViewDesc {
    fn clone(&self) -> Self { *self }
}
/**
 * CUDA pointer attributes
 */
#[repr(C)]
#[derive(Debug, Copy)]
pub struct cudaPointerAttributes {
    /**
     * The physical location of the memory, ::cudaMemoryTypeHost or
     * ::cudaMemoryTypeDevice.
     */
    pub memoryType: cudaMemoryType,
    /**
     * The device against which the memory was allocated or registered.
     * If the memory type is ::cudaMemoryTypeDevice then this identifies
     * the device on which the memory referred physically resides.  If
     * the memory type is ::cudaMemoryTypeHost then this identifies the
     * device which was current when the memory was allocated or registered
     * (and if that device is deinitialized then this allocation will vanish
     * with that device's state).
     */
    pub device: ::libc::c_int,
    /**
     * The address which may be dereferenced on the current device to access
     * the memory or NULL if no such address exists.
     */
    pub devicePointer: *mut ::libc::c_void,
    /**
     * The address which may be dereferenced on the host to access the
     * memory or NULL if no such address exists.
     */
    pub hostPointer: *mut ::libc::c_void,
    /**
     * Indicates if this pointer points to managed memory
     */
    pub isManaged: ::libc::c_int,
}
#[test]
fn bindgen_test_layout_cudaPointerAttributes() {
    assert_eq!(::std::mem::size_of::<cudaPointerAttributes>() , 32usize);
    assert_eq!(::std::mem::align_of::<cudaPointerAttributes>() , 8usize);
}
impl Clone for cudaPointerAttributes {
    fn clone(&self) -> Self { *self }
}
/**
 * CUDA function attributes
 */
#[repr(C)]
#[derive(Debug, Copy)]
pub struct cudaFuncAttributes {
    /**
    * The size in bytes of statically-allocated shared memory per block
    * required by this function. This does not include dynamically-allocated
    * shared memory requested by the user at runtime.
    */
    pub sharedSizeBytes: usize,
    /**
    * The size in bytes of user-allocated constant memory required by this
    * function.
    */
    pub constSizeBytes: usize,
    /**
    * The size in bytes of local memory used by each thread of this function.
    */
    pub localSizeBytes: usize,
    /**
    * The maximum number of threads per block, beyond which a launch of the
    * function would fail. This number depends on both the function and the
    * device on which the function is currently loaded.
    */
    pub maxThreadsPerBlock: ::libc::c_int,
    /**
    * The number of registers used by each thread of this function.
    */
    pub numRegs: ::libc::c_int,
    /**
    * The PTX virtual architecture version for which the function was
    * compiled. This value is the major PTX version * 10 + the minor PTX
    * version, so a PTX version 1.3 function would return the value 13.
    */
    pub ptxVersion: ::libc::c_int,
    /**
    * The binary architecture version for which the function was compiled.
    * This value is the major binary version * 10 + the minor binary version,
    * so a binary version 1.3 function would return the value 13.
    */
    pub binaryVersion: ::libc::c_int,
    /**
    * The attribute to indicate whether the function has been compiled with
    * user specified option "-Xptxas --dlcm=ca" set.
    */
    pub cacheModeCA: ::libc::c_int,
}
#[test]
fn bindgen_test_layout_cudaFuncAttributes() {
    assert_eq!(::std::mem::size_of::<cudaFuncAttributes>() , 48usize);
    assert_eq!(::std::mem::align_of::<cudaFuncAttributes>() , 8usize);
}
impl Clone for cudaFuncAttributes {
    fn clone(&self) -> Self { *self }
}
#[repr(u32)]
/**
 * CUDA function cache configurations
 */
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum cudaFuncCache {
    cudaFuncCachePreferNone = 0,
    cudaFuncCachePreferShared = 1,
    cudaFuncCachePreferL1 = 2,
    cudaFuncCachePreferEqual = 3,
}
#[repr(u32)]
/**
 * CUDA shared memory configuration
 */
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum cudaSharedMemConfig {
    cudaSharedMemBankSizeDefault = 0,
    cudaSharedMemBankSizeFourByte = 1,
    cudaSharedMemBankSizeEightByte = 2,
}
#[repr(u32)]
/**
 * CUDA device compute modes
 */
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum cudaComputeMode {
    cudaComputeModeDefault = 0,
    cudaComputeModeExclusive = 1,
    cudaComputeModeProhibited = 2,
    cudaComputeModeExclusiveProcess = 3,
}
#[repr(u32)]
/**
 * CUDA Limits
 */
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum cudaLimit {
    cudaLimitStackSize = 0,
    cudaLimitPrintfFifoSize = 1,
    cudaLimitMallocHeapSize = 2,
    cudaLimitDevRuntimeSyncDepth = 3,
    cudaLimitDevRuntimePendingLaunchCount = 4,
}
#[repr(u32)]
/**
 * CUDA Memory Advise values
 */
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum cudaMemoryAdvise {
    cudaMemAdviseSetReadMostly = 1,
    cudaMemAdviseUnsetReadMostly = 2,
    cudaMemAdviseSetPreferredLocation = 3,
    cudaMemAdviseUnsetPreferredLocation = 4,
    cudaMemAdviseSetAccessedBy = 5,
    cudaMemAdviseUnsetAccessedBy = 6,
}
#[repr(u32)]
/**
 * CUDA range attributes
 */
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum cudaMemRangeAttribute {
    cudaMemRangeAttributeReadMostly = 1,
    cudaMemRangeAttributePreferredLocation = 2,
    cudaMemRangeAttributeAccessedBy = 3,
    cudaMemRangeAttributeLastPrefetchLocation = 4,
}
#[repr(u32)]
/**
 * CUDA Profiler Output modes
 */
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum cudaOutputMode { cudaKeyValuePair = 0, cudaCSV = 1, }
#[repr(u32)]
/**
 * CUDA device attributes
 */
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum cudaDeviceAttr {
    cudaDevAttrMaxThreadsPerBlock = 1,
    cudaDevAttrMaxBlockDimX = 2,
    cudaDevAttrMaxBlockDimY = 3,
    cudaDevAttrMaxBlockDimZ = 4,
    cudaDevAttrMaxGridDimX = 5,
    cudaDevAttrMaxGridDimY = 6,
    cudaDevAttrMaxGridDimZ = 7,
    cudaDevAttrMaxSharedMemoryPerBlock = 8,
    cudaDevAttrTotalConstantMemory = 9,
    cudaDevAttrWarpSize = 10,
    cudaDevAttrMaxPitch = 11,
    cudaDevAttrMaxRegistersPerBlock = 12,
    cudaDevAttrClockRate = 13,
    cudaDevAttrTextureAlignment = 14,
    cudaDevAttrGpuOverlap = 15,
    cudaDevAttrMultiProcessorCount = 16,
    cudaDevAttrKernelExecTimeout = 17,
    cudaDevAttrIntegrated = 18,
    cudaDevAttrCanMapHostMemory = 19,
    cudaDevAttrComputeMode = 20,
    cudaDevAttrMaxTexture1DWidth = 21,
    cudaDevAttrMaxTexture2DWidth = 22,
    cudaDevAttrMaxTexture2DHeight = 23,
    cudaDevAttrMaxTexture3DWidth = 24,
    cudaDevAttrMaxTexture3DHeight = 25,
    cudaDevAttrMaxTexture3DDepth = 26,
    cudaDevAttrMaxTexture2DLayeredWidth = 27,
    cudaDevAttrMaxTexture2DLayeredHeight = 28,
    cudaDevAttrMaxTexture2DLayeredLayers = 29,
    cudaDevAttrSurfaceAlignment = 30,
    cudaDevAttrConcurrentKernels = 31,
    cudaDevAttrEccEnabled = 32,
    cudaDevAttrPciBusId = 33,
    cudaDevAttrPciDeviceId = 34,
    cudaDevAttrTccDriver = 35,
    cudaDevAttrMemoryClockRate = 36,
    cudaDevAttrGlobalMemoryBusWidth = 37,
    cudaDevAttrL2CacheSize = 38,
    cudaDevAttrMaxThreadsPerMultiProcessor = 39,
    cudaDevAttrAsyncEngineCount = 40,
    cudaDevAttrUnifiedAddressing = 41,
    cudaDevAttrMaxTexture1DLayeredWidth = 42,
    cudaDevAttrMaxTexture1DLayeredLayers = 43,
    cudaDevAttrMaxTexture2DGatherWidth = 45,
    cudaDevAttrMaxTexture2DGatherHeight = 46,
    cudaDevAttrMaxTexture3DWidthAlt = 47,
    cudaDevAttrMaxTexture3DHeightAlt = 48,
    cudaDevAttrMaxTexture3DDepthAlt = 49,
    cudaDevAttrPciDomainId = 50,
    cudaDevAttrTexturePitchAlignment = 51,
    cudaDevAttrMaxTextureCubemapWidth = 52,
    cudaDevAttrMaxTextureCubemapLayeredWidth = 53,
    cudaDevAttrMaxTextureCubemapLayeredLayers = 54,
    cudaDevAttrMaxSurface1DWidth = 55,
    cudaDevAttrMaxSurface2DWidth = 56,
    cudaDevAttrMaxSurface2DHeight = 57,
    cudaDevAttrMaxSurface3DWidth = 58,
    cudaDevAttrMaxSurface3DHeight = 59,
    cudaDevAttrMaxSurface3DDepth = 60,
    cudaDevAttrMaxSurface1DLayeredWidth = 61,
    cudaDevAttrMaxSurface1DLayeredLayers = 62,
    cudaDevAttrMaxSurface2DLayeredWidth = 63,
    cudaDevAttrMaxSurface2DLayeredHeight = 64,
    cudaDevAttrMaxSurface2DLayeredLayers = 65,
    cudaDevAttrMaxSurfaceCubemapWidth = 66,
    cudaDevAttrMaxSurfaceCubemapLayeredWidth = 67,
    cudaDevAttrMaxSurfaceCubemapLayeredLayers = 68,
    cudaDevAttrMaxTexture1DLinearWidth = 69,
    cudaDevAttrMaxTexture2DLinearWidth = 70,
    cudaDevAttrMaxTexture2DLinearHeight = 71,
    cudaDevAttrMaxTexture2DLinearPitch = 72,
    cudaDevAttrMaxTexture2DMipmappedWidth = 73,
    cudaDevAttrMaxTexture2DMipmappedHeight = 74,
    cudaDevAttrComputeCapabilityMajor = 75,
    cudaDevAttrComputeCapabilityMinor = 76,
    cudaDevAttrMaxTexture1DMipmappedWidth = 77,
    cudaDevAttrStreamPrioritiesSupported = 78,
    cudaDevAttrGlobalL1CacheSupported = 79,
    cudaDevAttrLocalL1CacheSupported = 80,
    cudaDevAttrMaxSharedMemoryPerMultiprocessor = 81,
    cudaDevAttrMaxRegistersPerMultiprocessor = 82,
    cudaDevAttrManagedMemory = 83,
    cudaDevAttrIsMultiGpuBoard = 84,
    cudaDevAttrMultiGpuBoardGroupID = 85,
    cudaDevAttrHostNativeAtomicSupported = 86,
    cudaDevAttrSingleToDoublePrecisionPerfRatio = 87,
    cudaDevAttrPageableMemoryAccess = 88,
    cudaDevAttrConcurrentManagedAccess = 89,
    cudaDevAttrComputePreemptionSupported = 90,
    cudaDevAttrCanUseHostPointerForRegisteredMem = 91,
}
#[repr(u32)]
/**
 * CUDA device P2P attributes
 */
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum cudaDeviceP2PAttr {
    cudaDevP2PAttrPerformanceRank = 1,
    cudaDevP2PAttrAccessSupported = 2,
    cudaDevP2PAttrNativeAtomicSupported = 3,
}
/**
 * CUDA device properties
 */
#[repr(C)]
pub struct cudaDeviceProp {
    /**< ASCII string identifying device */
    pub name: [::libc::c_char; 256usize],
    /**< Global memory available on device in bytes */
    pub totalGlobalMem: usize,
    /**< Shared memory available per block in bytes */
    pub sharedMemPerBlock: usize,
    /**< 32-bit registers available per block */
    pub regsPerBlock: ::libc::c_int,
    /**< Warp size in threads */
    pub warpSize: ::libc::c_int,
    /**< Maximum pitch in bytes allowed by memory copies */
    pub memPitch: usize,
    /**< Maximum number of threads per block */
    pub maxThreadsPerBlock: ::libc::c_int,
    /**< Maximum size of each dimension of a block */
    pub maxThreadsDim: [::libc::c_int; 3usize],
    /**< Maximum size of each dimension of a grid */
    pub maxGridSize: [::libc::c_int; 3usize],
    /**< Clock frequency in kilohertz */
    pub clockRate: ::libc::c_int,
    /**< Constant memory available on device in bytes */
    pub totalConstMem: usize,
    /**< Major compute capability */
    pub major: ::libc::c_int,
    /**< Minor compute capability */
    pub minor: ::libc::c_int,
    /**< Alignment requirement for textures */
    pub textureAlignment: usize,
    /**< Pitch alignment requirement for texture references bound to pitched memory */
    pub texturePitchAlignment: usize,
    /**< Device can concurrently copy memory and execute a kernel. Deprecated. Use instead asyncEngineCount. */
    pub deviceOverlap: ::libc::c_int,
    /**< Number of multiprocessors on device */
    pub multiProcessorCount: ::libc::c_int,
    /**< Specified whether there is a run time limit on kernels */
    pub kernelExecTimeoutEnabled: ::libc::c_int,
    /**< Device is integrated as opposed to discrete */
    pub integrated: ::libc::c_int,
    /**< Device can map host memory with cudaHostAlloc/cudaHostGetDevicePointer */
    pub canMapHostMemory: ::libc::c_int,
    /**< Compute mode (See ::cudaComputeMode) */
    pub computeMode: ::libc::c_int,
    /**< Maximum 1D texture size */
    pub maxTexture1D: ::libc::c_int,
    /**< Maximum 1D mipmapped texture size */
    pub maxTexture1DMipmap: ::libc::c_int,
    /**< Maximum size for 1D textures bound to linear memory */
    pub maxTexture1DLinear: ::libc::c_int,
    /**< Maximum 2D texture dimensions */
    pub maxTexture2D: [::libc::c_int; 2usize],
    /**< Maximum 2D mipmapped texture dimensions */
    pub maxTexture2DMipmap: [::libc::c_int; 2usize],
    /**< Maximum dimensions (width, height, pitch) for 2D textures bound to pitched memory */
    pub maxTexture2DLinear: [::libc::c_int; 3usize],
    /**< Maximum 2D texture dimensions if texture gather operations have to be performed */
    pub maxTexture2DGather: [::libc::c_int; 2usize],
    /**< Maximum 3D texture dimensions */
    pub maxTexture3D: [::libc::c_int; 3usize],
    /**< Maximum alternate 3D texture dimensions */
    pub maxTexture3DAlt: [::libc::c_int; 3usize],
    /**< Maximum Cubemap texture dimensions */
    pub maxTextureCubemap: ::libc::c_int,
    /**< Maximum 1D layered texture dimensions */
    pub maxTexture1DLayered: [::libc::c_int; 2usize],
    /**< Maximum 2D layered texture dimensions */
    pub maxTexture2DLayered: [::libc::c_int; 3usize],
    /**< Maximum Cubemap layered texture dimensions */
    pub maxTextureCubemapLayered: [::libc::c_int; 2usize],
    /**< Maximum 1D surface size */
    pub maxSurface1D: ::libc::c_int,
    /**< Maximum 2D surface dimensions */
    pub maxSurface2D: [::libc::c_int; 2usize],
    /**< Maximum 3D surface dimensions */
    pub maxSurface3D: [::libc::c_int; 3usize],
    /**< Maximum 1D layered surface dimensions */
    pub maxSurface1DLayered: [::libc::c_int; 2usize],
    /**< Maximum 2D layered surface dimensions */
    pub maxSurface2DLayered: [::libc::c_int; 3usize],
    /**< Maximum Cubemap surface dimensions */
    pub maxSurfaceCubemap: ::libc::c_int,
    /**< Maximum Cubemap layered surface dimensions */
    pub maxSurfaceCubemapLayered: [::libc::c_int; 2usize],
    /**< Alignment requirements for surfaces */
    pub surfaceAlignment: usize,
    /**< Device can possibly execute multiple kernels concurrently */
    pub concurrentKernels: ::libc::c_int,
    /**< Device has ECC support enabled */
    pub ECCEnabled: ::libc::c_int,
    /**< PCI bus ID of the device */
    pub pciBusID: ::libc::c_int,
    /**< PCI device ID of the device */
    pub pciDeviceID: ::libc::c_int,
    /**< PCI domain ID of the device */
    pub pciDomainID: ::libc::c_int,
    /**< 1 if device is a Tesla device using TCC driver, 0 otherwise */
    pub tccDriver: ::libc::c_int,
    /**< Number of asynchronous engines */
    pub asyncEngineCount: ::libc::c_int,
    /**< Device shares a unified address space with the host */
    pub unifiedAddressing: ::libc::c_int,
    /**< Peak memory clock frequency in kilohertz */
    pub memoryClockRate: ::libc::c_int,
    /**< Global memory bus width in bits */
    pub memoryBusWidth: ::libc::c_int,
    /**< Size of L2 cache in bytes */
    pub l2CacheSize: ::libc::c_int,
    /**< Maximum resident threads per multiprocessor */
    pub maxThreadsPerMultiProcessor: ::libc::c_int,
    /**< Device supports stream priorities */
    pub streamPrioritiesSupported: ::libc::c_int,
    /**< Device supports caching globals in L1 */
    pub globalL1CacheSupported: ::libc::c_int,
    /**< Device supports caching locals in L1 */
    pub localL1CacheSupported: ::libc::c_int,
    /**< Shared memory available per multiprocessor in bytes */
    pub sharedMemPerMultiprocessor: usize,
    /**< 32-bit registers available per multiprocessor */
    pub regsPerMultiprocessor: ::libc::c_int,
    /**< Device supports allocating managed memory on this system */
    pub managedMemory: ::libc::c_int,
    /**< Device is on a multi-GPU board */
    pub isMultiGpuBoard: ::libc::c_int,
    /**< Unique identifier for a group of devices on the same multi-GPU board */
    pub multiGpuBoardGroupID: ::libc::c_int,
    /**< Link between the device and the host supports native atomic operations */
    pub hostNativeAtomicSupported: ::libc::c_int,
    /**< Ratio of single precision performance (in floating-point operations per second) to double precision performance */
    pub singleToDoublePrecisionPerfRatio: ::libc::c_int,
    /**< Device supports coherently accessing pageable memory without calling cudaHostRegister on it */
    pub pageableMemoryAccess: ::libc::c_int,
    /**< Device can coherently access managed memory concurrently with the CPU */
    pub concurrentManagedAccess: ::libc::c_int,
}
#[test]
fn bindgen_test_layout_cudaDeviceProp() {
    assert_eq!(::std::mem::size_of::<cudaDeviceProp>() , 648usize);
    assert_eq!(::std::mem::align_of::<cudaDeviceProp>() , 8usize);
}
/**
 * CUDA IPC event handle
 */
#[repr(C)]
pub struct cudaIpcEventHandle_st {
    pub reserved: [::libc::c_char; 64usize],
}
#[test]
fn bindgen_test_layout_cudaIpcEventHandle_st() {
    assert_eq!(::std::mem::size_of::<cudaIpcEventHandle_st>() , 64usize);
    assert_eq!(::std::mem::align_of::<cudaIpcEventHandle_st>() , 1usize);
}
pub type cudaIpcEventHandle_t = cudaIpcEventHandle_st;
/**
 * CUDA IPC memory handle
 */
#[repr(C)]
pub struct cudaIpcMemHandle_st {
    pub reserved: [::libc::c_char; 64usize],
}
#[test]
fn bindgen_test_layout_cudaIpcMemHandle_st() {
    assert_eq!(::std::mem::size_of::<cudaIpcMemHandle_st>() , 64usize);
    assert_eq!(::std::mem::align_of::<cudaIpcMemHandle_st>() , 1usize);
}
pub type cudaIpcMemHandle_t = cudaIpcMemHandle_st;
/**
 * CUDA Error types
 */
pub use self::cudaError as cudaError_t;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct CUstream_st([u8; 0]);
/**
 * CUDA stream
 */
pub type cudaStream_t = *mut CUstream_st;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct CUevent_st([u8; 0]);
/**
 * CUDA event types
 */
pub type cudaEvent_t = *mut CUevent_st;
/**
 * CUDA graphics resource types
 */
pub type cudaGraphicsResource_t = *mut cudaGraphicsResource;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct CUuuid_st([u8; 0]);
/**
 * CUDA UUID types
 */
pub type cudaUUID_t = CUuuid_st;
/**
 * CUDA output file modes
 */
pub use self::cudaOutputMode as cudaOutputMode_t;
#[repr(u32)]
/*******************************************************************************
*                                                                              *
*                                                                              *
*                                                                              *
*******************************************************************************/
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum cudaRoundMode {
    cudaRoundNearest = 0,
    cudaRoundZero = 1,
    cudaRoundPosInf = 2,
    cudaRoundMinInf = 3,
}
#[repr(u32)]
/**
 * CUDA Surface boundary modes
 */
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum cudaSurfaceBoundaryMode {
    cudaBoundaryModeZero = 0,
    cudaBoundaryModeClamp = 1,
    cudaBoundaryModeTrap = 2,
}
#[repr(u32)]
/**
 * CUDA Surface format modes
 */
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum cudaSurfaceFormatMode {
    cudaFormatModeForced = 0,
    cudaFormatModeAuto = 1,
}
/**
 * CUDA Surface reference
 */
#[repr(C)]
#[derive(Debug, Copy)]
pub struct surfaceReference {
    /**
     * Channel descriptor for surface reference
     */
    pub channelDesc: cudaChannelFormatDesc,
}
#[test]
fn bindgen_test_layout_surfaceReference() {
    assert_eq!(::std::mem::size_of::<surfaceReference>() , 20usize);
    assert_eq!(::std::mem::align_of::<surfaceReference>() , 4usize);
}
impl Clone for surfaceReference {
    fn clone(&self) -> Self { *self }
}
/**
 * An opaque value that represents a CUDA Surface object
 */
pub type cudaSurfaceObject_t = ::libc::c_ulonglong;
#[repr(u32)]
/**
 * CUDA texture address modes
 */
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum cudaTextureAddressMode {
    cudaAddressModeWrap = 0,
    cudaAddressModeClamp = 1,
    cudaAddressModeMirror = 2,
    cudaAddressModeBorder = 3,
}
#[repr(u32)]
/**
 * CUDA texture filter modes
 */
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum cudaTextureFilterMode {
    cudaFilterModePoint = 0,
    cudaFilterModeLinear = 1,
}
#[repr(u32)]
/**
 * CUDA texture read modes
 */
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum cudaTextureReadMode {
    cudaReadModeElementType = 0,
    cudaReadModeNormalizedFloat = 1,
}
/**
 * CUDA texture reference
 */
#[repr(C)]
#[derive(Debug, Copy)]
pub struct textureReference {
    /**
     * Indicates whether texture reads are normalized or not
     */
    pub normalized: ::libc::c_int,
    /**
     * Texture filter mode
     */
    pub filterMode: cudaTextureFilterMode,
    /**
     * Texture address mode for up to 3 dimensions
     */
    pub addressMode: [cudaTextureAddressMode; 3usize],
    /**
     * Channel descriptor for the texture reference
     */
    pub channelDesc: cudaChannelFormatDesc,
    /**
     * Perform sRGB->linear conversion during texture read
     */
    pub sRGB: ::libc::c_int,
    /**
     * Limit to the anisotropy ratio
     */
    pub maxAnisotropy: ::libc::c_uint,
    /**
     * Mipmap filter mode
     */
    pub mipmapFilterMode: cudaTextureFilterMode,
    /**
     * Offset applied to the supplied mipmap level
     */
    pub mipmapLevelBias: f32,
    /**
     * Lower end of the mipmap level range to clamp access to
     */
    pub minMipmapLevelClamp: f32,
    /**
     * Upper end of the mipmap level range to clamp access to
     */
    pub maxMipmapLevelClamp: f32,
    pub __cudaReserved: [::libc::c_int; 15usize],
}
#[test]
fn bindgen_test_layout_textureReference() {
    assert_eq!(::std::mem::size_of::<textureReference>() , 124usize);
    assert_eq!(::std::mem::align_of::<textureReference>() , 4usize);
}
impl Clone for textureReference {
    fn clone(&self) -> Self { *self }
}
/**
 * CUDA texture descriptor
 */
#[repr(C)]
#[derive(Debug, Copy)]
pub struct cudaTextureDesc {
    /**
     * Texture address mode for up to 3 dimensions
     */
    pub addressMode: [cudaTextureAddressMode; 3usize],
    /**
     * Texture filter mode
     */
    pub filterMode: cudaTextureFilterMode,
    /**
     * Texture read mode
     */
    pub readMode: cudaTextureReadMode,
    /**
     * Perform sRGB->linear conversion during texture read
     */
    pub sRGB: ::libc::c_int,
    /**
     * Texture Border Color
     */
    pub borderColor: [f32; 4usize],
    /**
     * Indicates whether texture reads are normalized or not
     */
    pub normalizedCoords: ::libc::c_int,
    /**
     * Limit to the anisotropy ratio
     */
    pub maxAnisotropy: ::libc::c_uint,
    /**
     * Mipmap filter mode
     */
    pub mipmapFilterMode: cudaTextureFilterMode,
    /**
     * Offset applied to the supplied mipmap level
     */
    pub mipmapLevelBias: f32,
    /**
     * Lower end of the mipmap level range to clamp access to
     */
    pub minMipmapLevelClamp: f32,
    /**
     * Upper end of the mipmap level range to clamp access to
     */
    pub maxMipmapLevelClamp: f32,
}
#[test]
fn bindgen_test_layout_cudaTextureDesc() {
    assert_eq!(::std::mem::size_of::<cudaTextureDesc>() , 64usize);
    assert_eq!(::std::mem::align_of::<cudaTextureDesc>() , 4usize);
}
impl Clone for cudaTextureDesc {
    fn clone(&self) -> Self { *self }
}
/**
 * An opaque value that represents a CUDA texture object
 */
pub type cudaTextureObject_t = ::libc::c_ulonglong;
#[repr(C)]
#[derive(Debug, Copy)]
pub struct char1 {
    pub x: ::libc::c_char,
}
#[test]
fn bindgen_test_layout_char1() {
    assert_eq!(::std::mem::size_of::<char1>() , 1usize);
    assert_eq!(::std::mem::align_of::<char1>() , 1usize);
}
impl Clone for char1 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct uchar1 {
    pub x: ::libc::c_uchar,
}
#[test]
fn bindgen_test_layout_uchar1() {
    assert_eq!(::std::mem::size_of::<uchar1>() , 1usize);
    assert_eq!(::std::mem::align_of::<uchar1>() , 1usize);
}
impl Clone for uchar1 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct char2 {
    pub x: ::libc::c_char,
    pub y: ::libc::c_char,
}
impl Clone for char2 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct uchar2 {
    pub x: ::libc::c_uchar,
    pub y: ::libc::c_uchar,
}
impl Clone for uchar2 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct char3 {
    pub x: ::libc::c_char,
    pub y: ::libc::c_char,
    pub z: ::libc::c_char,
}
#[test]
fn bindgen_test_layout_char3() {
    assert_eq!(::std::mem::size_of::<char3>() , 3usize);
    assert_eq!(::std::mem::align_of::<char3>() , 1usize);
}
impl Clone for char3 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct uchar3 {
    pub x: ::libc::c_uchar,
    pub y: ::libc::c_uchar,
    pub z: ::libc::c_uchar,
}
#[test]
fn bindgen_test_layout_uchar3() {
    assert_eq!(::std::mem::size_of::<uchar3>() , 3usize);
    assert_eq!(::std::mem::align_of::<uchar3>() , 1usize);
}
impl Clone for uchar3 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct char4 {
    pub x: ::libc::c_char,
    pub y: ::libc::c_char,
    pub z: ::libc::c_char,
    pub w: ::libc::c_char,
}
impl Clone for char4 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct uchar4 {
    pub x: ::libc::c_uchar,
    pub y: ::libc::c_uchar,
    pub z: ::libc::c_uchar,
    pub w: ::libc::c_uchar,
}
impl Clone for uchar4 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct short1 {
    pub x: ::libc::c_short,
}
#[test]
fn bindgen_test_layout_short1() {
    assert_eq!(::std::mem::size_of::<short1>() , 2usize);
    assert_eq!(::std::mem::align_of::<short1>() , 2usize);
}
impl Clone for short1 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct ushort1 {
    pub x: ::libc::c_ushort,
}
#[test]
fn bindgen_test_layout_ushort1() {
    assert_eq!(::std::mem::size_of::<ushort1>() , 2usize);
    assert_eq!(::std::mem::align_of::<ushort1>() , 2usize);
}
impl Clone for ushort1 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct short2 {
    pub x: ::libc::c_short,
    pub y: ::libc::c_short,
}
impl Clone for short2 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct ushort2 {
    pub x: ::libc::c_ushort,
    pub y: ::libc::c_ushort,
}
impl Clone for ushort2 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct short3 {
    pub x: ::libc::c_short,
    pub y: ::libc::c_short,
    pub z: ::libc::c_short,
}
#[test]
fn bindgen_test_layout_short3() {
    assert_eq!(::std::mem::size_of::<short3>() , 6usize);
    assert_eq!(::std::mem::align_of::<short3>() , 2usize);
}
impl Clone for short3 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct ushort3 {
    pub x: ::libc::c_ushort,
    pub y: ::libc::c_ushort,
    pub z: ::libc::c_ushort,
}
#[test]
fn bindgen_test_layout_ushort3() {
    assert_eq!(::std::mem::size_of::<ushort3>() , 6usize);
    assert_eq!(::std::mem::align_of::<ushort3>() , 2usize);
}
impl Clone for ushort3 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct short4 {
    pub x: ::libc::c_short,
    pub y: ::libc::c_short,
    pub z: ::libc::c_short,
    pub w: ::libc::c_short,
}
impl Clone for short4 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct ushort4 {
    pub x: ::libc::c_ushort,
    pub y: ::libc::c_ushort,
    pub z: ::libc::c_ushort,
    pub w: ::libc::c_ushort,
}
impl Clone for ushort4 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct int1 {
    pub x: ::libc::c_int,
}
#[test]
fn bindgen_test_layout_int1() {
    assert_eq!(::std::mem::size_of::<int1>() , 4usize);
    assert_eq!(::std::mem::align_of::<int1>() , 4usize);
}
impl Clone for int1 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct uint1 {
    pub x: ::libc::c_uint,
}
#[test]
fn bindgen_test_layout_uint1() {
    assert_eq!(::std::mem::size_of::<uint1>() , 4usize);
    assert_eq!(::std::mem::align_of::<uint1>() , 4usize);
}
impl Clone for uint1 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct int2 {
    pub x: ::libc::c_int,
    pub y: ::libc::c_int,
}
impl Clone for int2 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct uint2 {
    pub x: ::libc::c_uint,
    pub y: ::libc::c_uint,
}
impl Clone for uint2 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct int3 {
    pub x: ::libc::c_int,
    pub y: ::libc::c_int,
    pub z: ::libc::c_int,
}
#[test]
fn bindgen_test_layout_int3() {
    assert_eq!(::std::mem::size_of::<int3>() , 12usize);
    assert_eq!(::std::mem::align_of::<int3>() , 4usize);
}
impl Clone for int3 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct uint3 {
    pub x: ::libc::c_uint,
    pub y: ::libc::c_uint,
    pub z: ::libc::c_uint,
}
#[test]
fn bindgen_test_layout_uint3() {
    assert_eq!(::std::mem::size_of::<uint3>() , 12usize);
    assert_eq!(::std::mem::align_of::<uint3>() , 4usize);
}
impl Clone for uint3 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct int4 {
    pub x: ::libc::c_int,
    pub y: ::libc::c_int,
    pub z: ::libc::c_int,
    pub w: ::libc::c_int,
}
impl Clone for int4 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct uint4 {
    pub x: ::libc::c_uint,
    pub y: ::libc::c_uint,
    pub z: ::libc::c_uint,
    pub w: ::libc::c_uint,
}
impl Clone for uint4 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct long1 {
    pub x: ::libc::c_long,
}
#[test]
fn bindgen_test_layout_long1() {
    assert_eq!(::std::mem::size_of::<long1>() , 8usize);
    assert_eq!(::std::mem::align_of::<long1>() , 8usize);
}
impl Clone for long1 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct ulong1 {
    pub x: ::libc::c_ulong,
}
#[test]
fn bindgen_test_layout_ulong1() {
    assert_eq!(::std::mem::size_of::<ulong1>() , 8usize);
    assert_eq!(::std::mem::align_of::<ulong1>() , 8usize);
}
impl Clone for ulong1 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct long2 {
    pub x: ::libc::c_long,
    pub y: ::libc::c_long,
}
impl Clone for long2 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct ulong2 {
    pub x: ::libc::c_ulong,
    pub y: ::libc::c_ulong,
}
impl Clone for ulong2 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct long3 {
    pub x: ::libc::c_long,
    pub y: ::libc::c_long,
    pub z: ::libc::c_long,
}
#[test]
fn bindgen_test_layout_long3() {
    assert_eq!(::std::mem::size_of::<long3>() , 24usize);
    assert_eq!(::std::mem::align_of::<long3>() , 8usize);
}
impl Clone for long3 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct ulong3 {
    pub x: ::libc::c_ulong,
    pub y: ::libc::c_ulong,
    pub z: ::libc::c_ulong,
}
#[test]
fn bindgen_test_layout_ulong3() {
    assert_eq!(::std::mem::size_of::<ulong3>() , 24usize);
    assert_eq!(::std::mem::align_of::<ulong3>() , 8usize);
}
impl Clone for ulong3 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct long4 {
    pub x: ::libc::c_long,
    pub y: ::libc::c_long,
    pub z: ::libc::c_long,
    pub w: ::libc::c_long,
}
impl Clone for long4 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct ulong4 {
    pub x: ::libc::c_ulong,
    pub y: ::libc::c_ulong,
    pub z: ::libc::c_ulong,
    pub w: ::libc::c_ulong,
}
impl Clone for ulong4 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct float1 {
    pub x: f32,
}
#[test]
fn bindgen_test_layout_float1() {
    assert_eq!(::std::mem::size_of::<float1>() , 4usize);
    assert_eq!(::std::mem::align_of::<float1>() , 4usize);
}
impl Clone for float1 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct float2 {
    pub x: f32,
    pub y: f32,
}
impl Clone for float2 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct float3 {
    pub x: f32,
    pub y: f32,
    pub z: f32,
}
#[test]
fn bindgen_test_layout_float3() {
    assert_eq!(::std::mem::size_of::<float3>() , 12usize);
    assert_eq!(::std::mem::align_of::<float3>() , 4usize);
}
impl Clone for float3 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct float4 {
    pub x: f32,
    pub y: f32,
    pub z: f32,
    pub w: f32,
}
impl Clone for float4 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct longlong1 {
    pub x: ::libc::c_longlong,
}
#[test]
fn bindgen_test_layout_longlong1() {
    assert_eq!(::std::mem::size_of::<longlong1>() , 8usize);
    assert_eq!(::std::mem::align_of::<longlong1>() , 8usize);
}
impl Clone for longlong1 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct ulonglong1 {
    pub x: ::libc::c_ulonglong,
}
#[test]
fn bindgen_test_layout_ulonglong1() {
    assert_eq!(::std::mem::size_of::<ulonglong1>() , 8usize);
    assert_eq!(::std::mem::align_of::<ulonglong1>() , 8usize);
}
impl Clone for ulonglong1 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct longlong2 {
    pub x: ::libc::c_longlong,
    pub y: ::libc::c_longlong,
}
impl Clone for longlong2 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct ulonglong2 {
    pub x: ::libc::c_ulonglong,
    pub y: ::libc::c_ulonglong,
}
impl Clone for ulonglong2 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct longlong3 {
    pub x: ::libc::c_longlong,
    pub y: ::libc::c_longlong,
    pub z: ::libc::c_longlong,
}
#[test]
fn bindgen_test_layout_longlong3() {
    assert_eq!(::std::mem::size_of::<longlong3>() , 24usize);
    assert_eq!(::std::mem::align_of::<longlong3>() , 8usize);
}
impl Clone for longlong3 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct ulonglong3 {
    pub x: ::libc::c_ulonglong,
    pub y: ::libc::c_ulonglong,
    pub z: ::libc::c_ulonglong,
}
#[test]
fn bindgen_test_layout_ulonglong3() {
    assert_eq!(::std::mem::size_of::<ulonglong3>() , 24usize);
    assert_eq!(::std::mem::align_of::<ulonglong3>() , 8usize);
}
impl Clone for ulonglong3 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct longlong4 {
    pub x: ::libc::c_longlong,
    pub y: ::libc::c_longlong,
    pub z: ::libc::c_longlong,
    pub w: ::libc::c_longlong,
}
impl Clone for longlong4 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct ulonglong4 {
    pub x: ::libc::c_ulonglong,
    pub y: ::libc::c_ulonglong,
    pub z: ::libc::c_ulonglong,
    pub w: ::libc::c_ulonglong,
}
impl Clone for ulonglong4 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct double1 {
    pub x: f64,
}
#[test]
fn bindgen_test_layout_double1() {
    assert_eq!(::std::mem::size_of::<double1>() , 8usize);
    assert_eq!(::std::mem::align_of::<double1>() , 8usize);
}
impl Clone for double1 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct double2 {
    pub x: f64,
    pub y: f64,
}
impl Clone for double2 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct double3 {
    pub x: f64,
    pub y: f64,
    pub z: f64,
}
#[test]
fn bindgen_test_layout_double3() {
    assert_eq!(::std::mem::size_of::<double3>() , 24usize);
    assert_eq!(::std::mem::align_of::<double3>() , 8usize);
}
impl Clone for double3 {
    fn clone(&self) -> Self { *self }
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct double4 {
    pub x: f64,
    pub y: f64,
    pub z: f64,
    pub w: f64,
}
impl Clone for double4 {
    fn clone(&self) -> Self { *self }
}
/*******************************************************************************
*                                                                              *
*                                                                              *
*                                                                              *
*******************************************************************************/
#[repr(C)]
#[derive(Debug, Copy)]
pub struct dim3 {
    pub x: ::libc::c_uint,
    pub y: ::libc::c_uint,
    pub z: ::libc::c_uint,
}
#[test]
fn bindgen_test_layout_dim3() {
    assert_eq!(::std::mem::size_of::<dim3>() , 12usize);
    assert_eq!(::std::mem::align_of::<dim3>() , 4usize);
}
impl Clone for dim3 {
    fn clone(&self) -> Self { *self }
}
#[repr(u32)]
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum cudaDataType_t {
    CUDA_R_16F = 2,
    CUDA_C_16F = 6,
    CUDA_R_32F = 0,
    CUDA_C_32F = 4,
    CUDA_R_64F = 1,
    CUDA_C_64F = 5,
    CUDA_R_8I = 3,
    CUDA_C_8I = 7,
    CUDA_R_8U = 8,
    CUDA_C_8U = 9,
    CUDA_R_32I = 10,
    CUDA_C_32I = 11,
    CUDA_R_32U = 12,
    CUDA_C_32U = 13,
}
pub use self::cudaDataType_t as cudaDataType;
#[repr(u32)]
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum libraryPropertyType_t {
    MAJOR_VERSION = 0,
    MINOR_VERSION = 1,
    PATCH_LEVEL = 2,
}
pub use self::libraryPropertyType_t as libraryPropertyType;
extern "C" {
    /**
 * \brief Destroy all allocations and reset all state on the current device
 * in the current process.
 *
 * Explicitly destroys and cleans up all resources associated with the current
 * device in the current process.  Any subsequent API call to this device will
 * reinitialize the device.
 *
 * Note that this function will reset the device immediately.  It is the caller's
 * responsibility to ensure that the device is not being accessed by any
 * other host threads from the process when this function is called.
 *
 * \return
 * ::cudaSuccess
 * \notefnerr
 *
 * \sa ::cudaDeviceSynchronize
 */
    pub fn cudaDeviceReset() -> cudaError_t;
}
extern "C" {
    /**
 * \brief Wait for compute device to finish
 *
 * Blocks until the device has completed all preceding requested tasks.
 * ::cudaDeviceSynchronize() returns an error if one of the preceding tasks
 * has failed. If the ::cudaDeviceScheduleBlockingSync flag was set for
 * this device, the host thread will block until the device has finished
 * its work.
 *
 * \return
 * ::cudaSuccess
 * \notefnerr
 *
 * \sa ::cudaDeviceReset
 */
    pub fn cudaDeviceSynchronize() -> cudaError_t;
}
extern "C" {
    /**
 * \brief Set resource limits
 *
 * Setting \p limit to \p value is a request by the application to update
 * the current limit maintained by the device.  The driver is free to
 * modify the requested value to meet h/w requirements (this could be
 * clamping to minimum or maximum values, rounding up to nearest element
 * size, etc).  The application can use ::cudaDeviceGetLimit() to find out
 * exactly what the limit has been set to.
 *
 * Setting each ::cudaLimit has its own specific restrictions, so each is
 * discussed here.
 *
 * - ::cudaLimitStackSize controls the stack size in bytes of each GPU thread.
 *
 * - ::cudaLimitPrintfFifoSize controls the size in bytes of the shared FIFO
 *   used by the ::printf() and ::fprintf() device system calls. Setting
 *   ::cudaLimitPrintfFifoSize must not be performed after launching any kernel
 *   that uses the ::printf() or ::fprintf() device system calls - in such case
 *   ::cudaErrorInvalidValue will be returned.
 *
 * - ::cudaLimitMallocHeapSize controls the size in bytes of the heap used by
 *   the ::malloc() and ::free() device system calls. Setting
 *   ::cudaLimitMallocHeapSize must not be performed after launching any kernel
 *   that uses the ::malloc() or ::free() device system calls - in such case
 *   ::cudaErrorInvalidValue will be returned.
 *
 * - ::cudaLimitDevRuntimeSyncDepth controls the maximum nesting depth of a
 *   grid at which a thread can safely call ::cudaDeviceSynchronize(). Setting
 *   this limit must be performed before any launch of a kernel that uses the
 *   device runtime and calls ::cudaDeviceSynchronize() above the default sync
 *   depth, two levels of grids. Calls to ::cudaDeviceSynchronize() will fail
 *   with error code ::cudaErrorSyncDepthExceeded if the limitation is
 *   violated. This limit can be set smaller than the default or up the maximum
 *   launch depth of 24. When setting this limit, keep in mind that additional
 *   levels of sync depth require the runtime to reserve large amounts of
 *   device memory which can no longer be used for user allocations. If these
 *   reservations of device memory fail, ::cudaDeviceSetLimit will return
 *   ::cudaErrorMemoryAllocation, and the limit can be reset to a lower value.
 *   This limit is only applicable to devices of compute capability 3.5 and
 *   higher. Attempting to set this limit on devices of compute capability less
 *   than 3.5 will result in the error ::cudaErrorUnsupportedLimit being
 *   returned.
 *
 * - ::cudaLimitDevRuntimePendingLaunchCount controls the maximum number of
 *   outstanding device runtime launches that can be made from the current
 *   device. A grid is outstanding from the point of launch up until the grid
 *   is known to have been completed. Device runtime launches which violate
 *   this limitation fail and return ::cudaErrorLaunchPendingCountExceeded when
 *   ::cudaGetLastError() is called after launch. If more pending launches than
 *   the default (2048 launches) are needed for a module using the device
 *   runtime, this limit can be increased. Keep in mind that being able to
 *   sustain additional pending launches will require the runtime to reserve
 *   larger amounts of device memory upfront which can no longer be used for
 *   allocations. If these reservations fail, ::cudaDeviceSetLimit will return
 *   ::cudaErrorMemoryAllocation, and the limit can be reset to a lower value.
 *   This limit is only applicable to devices of compute capability 3.5 and
 *   higher. Attempting to set this limit on devices of compute capability less
 *   than 3.5 will result in the error ::cudaErrorUnsupportedLimit being
 *   returned.
 *
 * \param limit - Limit to set
 * \param value - Size of limit
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorUnsupportedLimit,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorMemoryAllocation
 * \notefnerr
 *
 * \sa ::cudaDeviceGetLimit
 */
    pub fn cudaDeviceSetLimit(limit: cudaLimit, value: usize) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Returns resource limits
 *
 * Returns in \p *pValue the current size of \p limit.  The supported
 * ::cudaLimit values are:
 * - ::cudaLimitStackSize: stack size in bytes of each GPU thread;
 * - ::cudaLimitPrintfFifoSize: size in bytes of the shared FIFO used by the
 *   ::printf() and ::fprintf() device system calls.
 * - ::cudaLimitMallocHeapSize: size in bytes of the heap used by the
 *   ::malloc() and ::free() device system calls;
 * - ::cudaLimitDevRuntimeSyncDepth: maximum grid depth at which a
 *   thread can isssue the device runtime call ::cudaDeviceSynchronize()
 *   to wait on child grid launches to complete.
 * - ::cudaLimitDevRuntimePendingLaunchCount: maximum number of outstanding
 *   device runtime launches.
 *
 * \param limit  - Limit to query
 * \param pValue - Returned size of the limit
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorUnsupportedLimit,
 * ::cudaErrorInvalidValue
 * \notefnerr
 *
 * \sa ::cudaDeviceSetLimit
 */
    pub fn cudaDeviceGetLimit(pValue: *mut usize, limit: cudaLimit)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Returns the preferred cache configuration for the current device.
 *
 * On devices where the L1 cache and shared memory use the same hardware
 * resources, this returns through \p pCacheConfig the preferred cache
 * configuration for the current device. This is only a preference. The
 * runtime will use the requested configuration if possible, but it is free to
 * choose a different configuration if required to execute functions.
 *
 * This will return a \p pCacheConfig of ::cudaFuncCachePreferNone on devices
 * where the size of the L1 cache and shared memory are fixed.
 *
 * The supported cache configurations are:
 * - ::cudaFuncCachePreferNone: no preference for shared memory or L1 (default)
 * - ::cudaFuncCachePreferShared: prefer larger shared memory and smaller L1 cache
 * - ::cudaFuncCachePreferL1: prefer larger L1 cache and smaller shared memory
 * - ::cudaFuncCachePreferEqual: prefer equal size L1 cache and shared memory
 *
 * \param pCacheConfig - Returned cache configuration
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInitializationError
 * \notefnerr
 *
 * \sa cudaDeviceSetCacheConfig,
 * \ref ::cudaFuncSetCacheConfig(const void*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C API)",
 * \ref ::cudaFuncSetCacheConfig(T*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C++ API)"
 */
    pub fn cudaDeviceGetCacheConfig(pCacheConfig: *mut cudaFuncCache)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Returns numerical values that correspond to the least and
 * greatest stream priorities.
 *
 * Returns in \p *leastPriority and \p *greatestPriority the numerical values that correspond
 * to the least and greatest stream priorities respectively. Stream priorities
 * follow a convention where lower numbers imply greater priorities. The range of
 * meaningful stream priorities is given by [\p *greatestPriority, \p *leastPriority].
 * If the user attempts to create a stream with a priority value that is
 * outside the the meaningful range as specified by this API, the priority is
 * automatically clamped down or up to either \p *leastPriority or \p *greatestPriority
 * respectively. See ::cudaStreamCreateWithPriority for details on creating a
 * priority stream.
 * A NULL may be passed in for \p *leastPriority or \p *greatestPriority if the value
 * is not desired.
 *
 * This function will return '0' in both \p *leastPriority and \p *greatestPriority if
 * the current context's device does not support stream priorities
 * (see ::cudaDeviceGetAttribute).
 *
 * \param leastPriority    - Pointer to an int in which the numerical value for least
 *                           stream priority is returned
 * \param greatestPriority - Pointer to an int in which the numerical value for greatest
 *                           stream priority is returned
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue
 * \notefnerr
 *
 * \sa ::cudaStreamCreateWithPriority,
 * ::cudaStreamGetPriority
 */
    pub fn cudaDeviceGetStreamPriorityRange(leastPriority:
                                                *mut ::libc::c_int,
                                            greatestPriority:
                                                *mut ::libc::c_int)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Sets the preferred cache configuration for the current device.
 *
 * On devices where the L1 cache and shared memory use the same hardware
 * resources, this sets through \p cacheConfig the preferred cache
 * configuration for the current device. This is only a preference. The
 * runtime will use the requested configuration if possible, but it is free to
 * choose a different configuration if required to execute the function. Any
 * function preference set via
 * \ref ::cudaFuncSetCacheConfig(const void*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C API)"
 * or
 * \ref ::cudaFuncSetCacheConfig(T*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C++ API)"
 * will be preferred over this device-wide setting. Setting the device-wide
 * cache configuration to ::cudaFuncCachePreferNone will cause subsequent
 * kernel launches to prefer to not change the cache configuration unless
 * required to launch the kernel.
 *
 * This setting does nothing on devices where the size of the L1 cache and
 * shared memory are fixed.
 *
 * Launching a kernel with a different preference than the most recent
 * preference setting may insert a device-side synchronization point.
 *
 * The supported cache configurations are:
 * - ::cudaFuncCachePreferNone: no preference for shared memory or L1 (default)
 * - ::cudaFuncCachePreferShared: prefer larger shared memory and smaller L1 cache
 * - ::cudaFuncCachePreferL1: prefer larger L1 cache and smaller shared memory
 * - ::cudaFuncCachePreferEqual: prefer equal size L1 cache and shared memory
 *
 * \param cacheConfig - Requested cache configuration
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInitializationError
 * \notefnerr
 *
 * \sa ::cudaDeviceGetCacheConfig,
 * \ref ::cudaFuncSetCacheConfig(const void*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C API)",
 * \ref ::cudaFuncSetCacheConfig(T*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C++ API)"
 */
    pub fn cudaDeviceSetCacheConfig(cacheConfig: cudaFuncCache)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Returns the shared memory configuration for the current device.
 *
 * This function will return in \p pConfig the current size of shared memory banks
 * on the current device. On devices with configurable shared memory banks,
 * ::cudaDeviceSetSharedMemConfig can be used to change this setting, so that all
 * subsequent kernel launches will by default use the new bank size. When
 * ::cudaDeviceGetSharedMemConfig is called on devices without configurable shared
 * memory, it will return the fixed bank size of the hardware.
 *
 * The returned bank configurations can be either:
 * - ::cudaSharedMemBankSizeFourByte - shared memory bank width is four bytes.
 * - ::cudaSharedMemBankSizeEightByte - shared memory bank width is eight bytes.
 *
 * \param pConfig - Returned cache configuration
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInitializationError
 * \notefnerr
 *
 * \sa ::cudaDeviceSetCacheConfig,
 * ::cudaDeviceGetCacheConfig,
 * ::cudaDeviceSetSharedMemConfig,
 * ::cudaFuncSetCacheConfig
 */
    pub fn cudaDeviceGetSharedMemConfig(pConfig: *mut cudaSharedMemConfig)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Sets the shared memory configuration for the current device.
 *
 * On devices with configurable shared memory banks, this function will set
 * the shared memory bank size which is used for all subsequent kernel launches.
 * Any per-function setting of shared memory set via ::cudaFuncSetSharedMemConfig
 * will override the device wide setting.
 *
 * Changing the shared memory configuration between launches may introduce
 * a device side synchronization point.
 *
 * Changing the shared memory bank size will not increase shared memory usage
 * or affect occupancy of kernels, but may have major effects on performance.
 * Larger bank sizes will allow for greater potential bandwidth to shared memory,
 * but will change what kinds of accesses to shared memory will result in bank
 * conflicts.
 *
 * This function will do nothing on devices with fixed shared memory bank size.
 *
 * The supported bank configurations are:
 * - ::cudaSharedMemBankSizeDefault: set bank width the device default (currently,
 *   four bytes)
 * - ::cudaSharedMemBankSizeFourByte: set shared memory bank width to be four bytes
 *   natively.
 * - ::cudaSharedMemBankSizeEightByte: set shared memory bank width to be eight
 *   bytes natively.
 *
 * \param config - Requested cache configuration
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInitializationError
 * \notefnerr
 *
 * \sa ::cudaDeviceSetCacheConfig,
 * ::cudaDeviceGetCacheConfig,
 * ::cudaDeviceGetSharedMemConfig,
 * ::cudaFuncSetCacheConfig
 */
    pub fn cudaDeviceSetSharedMemConfig(config: cudaSharedMemConfig)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Returns a handle to a compute device
 *
 * Returns in \p *device a device ordinal given a PCI bus ID string.
 *
 * \param device   - Returned device ordinal
 *
 * \param pciBusId - String in one of the following forms:
 * [domain]:[bus]:[device].[function]
 * [domain]:[bus]:[device]
 * [bus]:[device].[function]
 * where \p domain, \p bus, \p device, and \p function are all hexadecimal values
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidDevice
 * \notefnerr
 *
 * \sa ::cudaDeviceGetPCIBusId
 */
    pub fn cudaDeviceGetByPCIBusId(device: *mut ::libc::c_int,
                                   pciBusId: *const ::libc::c_char)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Returns a PCI Bus Id string for the device
 *
 * Returns an ASCII string identifying the device \p dev in the NULL-terminated
 * string pointed to by \p pciBusId. \p len specifies the maximum length of the
 * string that may be returned.
 *
 * \param pciBusId - Returned identifier string for the device in the following format
 * [domain]:[bus]:[device].[function]
 * where \p domain, \p bus, \p device, and \p function are all hexadecimal values.
 * pciBusId should be large enough to store 13 characters including the NULL-terminator.
 *
 * \param len      - Maximum length of string to store in \p name
 *
 * \param device   - Device to get identifier string for
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidDevice
 * \notefnerr
 *
 * \sa ::cudaDeviceGetByPCIBusId

 */
    pub fn cudaDeviceGetPCIBusId(pciBusId: *mut ::libc::c_char,
                                 len: ::libc::c_int,
                                 device: ::libc::c_int)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Gets an interprocess handle for a previously allocated event
 *
 * Takes as input a previously allocated event. This event must have been
 * created with the ::cudaEventInterprocess and ::cudaEventDisableTiming
 * flags set. This opaque handle may be copied into other processes and
 * opened with ::cudaIpcOpenEventHandle to allow efficient hardware
 * synchronization between GPU work in different processes.
 *
 * After the event has been been opened in the importing process,
 * ::cudaEventRecord, ::cudaEventSynchronize, ::cudaStreamWaitEvent and
 * ::cudaEventQuery may be used in either process. Performing operations
 * on the imported event after the exported event has been freed
 * with ::cudaEventDestroy will result in undefined behavior.
 *
 * IPC functionality is restricted to devices with support for unified
 * addressing on Linux operating systems.
 *
 * \param handle - Pointer to a user allocated cudaIpcEventHandle
 *                    in which to return the opaque event handle
 * \param event   - Event allocated with ::cudaEventInterprocess and
 *                    ::cudaEventDisableTiming flags.
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidResourceHandle,
 * ::cudaErrorMemoryAllocation,
 * ::cudaErrorMapBufferObjectFailed
 *
 * \sa
 * ::cudaEventCreate,
 * ::cudaEventDestroy,
 * ::cudaEventSynchronize,
 * ::cudaEventQuery,
 * ::cudaStreamWaitEvent,
 * ::cudaIpcOpenEventHandle,
 * ::cudaIpcGetMemHandle,
 * ::cudaIpcOpenMemHandle,
 * ::cudaIpcCloseMemHandle
 */
    pub fn cudaIpcGetEventHandle(handle: *mut cudaIpcEventHandle_t,
                                 event: cudaEvent_t) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Opens an interprocess event handle for use in the current process
 *
 * Opens an interprocess event handle exported from another process with
 * ::cudaIpcGetEventHandle. This function returns a ::cudaEvent_t that behaves like
 * a locally created event with the ::cudaEventDisableTiming flag specified.
 * This event must be freed with ::cudaEventDestroy.
 *
 * Performing operations on the imported event after the exported event has
 * been freed with ::cudaEventDestroy will result in undefined behavior.
 *
 * IPC functionality is restricted to devices with support for unified
 * addressing on Linux operating systems.
 *
 * \param event - Returns the imported event
 * \param handle  - Interprocess handle to open
 *
 * \returns
 * ::cudaSuccess,
 * ::cudaErrorMapBufferObjectFailed,
 * ::cudaErrorInvalidResourceHandle
 *
  * \sa
 * ::cudaEventCreate,
 * ::cudaEventDestroy,
 * ::cudaEventSynchronize,
 * ::cudaEventQuery,
 * ::cudaStreamWaitEvent,
 * ::cudaIpcGetEventHandle,
 * ::cudaIpcGetMemHandle,
 * ::cudaIpcOpenMemHandle,
 * ::cudaIpcCloseMemHandle
 */
    pub fn cudaIpcOpenEventHandle(event: *mut cudaEvent_t,
                                  handle: cudaIpcEventHandle_t)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Gets an interprocess memory handle for an existing device memory
 *          allocation
 *
 * Takes a pointer to the base of an existing device memory allocation created
 * with ::cudaMalloc and exports it for use in another process. This is a
 * lightweight operation and may be called multiple times on an allocation
 * without adverse effects.
 *
 * If a region of memory is freed with ::cudaFree and a subsequent call
 * to ::cudaMalloc returns memory with the same device address,
 * ::cudaIpcGetMemHandle will return a unique handle for the
 * new memory.
 *
 * IPC functionality is restricted to devices with support for unified
 * addressing on Linux operating systems.
 *
 * \param handle - Pointer to user allocated ::cudaIpcMemHandle to return
 *                    the handle in.
 * \param devPtr - Base pointer to previously allocated device memory
 *
 * \returns
 * ::cudaSuccess,
 * ::cudaErrorInvalidResourceHandle,
 * ::cudaErrorMemoryAllocation,
 * ::cudaErrorMapBufferObjectFailed,
 *
 * \sa
 * ::cudaMalloc,
 * ::cudaFree,
 * ::cudaIpcGetEventHandle,
 * ::cudaIpcOpenEventHandle,
 * ::cudaIpcOpenMemHandle,
 * ::cudaIpcCloseMemHandle
 */
    pub fn cudaIpcGetMemHandle(handle: *mut cudaIpcMemHandle_t,
                               devPtr: *mut ::libc::c_void)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Opens an interprocess memory handle exported from another process
 *          and returns a device pointer usable in the local process.
 *
 * Maps memory exported from another process with ::cudaIpcGetMemHandle into
 * the current device address space. For contexts on different devices
 * ::cudaIpcOpenMemHandle can attempt to enable peer access between the
 * devices as if the user called ::cudaDeviceEnablePeerAccess. This behavior is
 * controlled by the ::cudaIpcMemLazyEnablePeerAccess flag.
 * ::cudaDeviceCanAccessPeer can determine if a mapping is possible.
 *
 * Contexts that may open ::cudaIpcMemHandles are restricted in the following way.
 * ::cudaIpcMemHandles from each device in a given process may only be opened
 * by one context per device per other process.
 *
 * Memory returned from ::cudaIpcOpenMemHandle must be freed with
 * ::cudaIpcCloseMemHandle.
 *
 * Calling ::cudaFree on an exported memory region before calling
 * ::cudaIpcCloseMemHandle in the importing context will result in undefined
 * behavior.
 *
 * IPC functionality is restricted to devices with support for unified
 * addressing on Linux operating systems.
 *
 * \param devPtr - Returned device pointer
 * \param handle - ::cudaIpcMemHandle to open
 * \param flags  - Flags for this operation. Must be specified as ::cudaIpcMemLazyEnablePeerAccess
 *
 * \returns
 * ::cudaSuccess,
 * ::cudaErrorMapBufferObjectFailed,
 * ::cudaErrorInvalidResourceHandle,
 * ::cudaErrorTooManyPeers
 *
 * \note No guarantees are made about the address returned in \p *devPtr.
 * In particular, multiple processes may not receive the same address for the same \p handle.
 *
 * \sa
 * ::cudaMalloc,
 * ::cudaFree,
 * ::cudaIpcGetEventHandle,
 * ::cudaIpcOpenEventHandle,
 * ::cudaIpcGetMemHandle,
 * ::cudaIpcCloseMemHandle,
 * ::cudaDeviceEnablePeerAccess,
 * ::cudaDeviceCanAccessPeer,
 */
    pub fn cudaIpcOpenMemHandle(devPtr: *mut *mut ::libc::c_void,
                                handle: cudaIpcMemHandle_t,
                                flags: ::libc::c_uint) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Close memory mapped with cudaIpcOpenMemHandle
 *
 * Unmaps memory returnd by ::cudaIpcOpenMemHandle. The original allocation
 * in the exporting process as well as imported mappings in other processes
 * will be unaffected.
 *
 * Any resources used to enable peer access will be freed if this is the
 * last mapping using them.
 *
 * IPC functionality is restricted to devices with support for unified
 * addressing on Linux operating systems.
 *
 * \param devPtr - Device pointer returned by ::cudaIpcOpenMemHandle
 *
 * \returns
 * ::cudaSuccess,
 * ::cudaErrorMapBufferObjectFailed,
 * ::cudaErrorInvalidResourceHandle,
 *
 * \sa
 * ::cudaMalloc,
 * ::cudaFree,
 * ::cudaIpcGetEventHandle,
 * ::cudaIpcOpenEventHandle,
 * ::cudaIpcGetMemHandle,
 * ::cudaIpcOpenMemHandle,
 */
    pub fn cudaIpcCloseMemHandle(devPtr: *mut ::libc::c_void)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Exit and clean up from CUDA launches
 *
 * \deprecated
 *
 * Note that this function is deprecated because its name does not
 * reflect its behavior.  Its functionality is identical to the
 * non-deprecated function ::cudaDeviceReset(), which should be used
 * instead.
 *
 * Explicitly destroys all cleans up all resources associated with the current
 * device in the current process.  Any subsequent API call to this device will
 * reinitialize the device.
 *
 * Note that this function will reset the device immediately.  It is the caller's
 * responsibility to ensure that the device is not being accessed by any
 * other host threads from the process when this function is called.
 *
 * \return
 * ::cudaSuccess
 * \notefnerr
 *
 * \sa ::cudaDeviceReset
 */
    pub fn cudaThreadExit() -> cudaError_t;
}
extern "C" {
    /**
 * \brief Wait for compute device to finish
 *
 * \deprecated
 *
 * Note that this function is deprecated because its name does not
 * reflect its behavior.  Its functionality is similar to the
 * non-deprecated function ::cudaDeviceSynchronize(), which should be used
 * instead.
 *
 * Blocks until the device has completed all preceding requested tasks.
 * ::cudaThreadSynchronize() returns an error if one of the preceding tasks
 * has failed. If the ::cudaDeviceScheduleBlockingSync flag was set for
 * this device, the host thread will block until the device has finished
 * its work.
 *
 * \return
 * ::cudaSuccess
 * \notefnerr
 *
 * \sa ::cudaDeviceSynchronize
 */
    pub fn cudaThreadSynchronize() -> cudaError_t;
}
extern "C" {
    /**
 * \brief Set resource limits
 *
 * \deprecated
 *
 * Note that this function is deprecated because its name does not
 * reflect its behavior.  Its functionality is identical to the
 * non-deprecated function ::cudaDeviceSetLimit(), which should be used
 * instead.
 *
 * Setting \p limit to \p value is a request by the application to update
 * the current limit maintained by the device.  The driver is free to
 * modify the requested value to meet h/w requirements (this could be
 * clamping to minimum or maximum values, rounding up to nearest element
 * size, etc).  The application can use ::cudaThreadGetLimit() to find out
 * exactly what the limit has been set to.
 *
 * Setting each ::cudaLimit has its own specific restrictions, so each is
 * discussed here.
 *
 * - ::cudaLimitStackSize controls the stack size of each GPU thread.
 *
 * - ::cudaLimitPrintfFifoSize controls the size of the shared FIFO
 *   used by the ::printf() and ::fprintf() device system calls.
 *   Setting ::cudaLimitPrintfFifoSize must be performed before
 *   launching any kernel that uses the ::printf() or ::fprintf() device
 *   system calls, otherwise ::cudaErrorInvalidValue will be returned.
 *
 * - ::cudaLimitMallocHeapSize controls the size of the heap used
 *   by the ::malloc() and ::free() device system calls.  Setting
 *   ::cudaLimitMallocHeapSize must be performed before launching
 *   any kernel that uses the ::malloc() or ::free() device system calls,
 *   otherwise ::cudaErrorInvalidValue will be returned.
 *
 * \param limit - Limit to set
 * \param value - Size in bytes of limit
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorUnsupportedLimit,
 * ::cudaErrorInvalidValue
 * \notefnerr
 *
 * \sa ::cudaDeviceSetLimit
 */
    pub fn cudaThreadSetLimit(limit: cudaLimit, value: usize) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Returns resource limits
 *
 * \deprecated
 *
 * Note that this function is deprecated because its name does not
 * reflect its behavior.  Its functionality is identical to the
 * non-deprecated function ::cudaDeviceGetLimit(), which should be used
 * instead.
 *
 * Returns in \p *pValue the current size of \p limit.  The supported
 * ::cudaLimit values are:
 * - ::cudaLimitStackSize: stack size of each GPU thread;
 * - ::cudaLimitPrintfFifoSize: size of the shared FIFO used by the
 *   ::printf() and ::fprintf() device system calls.
 * - ::cudaLimitMallocHeapSize: size of the heap used by the
 *   ::malloc() and ::free() device system calls;
 *
 * \param limit  - Limit to query
 * \param pValue - Returned size in bytes of limit
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorUnsupportedLimit,
 * ::cudaErrorInvalidValue
 * \notefnerr
 *
 * \sa ::cudaDeviceGetLimit
 */
    pub fn cudaThreadGetLimit(pValue: *mut usize, limit: cudaLimit)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Returns the preferred cache configuration for the current device.
 *
 * \deprecated
 *
 * Note that this function is deprecated because its name does not
 * reflect its behavior.  Its functionality is identical to the
 * non-deprecated function ::cudaDeviceGetCacheConfig(), which should be
 * used instead.
 *
 * On devices where the L1 cache and shared memory use the same hardware
 * resources, this returns through \p pCacheConfig the preferred cache
 * configuration for the current device. This is only a preference. The
 * runtime will use the requested configuration if possible, but it is free to
 * choose a different configuration if required to execute functions.
 *
 * This will return a \p pCacheConfig of ::cudaFuncCachePreferNone on devices
 * where the size of the L1 cache and shared memory are fixed.
 *
 * The supported cache configurations are:
 * - ::cudaFuncCachePreferNone: no preference for shared memory or L1 (default)
 * - ::cudaFuncCachePreferShared: prefer larger shared memory and smaller L1 cache
 * - ::cudaFuncCachePreferL1: prefer larger L1 cache and smaller shared memory
 *
 * \param pCacheConfig - Returned cache configuration
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInitializationError
 * \notefnerr
 *
 * \sa cudaDeviceGetCacheConfig
 */
    pub fn cudaThreadGetCacheConfig(pCacheConfig: *mut cudaFuncCache)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Sets the preferred cache configuration for the current device.
 *
 * \deprecated
 *
 * Note that this function is deprecated because its name does not
 * reflect its behavior.  Its functionality is identical to the
 * non-deprecated function ::cudaDeviceSetCacheConfig(), which should be
 * used instead.
 *
 * On devices where the L1 cache and shared memory use the same hardware
 * resources, this sets through \p cacheConfig the preferred cache
 * configuration for the current device. This is only a preference. The
 * runtime will use the requested configuration if possible, but it is free to
 * choose a different configuration if required to execute the function. Any
 * function preference set via
 * \ref ::cudaFuncSetCacheConfig(const void*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C API)"
 * or
 * \ref ::cudaFuncSetCacheConfig(T*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C++ API)"
 * will be preferred over this device-wide setting. Setting the device-wide
 * cache configuration to ::cudaFuncCachePreferNone will cause subsequent
 * kernel launches to prefer to not change the cache configuration unless
 * required to launch the kernel.
 *
 * This setting does nothing on devices where the size of the L1 cache and
 * shared memory are fixed.
 *
 * Launching a kernel with a different preference than the most recent
 * preference setting may insert a device-side synchronization point.
 *
 * The supported cache configurations are:
 * - ::cudaFuncCachePreferNone: no preference for shared memory or L1 (default)
 * - ::cudaFuncCachePreferShared: prefer larger shared memory and smaller L1 cache
 * - ::cudaFuncCachePreferL1: prefer larger L1 cache and smaller shared memory
 *
 * \param cacheConfig - Requested cache configuration
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInitializationError
 * \notefnerr
 *
 * \sa ::cudaDeviceSetCacheConfig
 */
    pub fn cudaThreadSetCacheConfig(cacheConfig: cudaFuncCache)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Returns the last error from a runtime call
 *
 * Returns the last error that has been produced by any of the runtime calls
 * in the same host thread and resets it to ::cudaSuccess.
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorMissingConfiguration,
 * ::cudaErrorMemoryAllocation,
 * ::cudaErrorInitializationError,
 * ::cudaErrorLaunchFailure,
 * ::cudaErrorLaunchTimeout,
 * ::cudaErrorLaunchOutOfResources,
 * ::cudaErrorInvalidDeviceFunction,
 * ::cudaErrorInvalidConfiguration,
 * ::cudaErrorInvalidDevice,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidPitchValue,
 * ::cudaErrorInvalidSymbol,
 * ::cudaErrorUnmapBufferObjectFailed,
 * ::cudaErrorInvalidHostPointer,
 * ::cudaErrorInvalidDevicePointer,
 * ::cudaErrorInvalidTexture,
 * ::cudaErrorInvalidTextureBinding,
 * ::cudaErrorInvalidChannelDescriptor,
 * ::cudaErrorInvalidMemcpyDirection,
 * ::cudaErrorInvalidFilterSetting,
 * ::cudaErrorInvalidNormSetting,
 * ::cudaErrorUnknown,
 * ::cudaErrorInvalidResourceHandle,
 * ::cudaErrorInsufficientDriver,
 * ::cudaErrorSetOnActiveProcess,
 * ::cudaErrorStartupFailure,
 * \notefnerr
 *
 * \sa ::cudaPeekAtLastError, ::cudaGetErrorName, ::cudaGetErrorString, ::cudaError
 */
    pub fn cudaGetLastError() -> cudaError_t;
}
extern "C" {
    /**
 * \brief Returns the last error from a runtime call
 *
 * Returns the last error that has been produced by any of the runtime calls
 * in the same host thread. Note that this call does not reset the error to
 * ::cudaSuccess like ::cudaGetLastError().
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorMissingConfiguration,
 * ::cudaErrorMemoryAllocation,
 * ::cudaErrorInitializationError,
 * ::cudaErrorLaunchFailure,
 * ::cudaErrorLaunchTimeout,
 * ::cudaErrorLaunchOutOfResources,
 * ::cudaErrorInvalidDeviceFunction,
 * ::cudaErrorInvalidConfiguration,
 * ::cudaErrorInvalidDevice,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidPitchValue,
 * ::cudaErrorInvalidSymbol,
 * ::cudaErrorUnmapBufferObjectFailed,
 * ::cudaErrorInvalidHostPointer,
 * ::cudaErrorInvalidDevicePointer,
 * ::cudaErrorInvalidTexture,
 * ::cudaErrorInvalidTextureBinding,
 * ::cudaErrorInvalidChannelDescriptor,
 * ::cudaErrorInvalidMemcpyDirection,
 * ::cudaErrorInvalidFilterSetting,
 * ::cudaErrorInvalidNormSetting,
 * ::cudaErrorUnknown,
 * ::cudaErrorInvalidResourceHandle,
 * ::cudaErrorInsufficientDriver,
 * ::cudaErrorSetOnActiveProcess,
 * ::cudaErrorStartupFailure,
 * \notefnerr
 *
 * \sa ::cudaGetLastError, ::cudaGetErrorName, ::cudaGetErrorString, ::cudaError
 */
    pub fn cudaPeekAtLastError() -> cudaError_t;
}
extern "C" {
    /**
 * \brief Returns the string representation of an error code enum name
 *
 * Returns a string containing the name of an error code in the enum.  If the error
 * code is not recognized, "unrecognized error code" is returned.
 *
 * \param error - Error code to convert to string
 *
 * \return
 * \p char* pointer to a NULL-terminated string
 *
 * \sa ::cudaGetErrorString, ::cudaGetLastError, ::cudaPeekAtLastError, ::cudaError
 */
    pub fn cudaGetErrorName(error: cudaError_t)
     -> *const ::libc::c_char;
}
extern "C" {
    /**
 * \brief Returns the description string for an error code
 *
 * Returns the description string for an error code.  If the error
 * code is not recognized, "unrecognized error code" is returned.
 *
 * \param error - Error code to convert to string
 *
 * \return
 * \p char* pointer to a NULL-terminated string
 *
 * \sa ::cudaGetErrorName, ::cudaGetLastError, ::cudaPeekAtLastError, ::cudaError
 */
    pub fn cudaGetErrorString(error: cudaError_t)
     -> *const ::libc::c_char;
}
extern "C" {
    /**
 * \brief Returns the number of compute-capable devices
 *
 * Returns in \p *count the number of devices with compute capability greater
 * or equal to 2.0 that are available for execution.  If there is no such
 * device then ::cudaGetDeviceCount() will return ::cudaErrorNoDevice.
 * If no driver can be loaded to determine if any such devices exist then
 * ::cudaGetDeviceCount() will return ::cudaErrorInsufficientDriver.
 *
 * \param count - Returns the number of devices with compute capability
 * greater or equal to 2.0
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorNoDevice,
 * ::cudaErrorInsufficientDriver
 * \notefnerr
 *
 * \sa ::cudaGetDevice, ::cudaSetDevice, ::cudaGetDeviceProperties,
 * ::cudaChooseDevice
 */
    pub fn cudaGetDeviceCount(count: *mut ::libc::c_int)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Returns information about the compute-device
 *
 * Returns in \p *prop the properties of device \p dev. The ::cudaDeviceProp
 * structure is defined as:
 * \code
    struct cudaDeviceProp {
        char name[256];
        size_t totalGlobalMem;
        size_t sharedMemPerBlock;
        int regsPerBlock;
        int warpSize;
        size_t memPitch;
        int maxThreadsPerBlock;
        int maxThreadsDim[3];
        int maxGridSize[3];
        int clockRate;
        size_t totalConstMem;
        int major;
        int minor;
        size_t textureAlignment;
        size_t texturePitchAlignment;
        int deviceOverlap;
        int multiProcessorCount;
        int kernelExecTimeoutEnabled;
        int integrated;
        int canMapHostMemory;
        int computeMode;
        int maxTexture1D;
        int maxTexture1DMipmap;
        int maxTexture1DLinear;
        int maxTexture2D[2];
        int maxTexture2DMipmap[2];
        int maxTexture2DLinear[3];
        int maxTexture2DGather[2];
        int maxTexture3D[3];
        int maxTexture3DAlt[3];
        int maxTextureCubemap;
        int maxTexture1DLayered[2];
        int maxTexture2DLayered[3];
        int maxTextureCubemapLayered[2];
        int maxSurface1D;
        int maxSurface2D[2];
        int maxSurface3D[3];
        int maxSurface1DLayered[2];
        int maxSurface2DLayered[3];
        int maxSurfaceCubemap;
        int maxSurfaceCubemapLayered[2];
        size_t surfaceAlignment;
        int concurrentKernels;
        int ECCEnabled;
        int pciBusID;
        int pciDeviceID;
        int pciDomainID;
        int tccDriver;
        int asyncEngineCount;
        int unifiedAddressing;
        int memoryClockRate;
        int memoryBusWidth;
        int l2CacheSize;
        int maxThreadsPerMultiProcessor;
        int streamPrioritiesSupported;
        int globalL1CacheSupported;
        int localL1CacheSupported;
        size_t sharedMemPerMultiprocessor;
        int regsPerMultiprocessor;
        int managedMemSupported;
        int isMultiGpuBoard;
        int multiGpuBoardGroupID;
        int singleToDoublePrecisionPerfRatio;
        int pageableMemoryAccess;
        int concurrentManagedAccess;
    }
 \endcode
 * where:
 * - \ref ::cudaDeviceProp::name "name[256]" is an ASCII string identifying
 *   the device;
 * - \ref ::cudaDeviceProp::totalGlobalMem "totalGlobalMem" is the total
 *   amount of global memory available on the device in bytes;
 * - \ref ::cudaDeviceProp::sharedMemPerBlock "sharedMemPerBlock" is the
 *   maximum amount of shared memory available to a thread block in bytes;
 * - \ref ::cudaDeviceProp::regsPerBlock "regsPerBlock" is the maximum number
 *   of 32-bit registers available to a thread block;
 * - \ref ::cudaDeviceProp::warpSize "warpSize" is the warp size in threads;
 * - \ref ::cudaDeviceProp::memPitch "memPitch" is the maximum pitch in
 *   bytes allowed by the memory copy functions that involve memory regions
 *   allocated through ::cudaMallocPitch();
 * - \ref ::cudaDeviceProp::maxThreadsPerBlock "maxThreadsPerBlock" is the
 *   maximum number of threads per block;
 * - \ref ::cudaDeviceProp::maxThreadsDim "maxThreadsDim[3]" contains the
 *   maximum size of each dimension of a block;
 * - \ref ::cudaDeviceProp::maxGridSize "maxGridSize[3]" contains the
 *   maximum size of each dimension of a grid;
 * - \ref ::cudaDeviceProp::clockRate "clockRate" is the clock frequency in
 *   kilohertz;
 * - \ref ::cudaDeviceProp::totalConstMem "totalConstMem" is the total amount
 *   of constant memory available on the device in bytes;
 * - \ref ::cudaDeviceProp::major "major",
 *   \ref ::cudaDeviceProp::minor "minor" are the major and minor revision
 *   numbers defining the device's compute capability;
 * - \ref ::cudaDeviceProp::textureAlignment "textureAlignment" is the
 *   alignment requirement; texture base addresses that are aligned to
 *   \ref ::cudaDeviceProp::textureAlignment "textureAlignment" bytes do not
 *   need an offset applied to texture fetches;
 * - \ref ::cudaDeviceProp::texturePitchAlignment "texturePitchAlignment" is the
 *   pitch alignment requirement for 2D texture references that are bound to
 *   pitched memory;
 * - \ref ::cudaDeviceProp::deviceOverlap "deviceOverlap" is 1 if the device
 *   can concurrently copy memory between host and device while executing a
 *   kernel, or 0 if not.  Deprecated, use instead asyncEngineCount.
 * - \ref ::cudaDeviceProp::multiProcessorCount "multiProcessorCount" is the
 *   number of multiprocessors on the device;
 * - \ref ::cudaDeviceProp::kernelExecTimeoutEnabled "kernelExecTimeoutEnabled"
 *   is 1 if there is a run time limit for kernels executed on the device, or
 *   0 if not.
 * - \ref ::cudaDeviceProp::integrated "integrated" is 1 if the device is an
 *   integrated (motherboard) GPU and 0 if it is a discrete (card) component.
 * - \ref ::cudaDeviceProp::canMapHostMemory "canMapHostMemory" is 1 if the
 *   device can map host memory into the CUDA address space for use with
 *   ::cudaHostAlloc()/::cudaHostGetDevicePointer(), or 0 if not;
 * - \ref ::cudaDeviceProp::computeMode "computeMode" is the compute mode
 *   that the device is currently in. Available modes are as follows:
 *   - cudaComputeModeDefault: Default mode - Device is not restricted and
 *     multiple threads can use ::cudaSetDevice() with this device.
 *   - cudaComputeModeExclusive: Compute-exclusive mode - Only one thread will
 *     be able to use ::cudaSetDevice() with this device.
 *   - cudaComputeModeProhibited: Compute-prohibited mode - No threads can use
 *     ::cudaSetDevice() with this device.
 *   - cudaComputeModeExclusiveProcess: Compute-exclusive-process mode - Many
 *     threads in one process will be able to use ::cudaSetDevice() with this device.
 *   <br> If ::cudaSetDevice() is called on an already occupied \p device with
 *   computeMode ::cudaComputeModeExclusive, ::cudaErrorDeviceAlreadyInUse
 *   will be immediately returned indicating the device cannot be used.
 *   When an occupied exclusive mode device is chosen with ::cudaSetDevice,
 *   all subsequent non-device management runtime functions will return
 *   ::cudaErrorDevicesUnavailable.
 * - \ref ::cudaDeviceProp::maxTexture1D "maxTexture1D" is the maximum 1D
 *   texture size.
 * - \ref ::cudaDeviceProp::maxTexture1DMipmap "maxTexture1DMipmap" is the maximum
 *   1D mipmapped texture texture size.
 * - \ref ::cudaDeviceProp::maxTexture1DLinear "maxTexture1DLinear" is the maximum
 *   1D texture size for textures bound to linear memory.
 * - \ref ::cudaDeviceProp::maxTexture2D "maxTexture2D[2]" contains the maximum
 *   2D texture dimensions.
 * - \ref ::cudaDeviceProp::maxTexture2DMipmap "maxTexture2DMipmap[2]" contains the
 *   maximum 2D mipmapped texture dimensions.
 * - \ref ::cudaDeviceProp::maxTexture2DLinear "maxTexture2DLinear[3]" contains the
 *   maximum 2D texture dimensions for 2D textures bound to pitch linear memory.
 * - \ref ::cudaDeviceProp::maxTexture2DGather "maxTexture2DGather[2]" contains the
 *   maximum 2D texture dimensions if texture gather operations have to be performed.
 * - \ref ::cudaDeviceProp::maxTexture3D "maxTexture3D[3]" contains the maximum
 *   3D texture dimensions.
 * - \ref ::cudaDeviceProp::maxTexture3DAlt "maxTexture3DAlt[3]"
 *   contains the maximum alternate 3D texture dimensions.
 * - \ref ::cudaDeviceProp::maxTextureCubemap "maxTextureCubemap" is the
 *   maximum cubemap texture width or height.
 * - \ref ::cudaDeviceProp::maxTexture1DLayered "maxTexture1DLayered[2]" contains
 *   the maximum 1D layered texture dimensions.
 * - \ref ::cudaDeviceProp::maxTexture2DLayered "maxTexture2DLayered[3]" contains
 *   the maximum 2D layered texture dimensions.
 * - \ref ::cudaDeviceProp::maxTextureCubemapLayered "maxTextureCubemapLayered[2]"
 *   contains the maximum cubemap layered texture dimensions.
 * - \ref ::cudaDeviceProp::maxSurface1D "maxSurface1D" is the maximum 1D
 *   surface size.
 * - \ref ::cudaDeviceProp::maxSurface2D "maxSurface2D[2]" contains the maximum
 *   2D surface dimensions.
 * - \ref ::cudaDeviceProp::maxSurface3D "maxSurface3D[3]" contains the maximum
 *   3D surface dimensions.
 * - \ref ::cudaDeviceProp::maxSurface1DLayered "maxSurface1DLayered[2]" contains
 *   the maximum 1D layered surface dimensions.
 * - \ref ::cudaDeviceProp::maxSurface2DLayered "maxSurface2DLayered[3]" contains
 *   the maximum 2D layered surface dimensions.
 * - \ref ::cudaDeviceProp::maxSurfaceCubemap "maxSurfaceCubemap" is the maximum
 *   cubemap surface width or height.
 * - \ref ::cudaDeviceProp::maxSurfaceCubemapLayered "maxSurfaceCubemapLayered[2]"
 *   contains the maximum cubemap layered surface dimensions.
 * - \ref ::cudaDeviceProp::surfaceAlignment "surfaceAlignment" specifies the
 *   alignment requirements for surfaces.
 * - \ref ::cudaDeviceProp::concurrentKernels "concurrentKernels" is 1 if the
 *   device supports executing multiple kernels within the same context
 *   simultaneously, or 0 if not. It is not guaranteed that multiple kernels
 *   will be resident on the device concurrently so this feature should not be
 *   relied upon for correctness;
 * - \ref ::cudaDeviceProp::ECCEnabled "ECCEnabled" is 1 if the device has ECC
 *   support turned on, or 0 if not.
 * - \ref ::cudaDeviceProp::pciBusID "pciBusID" is the PCI bus identifier of
 *   the device.
 * - \ref ::cudaDeviceProp::pciDeviceID "pciDeviceID" is the PCI device
 *   (sometimes called slot) identifier of the device.
 * - \ref ::cudaDeviceProp::pciDomainID "pciDomainID" is the PCI domain identifier
 *   of the device.
 * - \ref ::cudaDeviceProp::tccDriver "tccDriver" is 1 if the device is using a
 *   TCC driver or 0 if not.
 * - \ref ::cudaDeviceProp::asyncEngineCount "asyncEngineCount" is 1 when the
 *   device can concurrently copy memory between host and device while executing
 *   a kernel. It is 2 when the device can concurrently copy memory between host
 *   and device in both directions and execute a kernel at the same time. It is
 *   0 if neither of these is supported.
 * - \ref ::cudaDeviceProp::unifiedAddressing "unifiedAddressing" is 1 if the device
 *   shares a unified address space with the host and 0 otherwise.
 * - \ref ::cudaDeviceProp::memoryClockRate "memoryClockRate" is the peak memory
 *   clock frequency in kilohertz.
 * - \ref ::cudaDeviceProp::memoryBusWidth "memoryBusWidth" is the memory bus width
 *   in bits.
 * - \ref ::cudaDeviceProp::l2CacheSize "l2CacheSize" is L2 cache size in bytes.
 * - \ref ::cudaDeviceProp::maxThreadsPerMultiProcessor "maxThreadsPerMultiProcessor"
 *   is the number of maximum resident threads per multiprocessor.
 * - \ref ::cudaDeviceProp::streamPrioritiesSupported "streamPrioritiesSupported"
 *   is 1 if the device supports stream priorities, or 0 if it is not supported.
 * - \ref ::cudaDeviceProp::globalL1CacheSupported "globalL1CacheSupported"
 *   is 1 if the device supports caching of globals in L1 cache, or 0 if it is not supported.
 * - \ref ::cudaDeviceProp::localL1CacheSupported "localL1CacheSupported"
 *   is 1 if the device supports caching of locals in L1 cache, or 0 if it is not supported.
 * - \ref ::cudaDeviceProp::sharedMemPerMultiprocessor "sharedMemPerMultiprocessor" is the
 *   maximum amount of shared memory available to a multiprocessor in bytes; this amount is
 *   shared by all thread blocks simultaneously resident on a multiprocessor;
 * - \ref ::cudaDeviceProp::regsPerMultiprocessor "regsPerMultiprocessor" is the maximum number
 *   of 32-bit registers available to a multiprocessor; this number is shared
 *   by all thread blocks simultaneously resident on a multiprocessor;
 * - \ref ::cudaDeviceProp::managedMemory "managedMemory"
 *   is 1 if the device supports allocating managed memory on this system, or 0 if it is not supported.
 * - \ref ::cudaDeviceProp::isMultiGpuBoard "isMultiGpuBoard"
 *   is 1 if the device is on a multi-GPU board (e.g. Gemini cards), and 0 if not;
 * - \ref ::cudaDeviceProp::multiGpuBoardGroupID "multiGpuBoardGroupID" is a unique identifier
 *   for a group of devices associated with the same board.
 *   Devices on the same multi-GPU board will share the same identifier;
 * - \ref ::cudaDeviceProp::singleToDoublePrecisionPerfRatio "singleToDoublePrecisionPerfRatio"
 *   is the ratio of single precision performance (in floating-point operations per second)
 *   to double precision performance.
 * - \ref ::cudaDeviceProp::pageableMemoryAccess "pageableMemoryAccess" is 1 if the device supports
 *   coherently accessing pageable memory without calling cudaHostRegister on it, and 0 otherwise.
 * - \ref ::cudaDeviceProp::concurrentManagedAccess "concurrentManagedAccess" is 1 if the device can
 *   coherently access managed memory concurrently with the CPU, and 0 otherwise.
 *
 * \param prop   - Properties for the specified device
 * \param device - Device number to get properties for
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidDevice
 *
 * \sa ::cudaGetDeviceCount, ::cudaGetDevice, ::cudaSetDevice, ::cudaChooseDevice,
 * ::cudaDeviceGetAttribute
 */
    pub fn cudaGetDeviceProperties(prop: *mut cudaDeviceProp,
                                   device: ::libc::c_int)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Returns information about the device
 *
 * Returns in \p *value the integer value of the attribute \p attr on device
 * \p device. The supported attributes are:
 * - ::cudaDevAttrMaxThreadsPerBlock: Maximum number of threads per block;
 * - ::cudaDevAttrMaxBlockDimX: Maximum x-dimension of a block;
 * - ::cudaDevAttrMaxBlockDimY: Maximum y-dimension of a block;
 * - ::cudaDevAttrMaxBlockDimZ: Maximum z-dimension of a block;
 * - ::cudaDevAttrMaxGridDimX: Maximum x-dimension of a grid;
 * - ::cudaDevAttrMaxGridDimY: Maximum y-dimension of a grid;
 * - ::cudaDevAttrMaxGridDimZ: Maximum z-dimension of a grid;
 * - ::cudaDevAttrMaxSharedMemoryPerBlock: Maximum amount of shared memory
 *   available to a thread block in bytes;
 * - ::cudaDevAttrTotalConstantMemory: Memory available on device for
 *   __constant__ variables in a CUDA C kernel in bytes;
 * - ::cudaDevAttrWarpSize: Warp size in threads;
 * - ::cudaDevAttrMaxPitch: Maximum pitch in bytes allowed by the memory copy
 *   functions that involve memory regions allocated through ::cudaMallocPitch();
 * - ::cudaDevAttrMaxTexture1DWidth: Maximum 1D texture width;
 * - ::cudaDevAttrMaxTexture1DLinearWidth: Maximum width for a 1D texture bound
 *   to linear memory;
 * - ::cudaDevAttrMaxTexture1DMipmappedWidth: Maximum mipmapped 1D texture width;
 * - ::cudaDevAttrMaxTexture2DWidth: Maximum 2D texture width;
 * - ::cudaDevAttrMaxTexture2DHeight: Maximum 2D texture height;
 * - ::cudaDevAttrMaxTexture2DLinearWidth: Maximum width for a 2D texture
 *   bound to linear memory;
 * - ::cudaDevAttrMaxTexture2DLinearHeight: Maximum height for a 2D texture
 *   bound to linear memory;
 * - ::cudaDevAttrMaxTexture2DLinearPitch: Maximum pitch in bytes for a 2D
 *   texture bound to linear memory;
 * - ::cudaDevAttrMaxTexture2DMipmappedWidth: Maximum mipmapped 2D texture
 *   width;
 * - ::cudaDevAttrMaxTexture2DMipmappedHeight: Maximum mipmapped 2D texture
 *   height;
 * - ::cudaDevAttrMaxTexture3DWidth: Maximum 3D texture width;
 * - ::cudaDevAttrMaxTexture3DHeight: Maximum 3D texture height;
 * - ::cudaDevAttrMaxTexture3DDepth: Maximum 3D texture depth;
 * - ::cudaDevAttrMaxTexture3DWidthAlt: Alternate maximum 3D texture width,
 *   0 if no alternate maximum 3D texture size is supported;
 * - ::cudaDevAttrMaxTexture3DHeightAlt: Alternate maximum 3D texture height,
 *   0 if no alternate maximum 3D texture size is supported;
 * - ::cudaDevAttrMaxTexture3DDepthAlt: Alternate maximum 3D texture depth,
 *   0 if no alternate maximum 3D texture size is supported;
 * - ::cudaDevAttrMaxTextureCubemapWidth: Maximum cubemap texture width or
 *   height;
 * - ::cudaDevAttrMaxTexture1DLayeredWidth: Maximum 1D layered texture width;
 * - ::cudaDevAttrMaxTexture1DLayeredLayers: Maximum layers in a 1D layered
 *   texture;
 * - ::cudaDevAttrMaxTexture2DLayeredWidth: Maximum 2D layered texture width;
 * - ::cudaDevAttrMaxTexture2DLayeredHeight: Maximum 2D layered texture height;
 * - ::cudaDevAttrMaxTexture2DLayeredLayers: Maximum layers in a 2D layered
 *   texture;
 * - ::cudaDevAttrMaxTextureCubemapLayeredWidth: Maximum cubemap layered
 *   texture width or height;
 * - ::cudaDevAttrMaxTextureCubemapLayeredLayers: Maximum layers in a cubemap
 *   layered texture;
 * - ::cudaDevAttrMaxSurface1DWidth: Maximum 1D surface width;
 * - ::cudaDevAttrMaxSurface2DWidth: Maximum 2D surface width;
 * - ::cudaDevAttrMaxSurface2DHeight: Maximum 2D surface height;
 * - ::cudaDevAttrMaxSurface3DWidth: Maximum 3D surface width;
 * - ::cudaDevAttrMaxSurface3DHeight: Maximum 3D surface height;
 * - ::cudaDevAttrMaxSurface3DDepth: Maximum 3D surface depth;
 * - ::cudaDevAttrMaxSurface1DLayeredWidth: Maximum 1D layered surface width;
 * - ::cudaDevAttrMaxSurface1DLayeredLayers: Maximum layers in a 1D layered
 *   surface;
 * - ::cudaDevAttrMaxSurface2DLayeredWidth: Maximum 2D layered surface width;
 * - ::cudaDevAttrMaxSurface2DLayeredHeight: Maximum 2D layered surface height;
 * - ::cudaDevAttrMaxSurface2DLayeredLayers: Maximum layers in a 2D layered
 *   surface;
 * - ::cudaDevAttrMaxSurfaceCubemapWidth: Maximum cubemap surface width;
 * - ::cudaDevAttrMaxSurfaceCubemapLayeredWidth: Maximum cubemap layered
 *   surface width;
 * - ::cudaDevAttrMaxSurfaceCubemapLayeredLayers: Maximum layers in a cubemap
 *   layered surface;
 * - ::cudaDevAttrMaxRegistersPerBlock: Maximum number of 32-bit registers
 *   available to a thread block;
 * - ::cudaDevAttrClockRate: Peak clock frequency in kilohertz;
 * - ::cudaDevAttrTextureAlignment: Alignment requirement; texture base
 *   addresses aligned to ::textureAlign bytes do not need an offset applied
 *   to texture fetches;
 * - ::cudaDevAttrTexturePitchAlignment: Pitch alignment requirement for 2D
 *   texture references bound to pitched memory;
 * - ::cudaDevAttrGpuOverlap: 1 if the device can concurrently copy memory
 *   between host and device while executing a kernel, or 0 if not;
 * - ::cudaDevAttrMultiProcessorCount: Number of multiprocessors on the device;
 * - ::cudaDevAttrKernelExecTimeout: 1 if there is a run time limit for kernels
 *   executed on the device, or 0 if not;
 * - ::cudaDevAttrIntegrated: 1 if the device is integrated with the memory
 *   subsystem, or 0 if not;
 * - ::cudaDevAttrCanMapHostMemory: 1 if the device can map host memory into
 *   the CUDA address space, or 0 if not;
 * - ::cudaDevAttrComputeMode: Compute mode is the compute mode that the device
 *   is currently in. Available modes are as follows:
 *   - ::cudaComputeModeDefault: Default mode - Device is not restricted and
 *     multiple threads can use ::cudaSetDevice() with this device.
 *   - ::cudaComputeModeExclusive: Compute-exclusive mode - Only one thread will
 *     be able to use ::cudaSetDevice() with this device.
 *   - ::cudaComputeModeProhibited: Compute-prohibited mode - No threads can use
 *     ::cudaSetDevice() with this device.
 *   - ::cudaComputeModeExclusiveProcess: Compute-exclusive-process mode - Many
 *     threads in one process will be able to use ::cudaSetDevice() with this
 *     device.
 * - ::cudaDevAttrConcurrentKernels: 1 if the device supports executing
 *   multiple kernels within the same context simultaneously, or 0 if
 *   not. It is not guaranteed that multiple kernels will be resident on the
 *   device concurrently so this feature should not be relied upon for
 *   correctness;
 * - ::cudaDevAttrEccEnabled: 1 if error correction is enabled on the device,
 *   0 if error correction is disabled or not supported by the device;
 * - ::cudaDevAttrPciBusId: PCI bus identifier of the device;
 * - ::cudaDevAttrPciDeviceId: PCI device (also known as slot) identifier of
 *   the device;
 * - ::cudaDevAttrTccDriver: 1 if the device is using a TCC driver. TCC is only
 *   available on Tesla hardware running Windows Vista or later;
 * - ::cudaDevAttrMemoryClockRate: Peak memory clock frequency in kilohertz;
 * - ::cudaDevAttrGlobalMemoryBusWidth: Global memory bus width in bits;
 * - ::cudaDevAttrL2CacheSize: Size of L2 cache in bytes. 0 if the device
 *   doesn't have L2 cache;
 * - ::cudaDevAttrMaxThreadsPerMultiProcessor: Maximum resident threads per
 *   multiprocessor;
 * - ::cudaDevAttrUnifiedAddressing: 1 if the device shares a unified address
 *   space with the host, or 0 if not;
 * - ::cudaDevAttrComputeCapabilityMajor: Major compute capability version
 *   number;
 * - ::cudaDevAttrComputeCapabilityMinor: Minor compute capability version
 *   number;
 * - ::cudaDevAttrStreamPrioritiesSupported: 1 if the device supports stream
 *   priorities, or 0 if not;
 * - ::cudaDevAttrGlobalL1CacheSupported: 1 if device supports caching globals
 *    in L1 cache, 0 if not;
 * - ::cudaDevAttrGlobalL1CacheSupported: 1 if device supports caching locals
 *    in L1 cache, 0 if not;
 * - ::cudaDevAttrMaxSharedMemoryPerMultiprocessor: Maximum amount of shared memory
 *   available to a multiprocessor in bytes; this amount is shared by all
 *   thread blocks simultaneously resident on a multiprocessor;
 * - ::cudaDevAttrMaxRegistersPerMultiprocessor: Maximum number of 32-bit registers
 *   available to a multiprocessor; this number is shared by all thread blocks
 *   simultaneously resident on a multiprocessor;
 * - ::cudaDevAttrManagedMemSupported: 1 if device supports allocating
 *   managed memory, 0 if not;
 * - ::cudaDevAttrIsMultiGpuBoard: 1 if device is on a multi-GPU board, 0 if not;
 * - ::cudaDevAttrMultiGpuBoardGroupID: Unique identifier for a group of devices on the
 *   same multi-GPU board;
 * - ::cudaDevAttrHostNativeAtomicSupported: 1 if the link between the device and the
 *   host supports native atomic operations;
 * - ::cudaDevAttrSingleToDoublePrecisionPerfRatio: Ratio of single precision performance
 *   (in floating-point operations per second) to double precision performance;
 * - ::cudaDevAttrPageableMemoryAccess: 1 if the device supports coherently accessing
 *   pageable memory without calling cudaHostRegister on it, and 0 otherwise.
 * - ::cudaDevAttrConcurrentManagedAccess: 1 if the device can coherently access managed
 *   memory concurrently with the CPU, and 0 otherwise.
 * - ::cudaDevAttrComputePreemptionSupported: 1 if the device supports
 *   Compute Preemption, 0 if not.
 * - ::cudaDevAttrCanUseHostPointerForRegisteredMem: 1 if the device can access host
 *   registered memory at the same virtual address as the CPU, and 0 otherwise.
 *
 * \param value  - Returned device attribute value
 * \param attr   - Device attribute to query
 * \param device - Device number to query
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidDevice,
 * ::cudaErrorInvalidValue
 * \notefnerr
 *
 * \sa ::cudaGetDeviceCount, ::cudaGetDevice, ::cudaSetDevice, ::cudaChooseDevice,
 * ::cudaGetDeviceProperties
 */
    pub fn cudaDeviceGetAttribute(value: *mut ::libc::c_int,
                                  attr: cudaDeviceAttr,
                                  device: ::libc::c_int)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Queries attributes of the link between two devices.
 *
 * Returns in \p *value the value of the requested attribute \p attrib of the
 * link between \p srcDevice and \p dstDevice. The supported attributes are:
 * - ::CudaDevP2PAttrPerformanceRank: A relative value indicating the
 *   performance of the link between two devices. Lower value means better
 *   performance (0 being the value used for most performant link).
 * - ::CudaDevP2PAttrAccessSupported: 1 if peer access is enabled.
 * - ::CudaDevP2PAttrNativeAtomicSupported: 1 if native atomic operations over
 *   the link are supported.
 *
 * Returns ::cudaErrorInvalidDevice if \p srcDevice or \p dstDevice are not valid
 * or if they represent the same device.
 *
 * Returns ::cudaErrorInvalidValue if \p attrib is not valid or if \p value is
 * a null pointer.
 *
 * \param value         - Returned value of the requested attribute
 * \param attrib        - The requested attribute of the link between \p srcDevice and \p dstDevice.
 * \param srcDevice     - The source device of the target link.
 * \param dstDevice     - The destination device of the target link.
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidDevice,
 * ::cudaErrorInvalidValue
 * \notefnerr
 *
 * \sa ::cudaCtxEnablePeerAccess,
 * ::cudaCtxDisablePeerAccess,
 * ::cudaCtxCanAccessPeer
 */
    pub fn cudaDeviceGetP2PAttribute(value: *mut ::libc::c_int,
                                     attr: cudaDeviceP2PAttr,
                                     srcDevice: ::libc::c_int,
                                     dstDevice: ::libc::c_int)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Select compute-device which best matches criteria
 *
 * Returns in \p *device the device which has properties that best match
 * \p *prop.
 *
 * \param device - Device with best match
 * \param prop   - Desired device properties
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue
 * \notefnerr
 *
 * \sa ::cudaGetDeviceCount, ::cudaGetDevice, ::cudaSetDevice,
 * ::cudaGetDeviceProperties
 */
    pub fn cudaChooseDevice(device: *mut ::libc::c_int,
                            prop: *const cudaDeviceProp) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Set device to be used for GPU executions
 *
 * Sets \p device as the current device for the calling host thread.
 * Valid device id's are 0 to (::cudaGetDeviceCount() - 1).
 *
 * Any device memory subsequently allocated from this host thread
 * using ::cudaMalloc(), ::cudaMallocPitch() or ::cudaMallocArray()
 * will be physically resident on \p device.  Any host memory allocated
 * from this host thread using ::cudaMallocHost() or ::cudaHostAlloc()
 * or ::cudaHostRegister() will have its lifetime associated  with
 * \p device.  Any streams or events created from this host thread will
 * be associated with \p device.  Any kernels launched from this host
 * thread using the <<<>>> operator or ::cudaLaunchKernel() will be executed
 * on \p device.
 *
 * This call may be made from any host thread, to any device, and at
 * any time.  This function will do no synchronization with the previous
 * or new device, and should be considered a very low overhead call.
 *
 * \param device - Device on which the active host thread should execute the
 * device code.
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidDevice,
 * ::cudaErrorDeviceAlreadyInUse
 * \notefnerr
 *
 * \sa ::cudaGetDeviceCount, ::cudaGetDevice, ::cudaGetDeviceProperties,
 * ::cudaChooseDevice
 */
    pub fn cudaSetDevice(device: ::libc::c_int) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Returns which device is currently being used
 *
 * Returns in \p *device the current device for the calling host thread.
 *
 * \param device - Returns the device on which the active host thread
 * executes the device code.
 *
 * \return
 * ::cudaSuccess
 * \notefnerr
 *
 * \sa ::cudaGetDeviceCount, ::cudaSetDevice, ::cudaGetDeviceProperties,
 * ::cudaChooseDevice
 */
    pub fn cudaGetDevice(device: *mut ::libc::c_int) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Set a list of devices that can be used for CUDA
 *
 * Sets a list of devices for CUDA execution in priority order using
 * \p device_arr. The parameter \p len specifies the number of elements in the
 * list.  CUDA will try devices from the list sequentially until it finds one
 * that works.  If this function is not called, or if it is called with a \p len
 * of 0, then CUDA will go back to its default behavior of trying devices
 * sequentially from a default list containing all of the available CUDA
 * devices in the system. If a specified device ID in the list does not exist,
 * this function will return ::cudaErrorInvalidDevice. If \p len is not 0 and
 * \p device_arr is NULL or if \p len exceeds the number of devices in
 * the system, then ::cudaErrorInvalidValue is returned.
 *
 * \param device_arr - List of devices to try
 * \param len        - Number of devices in specified list
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidDevice
 * \notefnerr
 *
 * \sa ::cudaGetDeviceCount, ::cudaSetDevice, ::cudaGetDeviceProperties,
 * ::cudaSetDeviceFlags,
 * ::cudaChooseDevice
 */
    pub fn cudaSetValidDevices(device_arr: *mut ::libc::c_int,
                               len: ::libc::c_int) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Sets flags to be used for device executions
 *
 * Records \p flags as the flags to use when initializing the current
 * device.  If no device has been made current to the calling thread,
 * then \p flags will be applied to the initialization of any device
 * initialized by the calling host thread, unless that device has had
 * its initialization flags set explicitly by this or any host thread.
 *
 * If the current device has been set and that device has already been
 * initialized then this call will fail with the error
 * ::cudaErrorSetOnActiveProcess.  In this case it is necessary
 * to reset \p device using ::cudaDeviceReset() before the device's
 * initialization flags may be set.
 *
 * The two LSBs of the \p flags parameter can be used to control how the CPU
 * thread interacts with the OS scheduler when waiting for results from the
 * device.
 *
 * - ::cudaDeviceScheduleAuto: The default value if the \p flags parameter is
 * zero, uses a heuristic based on the number of active CUDA contexts in the
 * process \p C and the number of logical processors in the system \p P. If
 * \p C \> \p P, then CUDA will yield to other OS threads when waiting for the
 * device, otherwise CUDA will not yield while waiting for results and
 * actively spin on the processor.
 * - ::cudaDeviceScheduleSpin: Instruct CUDA to actively spin when waiting for
 * results from the device. This can decrease latency when waiting for the
 * device, but may lower the performance of CPU threads if they are performing
 * work in parallel with the CUDA thread.
 * - ::cudaDeviceScheduleYield: Instruct CUDA to yield its thread when waiting
 * for results from the device. This can increase latency when waiting for the
 * device, but can increase the performance of CPU threads performing work in
 * parallel with the device.
 * - ::cudaDeviceScheduleBlockingSync: Instruct CUDA to block the CPU thread
 * on a synchronization primitive when waiting for the device to finish work.
 * - ::cudaDeviceBlockingSync: Instruct CUDA to block the CPU thread on a
 * synchronization primitive when waiting for the device to finish work. <br>
 * \ref deprecated "Deprecated:" This flag was deprecated as of CUDA 4.0 and
 * replaced with ::cudaDeviceScheduleBlockingSync.
 * - ::cudaDeviceMapHost: This flag enables allocating pinned
 * host memory that is accessible to the device. It is implicit for the
 * runtime but may be absent if a context is created using the driver API.
 * If this flag is not set, ::cudaHostGetDevicePointer() will always return
 * a failure code.
 * - ::cudaDeviceLmemResizeToMax: Instruct CUDA to not reduce local memory
 * after resizing local memory for a kernel. This can prevent thrashing by
 * local memory allocations when launching many kernels with high local
 * memory usage at the cost of potentially increased memory usage.
 *
 * \param flags - Parameters for device operation
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidDevice,
 * ::cudaErrorSetOnActiveProcess
 *
 * \sa ::cudaGetDeviceFlags, ::cudaGetDeviceCount, ::cudaGetDevice, ::cudaGetDeviceProperties,
 * ::cudaSetDevice, ::cudaSetValidDevices,
 * ::cudaChooseDevice
 */
    pub fn cudaSetDeviceFlags(flags: ::libc::c_uint) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Gets the flags for the current device
 *
 * Returns in \p flags the flags for the current device.  If there is a
 * current device for the calling thread, and the device has been initialized
 * or flags have been set on that device specifically, the flags for the
 * device are returned.  If there is no current device, but flags have been
 * set for the thread with ::cudaSetDeviceFlags, the thread flags are returned.
 * Finally, if there is no current device and no thread flags, the flags for
 * the first device are returned, which may be the default flags.  Compare
 * to the behavior of ::cudaSetDeviceFlags.
 *
 * Typically, the flags returned should match the behavior that will be seen
 * if the calling thread uses a device after this call, without any change to
 * the flags or current device inbetween by this or another thread.  Note that
 * if the device is not initialized, it is possible for another thread to
 * change the flags for the current device before it is initialized.
 * Additionally, when using exclusive mode, if this thread has not requested a
 * specific device, it may use a device other than the first device, contrary
 * to the assumption made by this function.
 *
 * If a context has been created via the driver API and is current to the
 * calling thread, the flags for that context are always returned.
 *
 * Flags returned by this function may specifically include ::cudaDeviceMapHost
 * even though it is not accepted by ::cudaSetDeviceFlags because it is
 * implicit in runtime API flags.  The reason for this is that the current
 * context may have been created via the driver API in which case the flag is
 * not implicit and may be unset.
 *
 * \param flags - Pointer to store the device flags
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidDevice
 *
 * \sa ::cudaGetDevice, ::cudaGetDeviceProperties,
 * ::cudaSetDevice, ::cudaSetDeviceFlags
 */
    pub fn cudaGetDeviceFlags(flags: *mut ::libc::c_uint)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Create an asynchronous stream
 *
 * Creates a new asynchronous stream.
 *
 * \param pStream - Pointer to new stream identifier
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue
 * \notefnerr
 *
 * \sa ::cudaStreamCreateWithPriority,
 * ::cudaStreamCreateWithFlags,
 * ::cudaStreamGetPriority,
 * ::cudaStreamGetFlags,
 * ::cudaStreamQuery,
 * ::cudaStreamSynchronize,
 * ::cudaStreamWaitEvent,
 * ::cudaStreamAddCallback,
 * ::cudaStreamDestroy
 */
    pub fn cudaStreamCreate(pStream: *mut cudaStream_t) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Create an asynchronous stream
 *
 * Creates a new asynchronous stream.  The \p flags argument determines the
 * behaviors of the stream.  Valid values for \p flags are
 * - ::cudaStreamDefault: Default stream creation flag.
 * - ::cudaStreamNonBlocking: Specifies that work running in the created
 *   stream may run concurrently with work in stream 0 (the NULL stream), and that
 *   the created stream should perform no implicit synchronization with stream 0.
 *
 * \param pStream - Pointer to new stream identifier
 * \param flags   - Parameters for stream creation
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue
 * \notefnerr
 *
 * \sa ::cudaStreamCreate,
 * ::cudaStreamCreateWithPriority,
 * ::cudaStreamGetFlags,
 * ::cudaStreamQuery,
 * ::cudaStreamSynchronize,
 * ::cudaStreamWaitEvent,
 * ::cudaStreamAddCallback,
 * ::cudaStreamDestroy
 */
    pub fn cudaStreamCreateWithFlags(pStream: *mut cudaStream_t,
                                     flags: ::libc::c_uint)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Create an asynchronous stream with the specified priority
 *
 * Creates a stream with the specified priority and returns a handle in \p pStream.
 * This API alters the scheduler priority of work in the stream. Work in a higher
 * priority stream may preempt work already executing in a low priority stream.
 *
 * \p priority follows a convention where lower numbers represent higher priorities.
 * '0' represents default priority. The range of meaningful numerical priorities can
 * be queried using ::cudaDeviceGetStreamPriorityRange. If the specified priority is
 * outside the numerical range returned by ::cudaDeviceGetStreamPriorityRange,
 * it will automatically be clamped to the lowest or the highest number in the range.
 *
 * \param pStream  - Pointer to new stream identifier
 * \param flags    - Flags for stream creation. See ::cudaStreamCreateWithFlags for a list of valid flags that can be passed
 * \param priority - Priority of the stream. Lower numbers represent higher priorities.
 *                   See ::cudaDeviceGetStreamPriorityRange for more information about
 *                   the meaningful stream priorities that can be passed.
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue
 * \notefnerr
 *
 * \note Stream priorities are supported only on GPUs
 * with compute capability 3.5 or higher.
 *
 * \note In the current implementation, only compute kernels launched in
 * priority streams are affected by the stream's priority. Stream priorities have
 * no effect on host-to-device and device-to-host memory operations.
 *
 * \sa ::cudaStreamCreate,
 * ::cudaStreamCreateWithFlags,
 * ::cudaDeviceGetStreamPriorityRange,
 * ::cudaStreamGetPriority,
 * ::cudaStreamQuery,
 * ::cudaStreamWaitEvent,
 * ::cudaStreamAddCallback,
 * ::cudaStreamSynchronize,
 * ::cudaStreamDestroy
 */
    pub fn cudaStreamCreateWithPriority(pStream: *mut cudaStream_t,
                                        flags: ::libc::c_uint,
                                        priority: ::libc::c_int)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Query the priority of a stream
 *
 * Query the priority of a stream. The priority is returned in in \p priority.
 * Note that if the stream was created with a priority outside the meaningful
 * numerical range returned by ::cudaDeviceGetStreamPriorityRange,
 * this function returns the clamped priority.
 * See ::cudaStreamCreateWithPriority for details about priority clamping.
 *
 * \param hStream    - Handle to the stream to be queried
 * \param priority   - Pointer to a signed integer in which the stream's priority is returned
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidResourceHandle
 * \notefnerr
 *
 * \sa ::cudaStreamCreateWithPriority,
 * ::cudaDeviceGetStreamPriorityRange,
 * ::cudaStreamGetFlags
 */
    pub fn cudaStreamGetPriority(hStream: cudaStream_t,
                                 priority: *mut ::libc::c_int)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Query the flags of a stream
 *
 * Query the flags of a stream. The flags are returned in \p flags.
 * See ::cudaStreamCreateWithFlags for a list of valid flags.
 *
 * \param hStream - Handle to the stream to be queried
 * \param flags   - Pointer to an unsigned integer in which the stream's flags are returned
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidResourceHandle
 * \notefnerr
 *
 * \sa ::cudaStreamCreateWithPriority,
 * ::cudaStreamCreateWithFlags,
 * ::cudaStreamGetPriority
 */
    pub fn cudaStreamGetFlags(hStream: cudaStream_t,
                              flags: *mut ::libc::c_uint)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Destroys and cleans up an asynchronous stream
 *
 * Destroys and cleans up the asynchronous stream specified by \p stream.
 *
 * In case the device is still doing work in the stream \p stream
 * when ::cudaStreamDestroy() is called, the function will return immediately
 * and the resources associated with \p stream will be released automatically
 * once the device has completed all work in \p stream.
 *
 * \param stream - Stream identifier
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidResourceHandle
 * \notefnerr
 *
 * \sa ::cudaStreamCreate, ::cudaStreamCreateWithFlags, ::cudaStreamQuery, ::cudaStreamWaitEvent, ::cudaStreamSynchronize, ::cudaStreamAddCallback
 */
    pub fn cudaStreamDestroy(stream: cudaStream_t) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Make a compute stream wait on an event
 *
 * Makes all future work submitted to \p stream wait until \p event reports
 * completion before beginning execution.  This synchronization will be
 * performed efficiently on the device.  The event \p event may
 * be from a different context than \p stream, in which case this function
 * will perform cross-device synchronization.
 *
 * The stream \p stream will wait only for the completion of the most recent
 * host call to ::cudaEventRecord() on \p event.  Once this call has returned,
 * any functions (including ::cudaEventRecord() and ::cudaEventDestroy()) may be
 * called on \p event again, and the subsequent calls will not have any effect
 * on \p stream.
 *
 * If ::cudaEventRecord() has not been called on \p event, this call acts as if
 * the record has already completed, and so is a functional no-op.
 *
 * \param stream - Stream to wait
 * \param event  - Event to wait on
 * \param flags  - Parameters for the operation (must be 0)
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidResourceHandle
 * \note_null_stream
 * \notefnerr
 *
 * \sa ::cudaStreamCreate, ::cudaStreamCreateWithFlags, ::cudaStreamQuery, ::cudaStreamSynchronize, ::cudaStreamAddCallback, ::cudaStreamDestroy
 */
    pub fn cudaStreamWaitEvent(stream: cudaStream_t, event: cudaEvent_t,
                               flags: ::libc::c_uint) -> cudaError_t;
}
/**
 * Type of stream callback functions.
 * \param stream The stream as passed to ::cudaStreamAddCallback, may be NULL.
 * \param status ::cudaSuccess or any persistent error on the stream.
 * \param userData User parameter provided at registration.
 */
pub type cudaStreamCallback_t =
    ::std::option::Option<unsafe extern "C" fn(stream: cudaStream_t,
                                               status: cudaError_t,
                                               userData:
                                                   *mut ::libc::c_void)>;
extern "C" {
    /**
 * \brief Add a callback to a compute stream
 *
 * Adds a callback to be called on the host after all currently enqueued
 * items in the stream have completed.  For each
 * cudaStreamAddCallback call, a callback will be executed exactly once.
 * The callback will block later work in the stream until it is finished.
 *
 * The callback may be passed ::cudaSuccess or an error code.  In the event
 * of a device error, all subsequently executed callbacks will receive an
 * appropriate ::cudaError_t.
 *
 * Callbacks must not make any CUDA API calls.  Attempting to use CUDA APIs
 * will result in ::cudaErrorNotPermitted.  Callbacks must not perform any
 * synchronization that may depend on outstanding device work or other callbacks
 * that are not mandated to run earlier.  Callbacks without a mandated order
 * (in independent streams) execute in undefined order and may be serialized.
 *
 * For the purposes of Unified Memory, callback execution makes a number of
 * guarantees:
 * <ul>
 *   <li>The callback stream is considered idle for the duration of the
 *   callback.  Thus, for example, a callback may always use memory attached
 *   to the callback stream.</li>
 *   <li>The start of execution of a callback has the same effect as
 *   synchronizing an event recorded in the same stream immediately prior to
 *   the callback.  It thus synchronizes streams which have been "joined"
 *   prior to the callback.</li>
 *   <li>Adding device work to any stream does not have the effect of making
 *   the stream active until all preceding callbacks have executed.  Thus, for
 *   example, a callback might use global attached memory even if work has
 *   been added to another stream, if it has been properly ordered with an
 *   event.</li>
 *   <li>Completion of a callback does not cause a stream to become
 *   active except as described above.  The callback stream will remain idle
 *   if no device work follows the callback, and will remain idle across
 *   consecutive callbacks without device work in between.  Thus, for example,
 *   stream synchronization can be done by signaling from a callback at the
 *   end of the stream.</li>
 * </ul>
 *
 * \param stream   - Stream to add callback to
 * \param callback - The function to call once preceding stream operations are complete
 * \param userData - User specified data to be passed to the callback function
 * \param flags    - Reserved for future use, must be 0
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidResourceHandle,
 * ::cudaErrorNotSupported
 * \note_null_stream
 * \notefnerr
 *
 * \sa ::cudaStreamCreate, ::cudaStreamCreateWithFlags, ::cudaStreamQuery, ::cudaStreamSynchronize, ::cudaStreamWaitEvent, ::cudaStreamDestroy, ::cudaMallocManaged, ::cudaStreamAttachMemAsync
 */
    pub fn cudaStreamAddCallback(stream: cudaStream_t,
                                 callback: cudaStreamCallback_t,
                                 userData: *mut ::libc::c_void,
                                 flags: ::libc::c_uint)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Waits for stream tasks to complete
 *
 * Blocks until \p stream has completed all operations. If the
 * ::cudaDeviceScheduleBlockingSync flag was set for this device,
 * the host thread will block until the stream is finished with
 * all of its tasks.
 *
 * \param stream - Stream identifier
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidResourceHandle
 * \notefnerr
 *
 * \sa ::cudaStreamCreate, ::cudaStreamCreateWithFlags, ::cudaStreamQuery, ::cudaStreamWaitEvent, ::cudaStreamAddCallback, ::cudaStreamDestroy
 */
    pub fn cudaStreamSynchronize(stream: cudaStream_t) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Queries an asynchronous stream for completion status
 *
 * Returns ::cudaSuccess if all operations in \p stream have
 * completed, or ::cudaErrorNotReady if not.
 *
 * For the purposes of Unified Memory, a return value of ::cudaSuccess
 * is equivalent to having called ::cudaStreamSynchronize().
 *
 * \param stream - Stream identifier
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorNotReady,
 * ::cudaErrorInvalidResourceHandle
 * \notefnerr
 *
 * \sa ::cudaStreamCreate, ::cudaStreamCreateWithFlags, ::cudaStreamWaitEvent, ::cudaStreamSynchronize, ::cudaStreamAddCallback, ::cudaStreamDestroy
 */
    pub fn cudaStreamQuery(stream: cudaStream_t) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Attach memory to a stream asynchronously
 *
 * Enqueues an operation in \p stream to specify stream association of
 * \p length bytes of memory starting from \p devPtr. This function is a
 * stream-ordered operation, meaning that it is dependent on, and will
 * only take effect when, previous work in stream has completed. Any
 * previous association is automatically replaced.
 *
 * \p devPtr must point to an address within managed memory space declared
 * using the __managed__ keyword or allocated with ::cudaMallocManaged.
 *
 * \p length must be zero, to indicate that the entire allocation's
 * stream association is being changed.  Currently, it's not possible
 * to change stream association for a portion of an allocation. The default
 * value for \p length is zero.
 *
 * The stream association is specified using \p flags which must be
 * one of ::cudaMemAttachGlobal, ::cudaMemAttachHost or ::cudaMemAttachSingle.
 * The default value for \p flags is ::cudaMemAttachSingle
 * If the ::cudaMemAttachGlobal flag is specified, the memory can be accessed
 * by any stream on any device.
 * If the ::cudaMemAttachHost flag is specified, the program makes a guarantee
 * that it won't access the memory on the device from any stream on a device that
 * has a zero value for the device attribute ::cudaDevAttrConcurrentManagedAccess.
 * If the ::cudaMemAttachSingle flag is specified and \p stream is associated with
 * a device that has a zero value for the device attribute ::cudaDevAttrConcurrentManagedAccess,
 * the program makes a guarantee that it will only access the memory on the device
 * from \p stream. It is illegal to attach singly to the NULL stream, because the
 * NULL stream is a virtual global stream and not a specific stream. An error will
 * be returned in this case.
 *
 * When memory is associated with a single stream, the Unified Memory system will
 * allow CPU access to this memory region so long as all operations in \p stream
 * have completed, regardless of whether other streams are active. In effect,
 * this constrains exclusive ownership of the managed memory region by
 * an active GPU to per-stream activity instead of whole-GPU activity.
 *
 * Accessing memory on the device from streams that are not associated with
 * it will produce undefined results. No error checking is performed by the
 * Unified Memory system to ensure that kernels launched into other streams
 * do not access this region.
 *
 * It is a program's responsibility to order calls to ::cudaStreamAttachMemAsync
 * via events, synchronization or other means to ensure legal access to memory
 * at all times. Data visibility and coherency will be changed appropriately
 * for all kernels which follow a stream-association change.
 *
 * If \p stream is destroyed while data is associated with it, the association is
 * removed and the association reverts to the default visibility of the allocation
 * as specified at ::cudaMallocManaged. For __managed__ variables, the default
 * association is always ::cudaMemAttachGlobal. Note that destroying a stream is an
 * asynchronous operation, and as a result, the change to default association won't
 * happen until all work in the stream has completed.
 *
 * \param stream  - Stream in which to enqueue the attach operation
 * \param devPtr  - Pointer to memory (must be a pointer to managed memory)
 * \param length  - Length of memory (must be zero, defaults to zero)
 * \param flags   - Must be one of ::cudaMemAttachGlobal, ::cudaMemAttachHost or ::cudaMemAttachSingle (defaults to ::cudaMemAttachSingle)
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorNotReady,
 * ::cudaErrorInvalidValue
 * ::cudaErrorInvalidResourceHandle
 * \notefnerr
 *
 * \sa ::cudaStreamCreate, ::cudaStreamCreateWithFlags, ::cudaStreamWaitEvent, ::cudaStreamSynchronize, ::cudaStreamAddCallback, ::cudaStreamDestroy, ::cudaMallocManaged
 */
    pub fn cudaStreamAttachMemAsync(stream: cudaStream_t,
                                    devPtr: *mut ::libc::c_void,
                                    length: usize,
                                    flags: ::libc::c_uint)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Creates an event object
 *
 * Creates an event object using ::cudaEventDefault.
 *
 * \param event - Newly created event
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInitializationError,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorLaunchFailure,
 * ::cudaErrorMemoryAllocation
 * \notefnerr
 *
 * \sa \ref ::cudaEventCreate(cudaEvent_t*, unsigned int) "cudaEventCreate (C++ API)",
 * ::cudaEventCreateWithFlags, ::cudaEventRecord, ::cudaEventQuery,
 * ::cudaEventSynchronize, ::cudaEventDestroy, ::cudaEventElapsedTime,
 * ::cudaStreamWaitEvent
 */
    pub fn cudaEventCreate(event: *mut cudaEvent_t) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Creates an event object with the specified flags
 *
 * Creates an event object with the specified flags. Valid flags include:
 * - ::cudaEventDefault: Default event creation flag.
 * - ::cudaEventBlockingSync: Specifies that event should use blocking
 *   synchronization. A host thread that uses ::cudaEventSynchronize() to wait
 *   on an event created with this flag will block until the event actually
 *   completes.
 * - ::cudaEventDisableTiming: Specifies that the created event does not need
 *   to record timing data.  Events created with this flag specified and
 *   the ::cudaEventBlockingSync flag not specified will provide the best
 *   performance when used with ::cudaStreamWaitEvent() and ::cudaEventQuery().
 * - ::cudaEventInterprocess: Specifies that the created event may be used as an
 *   interprocess event by ::cudaIpcGetEventHandle(). ::cudaEventInterprocess must
 *   be specified along with ::cudaEventDisableTiming.
 *
 * \param event - Newly created event
 * \param flags - Flags for new event
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInitializationError,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorLaunchFailure,
 * ::cudaErrorMemoryAllocation
 * \notefnerr
 *
 * \sa \ref ::cudaEventCreate(cudaEvent_t*) "cudaEventCreate (C API)",
 * ::cudaEventSynchronize, ::cudaEventDestroy, ::cudaEventElapsedTime,
 * ::cudaStreamWaitEvent
 */
    pub fn cudaEventCreateWithFlags(event: *mut cudaEvent_t,
                                    flags: ::libc::c_uint)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Records an event
 *
 * Records an event. See note about NULL stream behavior. Since operation
 * is asynchronous, ::cudaEventQuery() or ::cudaEventSynchronize() must
 * be used to determine when the event has actually been recorded.
 *
 * If ::cudaEventRecord() has previously been called on \p event, then this
 * call will overwrite any existing state in \p event.  Any subsequent calls
 * which examine the status of \p event will only examine the completion of
 * this most recent call to ::cudaEventRecord().
 *
 * \param event  - Event to record
 * \param stream - Stream in which to record event
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInitializationError,
 * ::cudaErrorInvalidResourceHandle,
 * ::cudaErrorLaunchFailure
 * \note_null_stream
 * \notefnerr
 *
 * \sa \ref ::cudaEventCreate(cudaEvent_t*) "cudaEventCreate (C API)",
 * ::cudaEventCreateWithFlags, ::cudaEventQuery,
 * ::cudaEventSynchronize, ::cudaEventDestroy, ::cudaEventElapsedTime,
 * ::cudaStreamWaitEvent
 */
    pub fn cudaEventRecord(event: cudaEvent_t, stream: cudaStream_t)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Queries an event's status
 *
 * Query the status of all device work preceding the most recent call to
 * ::cudaEventRecord() (in the appropriate compute streams, as specified by the
 * arguments to ::cudaEventRecord()).
 *
 * If this work has successfully been completed by the device, or if
 * ::cudaEventRecord() has not been called on \p event, then ::cudaSuccess is
 * returned. If this work has not yet been completed by the device then
 * ::cudaErrorNotReady is returned.
 *
 * For the purposes of Unified Memory, a return value of ::cudaSuccess
 * is equivalent to having called ::cudaEventSynchronize().
 *
 * \param event - Event to query
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorNotReady,
 * ::cudaErrorInitializationError,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidResourceHandle,
 * ::cudaErrorLaunchFailure
 * \notefnerr
 *
 * \sa \ref ::cudaEventCreate(cudaEvent_t*) "cudaEventCreate (C API)",
 * ::cudaEventCreateWithFlags, ::cudaEventRecord,
 * ::cudaEventSynchronize, ::cudaEventDestroy, ::cudaEventElapsedTime
 */
    pub fn cudaEventQuery(event: cudaEvent_t) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Waits for an event to complete
 *
 * Wait until the completion of all device work preceding the most recent
 * call to ::cudaEventRecord() (in the appropriate compute streams, as specified
 * by the arguments to ::cudaEventRecord()).
 *
 * If ::cudaEventRecord() has not been called on \p event, ::cudaSuccess is
 * returned immediately.
 *
 * Waiting for an event that was created with the ::cudaEventBlockingSync
 * flag will cause the calling CPU thread to block until the event has
 * been completed by the device.  If the ::cudaEventBlockingSync flag has
 * not been set, then the CPU thread will busy-wait until the event has
 * been completed by the device.
 *
 * \param event - Event to wait for
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInitializationError,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidResourceHandle,
 * ::cudaErrorLaunchFailure
 * \notefnerr
 *
 * \sa \ref ::cudaEventCreate(cudaEvent_t*) "cudaEventCreate (C API)",
 * ::cudaEventCreateWithFlags, ::cudaEventRecord,
 * ::cudaEventQuery, ::cudaEventDestroy, ::cudaEventElapsedTime
 */
    pub fn cudaEventSynchronize(event: cudaEvent_t) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Destroys an event object
 *
 * Destroys the event specified by \p event.
 *
 * In case \p event has been recorded but has not yet been completed
 * when ::cudaEventDestroy() is called, the function will return immediately and
 * the resources associated with \p event will be released automatically once
 * the device has completed \p event.
 *
 * \param event - Event to destroy
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInitializationError,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorLaunchFailure
 * \notefnerr
 *
 * \sa \ref ::cudaEventCreate(cudaEvent_t*) "cudaEventCreate (C API)",
 * ::cudaEventCreateWithFlags, ::cudaEventQuery,
 * ::cudaEventSynchronize, ::cudaEventRecord, ::cudaEventElapsedTime
 */
    pub fn cudaEventDestroy(event: cudaEvent_t) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Computes the elapsed time between events
 *
 * Computes the elapsed time between two events (in milliseconds with a
 * resolution of around 0.5 microseconds).
 *
 * If either event was last recorded in a non-NULL stream, the resulting time
 * may be greater than expected (even if both used the same stream handle). This
 * happens because the ::cudaEventRecord() operation takes place asynchronously
 * and there is no guarantee that the measured latency is actually just between
 * the two events. Any number of other different stream operations could execute
 * in between the two measured events, thus altering the timing in a significant
 * way.
 *
 * If ::cudaEventRecord() has not been called on either event, then
 * ::cudaErrorInvalidResourceHandle is returned. If ::cudaEventRecord() has been
 * called on both events but one or both of them has not yet been completed
 * (that is, ::cudaEventQuery() would return ::cudaErrorNotReady on at least one
 * of the events), ::cudaErrorNotReady is returned. If either event was created
 * with the ::cudaEventDisableTiming flag, then this function will return
 * ::cudaErrorInvalidResourceHandle.
 *
 * \param ms    - Time between \p start and \p end in ms
 * \param start - Starting event
 * \param end   - Ending event
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorNotReady,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInitializationError,
 * ::cudaErrorInvalidResourceHandle,
 * ::cudaErrorLaunchFailure
 * \notefnerr
 *
 * \sa \ref ::cudaEventCreate(cudaEvent_t*) "cudaEventCreate (C API)",
 * ::cudaEventCreateWithFlags, ::cudaEventQuery,
 * ::cudaEventSynchronize, ::cudaEventDestroy, ::cudaEventRecord
 */
    pub fn cudaEventElapsedTime(ms: *mut f32, start: cudaEvent_t,
                                end: cudaEvent_t) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Launches a device function
 *
 * The function invokes kernel \p func on \p gridDim (\p gridDim.x × \p gridDim.y
 * × \p gridDim.z) grid of blocks. Each block contains \p blockDim (\p blockDim.x ×
 * \p blockDim.y × \p blockDim.z) threads.
 *
 * If the kernel has N parameters the \p args should point to array of N pointers.
 * Each pointer, from <tt>args[0]</tt> to <tt>args[N - 1]</tt>, point to the region
 * of memory from which the actual parameter will be copied.
 *
 * For templated functions, pass the function symbol as follows:
 * func_name<template_arg_0,...,template_arg_N>
 *
 * \p sharedMem sets the amount of dynamic shared memory that will be available to
 * each thread block.
 *
 * \p stream specifies a stream the invocation is associated to.
 *
 * \param func        - Device function symbol
 * \param gridDim     - Grid dimentions
 * \param blockDim    - Block dimentions
 * \param args        - Arguments
 * \param sharedMem   - Shared memory
 * \param stream      - Stream identifier
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidDeviceFunction,
 * ::cudaErrorInvalidConfiguration,
 * ::cudaErrorLaunchFailure,
 * ::cudaErrorLaunchTimeout,
 * ::cudaErrorLaunchOutOfResources,
 * ::cudaErrorSharedObjectInitFailed
 * \note_null_stream
 * \notefnerr
 *
 * \ref ::cudaLaunchKernel(const T *func, dim3 gridDim, dim3 blockDim, void **args, size_t sharedMem, cudaStream_t stream) "cudaLaunchKernel (C++ API)"
 */
    pub fn cudaLaunchKernel(func: *const ::libc::c_void,
                            gridDim: dim3, blockDim: dim3,
                            args: *mut *mut ::libc::c_void,
                            sharedMem: usize, stream: cudaStream_t)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Sets the preferred cache configuration for a device function
 *
 * On devices where the L1 cache and shared memory use the same hardware
 * resources, this sets through \p cacheConfig the preferred cache configuration
 * for the function specified via \p func. This is only a preference. The
 * runtime will use the requested configuration if possible, but it is free to
 * choose a different configuration if required to execute \p func.
 *
 * \p func is a device function symbol and must be declared as a
 * \c __global__ function. If the specified function does not exist,
 * then ::cudaErrorInvalidDeviceFunction is returned. For templated functions,
 * pass the function symbol as follows: func_name<template_arg_0,...,template_arg_N>
 *
 * This setting does nothing on devices where the size of the L1 cache and
 * shared memory are fixed.
 *
 * Launching a kernel with a different preference than the most recent
 * preference setting may insert a device-side synchronization point.
 *
 * The supported cache configurations are:
 * - ::cudaFuncCachePreferNone: no preference for shared memory or L1 (default)
 * - ::cudaFuncCachePreferShared: prefer larger shared memory and smaller L1 cache
 * - ::cudaFuncCachePreferL1: prefer larger L1 cache and smaller shared memory
 * - ::cudaFuncCachePreferEqual: prefer equal size L1 cache and shared memory
 *
 * \param func        - Device function symbol
 * \param cacheConfig - Requested cache configuration
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInitializationError,
 * ::cudaErrorInvalidDeviceFunction
 * \notefnerr
 * \note_string_api_deprecation2
 *
 * \sa ::cudaConfigureCall,
 * \ref ::cudaFuncSetCacheConfig(T*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C++ API)",
 * \ref ::cudaFuncGetAttributes(struct cudaFuncAttributes*, const void*) "cudaFuncGetAttributes (C API)",
 * \ref ::cudaLaunchKernel(const void *func, dim3 gridDim, dim3 blockDim, void **args, size_t sharedMem, cudaStream_t stream) "cudaLaunchKernel (C API)",
 * ::cudaSetDoubleForDevice,
 * ::cudaSetDoubleForHost,
 * \ref ::cudaSetupArgument(const void*, size_t, size_t) "cudaSetupArgument (C API)",
 * ::cudaThreadGetCacheConfig,
 * ::cudaThreadSetCacheConfig
 */
    pub fn cudaFuncSetCacheConfig(func: *const ::libc::c_void,
                                  cacheConfig: cudaFuncCache) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Sets the shared memory configuration for a device function
 *
 * On devices with configurable shared memory banks, this function will
 * force all subsequent launches of the specified device function to have
 * the given shared memory bank size configuration. On any given launch of the
 * function, the shared memory configuration of the device will be temporarily
 * changed if needed to suit the function's preferred configuration. Changes in
 * shared memory configuration between subsequent launches of functions,
 * may introduce a device side synchronization point.
 *
 * Any per-function setting of shared memory bank size set via
 * ::cudaFuncSetSharedMemConfig will override the device wide setting set by
 * ::cudaDeviceSetSharedMemConfig.
 *
 * Changing the shared memory bank size will not increase shared memory usage
 * or affect occupancy of kernels, but may have major effects on performance.
 * Larger bank sizes will allow for greater potential bandwidth to shared memory,
 * but will change what kinds of accesses to shared memory will result in bank
 * conflicts.
 *
 * This function will do nothing on devices with fixed shared memory bank size.
 *
 * For templated functions, pass the function symbol as follows:
 * func_name<template_arg_0,...,template_arg_N>
 *
 * The supported bank configurations are:
 * - ::cudaSharedMemBankSizeDefault: use the device's shared memory configuration
 *   when launching this function.
 * - ::cudaSharedMemBankSizeFourByte: set shared memory bank width to be
 *   four bytes natively when launching this function.
 * - ::cudaSharedMemBankSizeEightByte: set shared memory bank width to be eight
 *   bytes natively when launching this function.
 *
 * \param func   - Device function symbol
 * \param config - Requested shared memory configuration
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInitializationError,
 * ::cudaErrorInvalidDeviceFunction,
 * ::cudaErrorInvalidValue,
 * \notefnerr
 * \note_string_api_deprecation2
 *
 * \sa ::cudaConfigureCall,
 * ::cudaDeviceSetSharedMemConfig,
 * ::cudaDeviceGetSharedMemConfig,
 * ::cudaDeviceSetCacheConfig,
 * ::cudaDeviceGetCacheConfig,
 * ::cudaFuncSetCacheConfig
 */
    pub fn cudaFuncSetSharedMemConfig(func: *const ::libc::c_void,
                                      config: cudaSharedMemConfig)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Find out attributes for a given function
 *
 * This function obtains the attributes of a function specified via \p func.
 * \p func is a device function symbol and must be declared as a
 * \c __global__ function. The fetched attributes are placed in \p attr.
 * If the specified function does not exist, then
 * ::cudaErrorInvalidDeviceFunction is returned. For templated functions, pass
 * the function symbol as follows: func_name<template_arg_0,...,template_arg_N>
 *
 * Note that some function attributes such as
 * \ref ::cudaFuncAttributes::maxThreadsPerBlock "maxThreadsPerBlock"
 * may vary based on the device that is currently being used.
 *
 * \param attr - Return pointer to function's attributes
 * \param func - Device function symbol
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInitializationError,
 * ::cudaErrorInvalidDeviceFunction
 * \notefnerr
 * \note_string_api_deprecation2
 *
 * \sa ::cudaConfigureCall,
 * \ref ::cudaFuncSetCacheConfig(const void*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C API)",
 * \ref ::cudaFuncGetAttributes(struct cudaFuncAttributes*, T*) "cudaFuncGetAttributes (C++ API)",
 * \ref ::cudaLaunchKernel(const void *func, dim3 gridDim, dim3 blockDim, void **args, size_t sharedMem, cudaStream_t stream) "cudaLaunchKernel (C API)",
 * ::cudaSetDoubleForDevice,
 * ::cudaSetDoubleForHost,
 * \ref ::cudaSetupArgument(const void*, size_t, size_t) "cudaSetupArgument (C API)"
 */
    pub fn cudaFuncGetAttributes(attr: *mut cudaFuncAttributes,
                                 func: *const ::libc::c_void)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Converts a double argument to be executed on a device
 *
 * \param d - Double to convert
 *
 * \deprecated This function is deprecated as of CUDA 7.5
 *
 * Converts the double value of \p d to an internal float representation if
 * the device does not support double arithmetic. If the device does natively
 * support doubles, then this function does nothing.
 *
 * \return
 * ::cudaSuccess
 * \notefnerr
 *
 * \ref ::cudaLaunch(const void*) "cudaLaunch (C API)",
 * \ref ::cudaFuncSetCacheConfig(const void*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C API)",
 * \ref ::cudaFuncGetAttributes(struct cudaFuncAttributes*, const void*) "cudaFuncGetAttributes (C API)",
 * ::cudaSetDoubleForHost,
 * \ref ::cudaSetupArgument(const void*, size_t, size_t) "cudaSetupArgument (C API)"
 */
    pub fn cudaSetDoubleForDevice(d: *mut f64) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Converts a double argument after execution on a device
 *
 * \deprecated This function is deprecated as of CUDA 7.5
 *
 * Converts the double value of \p d from a potentially internal float
 * representation if the device does not support double arithmetic. If the
 * device does natively support doubles, then this function does nothing.
 *
 * \param d - Double to convert
 *
 * \return
 * ::cudaSuccess
 * \notefnerr
 *
 * \ref ::cudaLaunch(const void*) "cudaLaunch (C API)",
 * \ref ::cudaFuncSetCacheConfig(const void*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C API)",
 * \ref ::cudaFuncGetAttributes(struct cudaFuncAttributes*, const void*) "cudaFuncGetAttributes (C API)",
 * ::cudaSetDoubleForDevice,
 * \ref ::cudaSetupArgument(const void*, size_t, size_t) "cudaSetupArgument (C API)"
 */
    pub fn cudaSetDoubleForHost(d: *mut f64) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Returns occupancy for a device function
 *
 * Returns in \p *numBlocks the maximum number of active blocks per
 * streaming multiprocessor for the device function.
 *
 * \param numBlocks       - Returned occupancy
 * \param func            - Kernel function for which occupancy is calculated
 * \param blockSize       - Block size the kernel is intended to be launched with
 * \param dynamicSMemSize - Per-block dynamic shared memory usage intended, in bytes
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorCudartUnloading,
 * ::cudaErrorInitializationError,
 * ::cudaErrorInvalidDevice,
 * ::cudaErrorInvalidDeviceFunction,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorUnknown,
 * \notefnerr
 *
 * \sa ::cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags,
 * \ref ::cudaOccupancyMaxPotentialBlockSize(int*, int*, T, size_t, int) "cudaOccupancyMaxPotentialBlockSize (C++ API)",
 * \ref ::cudaOccupancyMaxPotentialBlockSizeWithFlags(int*, int*, T, size_t, int, unsigned int) "cudaOccupancyMaxPotentialBlockSizeWithFlags (C++ API)",
 * \ref ::cudaOccupancyMaxPotentialBlockSizeVariableSMem(int*, int*, T, UnaryFunction, int) "cudaOccupancyMaxPotentialBlockSizeVariableSMem (C++ API)"
 * \ref ::cudaOccupancyMaxPotentialBlockSizeVariableSMemWithFlags(int*, int*, T, UnaryFunction, int, unsigned int) "cudaOccupancyMaxPotentialBlockSizeVariableSMemWithFlags (C++ API)"
 */
    pub fn cudaOccupancyMaxActiveBlocksPerMultiprocessor(numBlocks:
                                                             *mut ::libc::c_int,
                                                         func:
                                                             *const ::libc::c_void,
                                                         blockSize:
                                                             ::libc::c_int,
                                                         dynamicSMemSize:
                                                             usize)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Returns occupancy for a device function with the specified flags
 *
 * Returns in \p *numBlocks the maximum number of active blocks per
 * streaming multiprocessor for the device function.
 *
 * The \p flags parameter controls how special cases are handled. Valid flags include:
 *
 * - ::cudaOccupancyDefault: keeps the default behavior as
 *   ::cudaOccupancyMaxActiveBlocksPerMultiprocessor
 *
 * - ::cudaOccupancyDisableCachingOverride: This flag suppresses the default behavior
 *   on platform where global caching affects occupancy. On such platforms, if caching
 *   is enabled, but per-block SM resource usage would result in zero occupancy, the
 *   occupancy calculator will calculate the occupancy as if caching is disabled.
 *   Setting this flag makes the occupancy calculator to return 0 in such cases.
 *   More information can be found about this feature in the "Unified L1/Texture Cache"
 *   section of the Maxwell tuning guide.
 *
 * \param numBlocks       - Returned occupancy
 * \param func            - Kernel function for which occupancy is calculated
 * \param blockSize       - Block size the kernel is intended to be launched with
 * \param dynamicSMemSize - Per-block dynamic shared memory usage intended, in bytes
 * \param flags           - Requested behavior for the occupancy calculator
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorCudartUnloading,
 * ::cudaErrorInitializationError,
 * ::cudaErrorInvalidDevice,
 * ::cudaErrorInvalidDeviceFunction,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorUnknown,
 * \notefnerr
 *
 * \sa ::cudaOccupancyMaxActiveBlocksPerMultiprocessor,
 * \ref ::cudaOccupancyMaxPotentialBlockSize(int*, int*, T, size_t, int) "cudaOccupancyMaxPotentialBlockSize (C++ API)",
 * \ref ::cudaOccupancyMaxPotentialBlockSizeWithFlags(int*, int*, T, size_t, int, unsigned int) "cudaOccupancyMaxPotentialBlockSizeWithFlags (C++ API)",
 * \ref ::cudaOccupancyMaxPotentialBlockSizeVariableSMem(int*, int*, T, UnaryFunction, int) "cudaOccupancyMaxPotentialBlockSizeVariableSMem (C++ API)"
 * \ref ::cudaOccupancyMaxPotentialBlockSizeVariableSMemWithFlags(int*, int*, T, UnaryFunction, int, unsigned int) "cudaOccupancyMaxPotentialBlockSizeVariableSMemWithFlags (C++ API)"
 */
    pub fn cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags(numBlocks:
                                                                      *mut ::libc::c_int,
                                                                  func:
                                                                      *const ::libc::c_void,
                                                                  blockSize:
                                                                      ::libc::c_int,
                                                                  dynamicSMemSize:
                                                                      usize,
                                                                  flags:
                                                                      ::libc::c_uint)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Configure a device-launch
 *
 * \deprecated This function is deprecated as of CUDA 7.0
 *
 * Specifies the grid and block dimensions for the device call to be executed
 * similar to the execution configuration syntax. ::cudaConfigureCall() is
 * stack based. Each call pushes data on top of an execution stack. This data
 * contains the dimension for the grid and thread blocks, together with any
 * arguments for the call.
 *
 * \param gridDim   - Grid dimensions
 * \param blockDim  - Block dimensions
 * \param sharedMem - Shared memory
 * \param stream    - Stream identifier
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidConfiguration
 * \note_null_stream
 * \notefnerr
 *
 * \ref ::cudaLaunchKernel(const void *func, dim3 gridDim, dim3 blockDim, void **args, size_t sharedMem, cudaStream_t stream) "cudaLaunchKernel (C API)",
 * \ref ::cudaFuncSetCacheConfig(const void*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C API)",
 * \ref ::cudaFuncGetAttributes(struct cudaFuncAttributes*, const void*) "cudaFuncGetAttributes (C API)",
 * \ref ::cudaLaunch(const void*) "cudaLaunch (C API)",
 * ::cudaSetDoubleForDevice,
 * ::cudaSetDoubleForHost,
 * \ref ::cudaSetupArgument(const void*, size_t, size_t) "cudaSetupArgument (C API)",
 */
    pub fn cudaConfigureCall(gridDim: dim3, blockDim: dim3, sharedMem: usize,
                             stream: cudaStream_t) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Configure a device launch
 *
 * \deprecated This function is deprecated as of CUDA 7.0
 *
 * Pushes \p size bytes of the argument pointed to by \p arg at \p offset
 * bytes from the start of the parameter passing area, which starts at
 * offset 0. The arguments are stored in the top of the execution stack.
 * \ref ::cudaSetupArgument(const void*, size_t, size_t) "cudaSetupArgument()"
 * must be preceded by a call to ::cudaConfigureCall().
 *
 * \param arg    - Argument to push for a kernel launch
 * \param size   - Size of argument
 * \param offset - Offset in argument stack to push new arg
 *
 * \return
 * ::cudaSuccess
 * \notefnerr
 *
 * \ref ::cudaLaunchKernel(const void *func, dim3 gridDim, dim3 blockDim, void **args, size_t sharedMem, cudaStream_t stream) "cudaLaunchKernel (C API)",
 * \ref ::cudaFuncSetCacheConfig(const void*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C API)",
 * \ref ::cudaFuncGetAttributes(struct cudaFuncAttributes*, const void*) "cudaFuncGetAttributes (C API)",
 * \ref ::cudaLaunch(const void*) "cudaLaunch (C API)",
 * ::cudaSetDoubleForDevice,
 * ::cudaSetDoubleForHost,
 * \ref ::cudaSetupArgument(T, size_t) "cudaSetupArgument (C++ API)",
 */
    pub fn cudaSetupArgument(arg: *const ::libc::c_void, size: usize,
                             offset: usize) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Launches a device function
 *
 * \deprecated This function is deprecated as of CUDA 7.0
 *
 * Launches the function \p func on the device. The parameter \p func must
 * be a device function symbol. The parameter specified by \p func must be
 * declared as a \p __global__ function. For templated functions, pass the
 * function symbol as follows: func_name<template_arg_0,...,template_arg_N>
 * \ref ::cudaLaunch(const void*) "cudaLaunch()" must be preceded by a call to
 * ::cudaConfigureCall() since it pops the data that was pushed by
 * ::cudaConfigureCall() from the execution stack.
 *
 * \param func - Device function symbol
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidDeviceFunction,
 * ::cudaErrorInvalidConfiguration,
 * ::cudaErrorLaunchFailure,
 * ::cudaErrorLaunchTimeout,
 * ::cudaErrorLaunchOutOfResources,
 * ::cudaErrorSharedObjectInitFailed
 * \notefnerr
 * \note_string_api_deprecation_50
 *
 * \ref ::cudaLaunchKernel(const void *func, dim3 gridDim, dim3 blockDim, void **args, size_t sharedMem, cudaStream_t stream) "cudaLaunchKernel (C API)",
 * \ref ::cudaFuncSetCacheConfig(const void*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C API)",
 * \ref ::cudaFuncGetAttributes(struct cudaFuncAttributes*, const void*) "cudaFuncGetAttributes (C API)",
 * \ref ::cudaLaunch(T*) "cudaLaunch (C++ API)",
 * ::cudaSetDoubleForDevice,
 * ::cudaSetDoubleForHost,
 * \ref ::cudaSetupArgument(const void*, size_t, size_t) "cudaSetupArgument (C API)",
 * ::cudaThreadGetCacheConfig,
 * ::cudaThreadSetCacheConfig
 */
    pub fn cudaLaunch(func: *const ::libc::c_void) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Allocates memory that will be automatically managed by the Unified Memory system
 *
 * Allocates \p size bytes of managed memory on the device and returns in
 * \p *devPtr a pointer to the allocated memory. If the device doesn't support
 * allocating managed memory, ::cudaErrorNotSupported is returned. Support
 * for managed memory can be queried using the device attribute
 * ::cudaDevAttrManagedMemory. The allocated memory is suitably
 * aligned for any kind of variable. The memory is not cleared. If \p size
 * is 0, ::cudaMallocManaged returns ::cudaErrorInvalidValue. The pointer
 * is valid on the CPU and on all GPUs in the system that support managed memory.
 * All accesses to this pointer must obey the Unified Memory programming model.
 *
 * \p flags specifies the default stream association for this allocation.
 * \p flags must be one of ::cudaMemAttachGlobal or ::cudaMemAttachHost. The
 * default value for \p flags is ::cudaMemAttachGlobal.
 * If ::cudaMemAttachGlobal is specified, then this memory is accessible from
 * any stream on any device. If ::cudaMemAttachHost is specified, then the
 * allocation should not be accessed from devices that have a zero value for the
 * device attribute ::cudaDevAttrConcurrentManagedAccess; an explicit call to
 * ::cudaStreamAttachMemAsync will be required to enable access on such devices.
 *
 * If the association is later changed via ::cudaStreamAttachMemAsync to
 * a single stream, the default association, as specifed during ::cudaMallocManaged,
 * is restored when that stream is destroyed. For __managed__ variables, the
 * default association is always ::cudaMemAttachGlobal. Note that destroying a
 * stream is an asynchronous operation, and as a result, the change to default
 * association won't happen until all work in the stream has completed.
 *
 * Memory allocated with ::cudaMallocManaged should be released with ::cudaFree.
 *
 * Device memory oversubscription is possible for GPUs that have a non-zero value for the
 * device attribute ::cudaDevAttrConcurrentManagedAccess. Managed memory on
 * such GPUs may be evicted from device memory to host memory at any time by the Unified
 * Memory driver in order to make room for other allocations.
 *
 * In a multi-GPU system where all GPUs have a non-zero value for the device attribute
 * ::cudaDevAttrConcurrentManagedAccess, managed memory may not be populated when this
 * API returns and instead may be populated on access. In such systems, managed memory can
 * migrate to any processor's memory at any time. The Unified Memory driver will employ heuristics to
 * maintain data locality and prevent excessive page faults to the extent possible. The application
 * can also guide the driver about memory usage patterns via ::cudaMemAdvise. The application
 * can also explicitly migrate memory to a desired processor's memory via
 * ::cudaMemPrefetchAsync.
 *
 * In a multi-GPU system where all of the GPUs have a zero value for the device attribute
 * ::cudaDevAttrConcurrentManagedAccess and all the GPUs have peer-to-peer support
 * with each other, the physical storage for managed memory is created on the GPU which is active
 * at the time ::cudaMallocManaged is called. All other GPUs will reference the data at reduced
 * bandwidth via peer mappings over the PCIe bus. The Unified Memory driver does not migrate
 * memory among such GPUs.
 *
 * In a multi-GPU system where not all GPUs have peer-to-peer support with each other and
 * where the value of the device attribute ::cudaDevAttrConcurrentManagedAccess
 * is zero for at least one of those GPUs, the location chosen for physical storage of managed
 * memory is system-dependent.
 * - On Linux, the location chosen will be device memory as long as the current set of active
 * contexts are on devices that either have peer-to-peer support with each other or have a
 * non-zero value for the device attribute ::cudaDevAttrConcurrentManagedAccess.
 * If there is an active context on a GPU that does not have a non-zero value for that device
 * attribute and it does not have peer-to-peer support with the other devices that have active
 * contexts on them, then the location for physical storage will be 'zero-copy' or host memory.
 * Note that this means that managed memory that is located in device memory is migrated to
 * host memory if a new context is created on a GPU that doesn't have a non-zero value for
 * the device attribute and does not support peer-to-peer with at least one of the other devices
 * that has an active context. This in turn implies that context creation may fail if there is
 * insufficient host memory to migrate all managed allocations.
 * - On Windows, the physical storage is always created in 'zero-copy' or host memory.
 * All GPUs will reference the data at reduced bandwidth over the PCIe bus. In these
 * circumstances, use of the environment variable CUDA_VISIBLE_DEVICES is recommended to
 * restrict CUDA to only use those GPUs that have peer-to-peer support.
 * Alternatively, users can also set CUDA_MANAGED_FORCE_DEVICE_ALLOC to a non-zero
 * value to force the driver to always use device memory for physical storage.
 * When this environment variable is set to a non-zero value, all devices used in
 * that process that support managed memory have to be peer-to-peer compatible
 * with each other. The error ::cudaErrorInvalidDevice will be returned if a device
 * that supports managed memory is used and it is not peer-to-peer compatible with
 * any of the other managed memory supporting devices that were previously used in
 * that process, even if ::cudaDeviceReset has been called on those devices. These
 * environment variables are described in the CUDA programming guide under the
 * "CUDA environment variables" section.
 *
 * \param devPtr - Pointer to allocated device memory
 * \param size   - Requested allocation size in bytes
 * \param flags  - Must be either ::cudaMemAttachGlobal or ::cudaMemAttachHost (defaults to ::cudaMemAttachGlobal)
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorMemoryAllocation
 * ::cudaErrorNotSupported
 * ::cudaErrorInvalidValue
 *
 * \sa ::cudaMallocPitch, ::cudaFree, ::cudaMallocArray, ::cudaFreeArray,
 * ::cudaMalloc3D, ::cudaMalloc3DArray,
 * \ref ::cudaMallocHost(void**, size_t) "cudaMallocHost (C API)",
 * ::cudaFreeHost, ::cudaHostAlloc, ::cudaDeviceGetAttribute, ::cudaStreamAttachMemAsync
 */
    pub fn cudaMallocManaged(devPtr: *mut *mut ::libc::c_void,
                             size: usize, flags: ::libc::c_uint)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Allocate memory on the device
 *
 * Allocates \p size bytes of linear memory on the device and returns in
 * \p *devPtr a pointer to the allocated memory. The allocated memory is
 * suitably aligned for any kind of variable. The memory is not cleared.
 * ::cudaMalloc() returns ::cudaErrorMemoryAllocation in case of failure.
 *
 * The device version of ::cudaFree cannot be used with a \p *devPtr
 * allocated using the host API, and vice versa.
 *
 * \param devPtr - Pointer to allocated device memory
 * \param size   - Requested allocation size in bytes
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorMemoryAllocation
 *
 * \sa ::cudaMallocPitch, ::cudaFree, ::cudaMallocArray, ::cudaFreeArray,
 * ::cudaMalloc3D, ::cudaMalloc3DArray,
 * \ref ::cudaMallocHost(void**, size_t) "cudaMallocHost (C API)",
 * ::cudaFreeHost, ::cudaHostAlloc
 */
    pub fn cudaMalloc(devPtr: *mut *mut ::libc::c_void, size: usize)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Allocates page-locked memory on the host
 *
 * Allocates \p size bytes of host memory that is page-locked and accessible
 * to the device. The driver tracks the virtual memory ranges allocated with
 * this function and automatically accelerates calls to functions such as
 * ::cudaMemcpy*(). Since the memory can be accessed directly by the device,
 * it can be read or written with much higher bandwidth than pageable memory
 * obtained with functions such as ::malloc(). Allocating excessive amounts of
 * memory with ::cudaMallocHost() may degrade system performance, since it
 * reduces the amount of memory available to the system for paging. As a
 * result, this function is best used sparingly to allocate staging areas for
 * data exchange between host and device.
 *
 * \param ptr  - Pointer to allocated host memory
 * \param size - Requested allocation size in bytes
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorMemoryAllocation
 * \notefnerr
 *
 * \sa ::cudaMalloc, ::cudaMallocPitch, ::cudaMallocArray, ::cudaMalloc3D,
 * ::cudaMalloc3DArray, ::cudaHostAlloc, ::cudaFree, ::cudaFreeArray,
 * \ref ::cudaMallocHost(void**, size_t, unsigned int) "cudaMallocHost (C++ API)",
 * ::cudaFreeHost, ::cudaHostAlloc
 */
    pub fn cudaMallocHost(ptr: *mut *mut ::libc::c_void, size: usize)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Allocates pitched memory on the device
 *
 * Allocates at least \p width (in bytes) * \p height bytes of linear memory
 * on the device and returns in \p *devPtr a pointer to the allocated memory.
 * The function may pad the allocation to ensure that corresponding pointers
 * in any given row will continue to meet the alignment requirements for
 * coalescing as the address is updated from row to row. The pitch returned in
 * \p *pitch by ::cudaMallocPitch() is the width in bytes of the allocation.
 * The intended usage of \p pitch is as a separate parameter of the allocation,
 * used to compute addresses within the 2D array. Given the row and column of
 * an array element of type \p T, the address is computed as:
 * \code
    T* pElement = (T*)((char*)BaseAddress + Row * pitch) + Column;
   \endcode
 *
 * For allocations of 2D arrays, it is recommended that programmers consider
 * performing pitch allocations using ::cudaMallocPitch(). Due to pitch
 * alignment restrictions in the hardware, this is especially true if the
 * application will be performing 2D memory copies between different regions
 * of device memory (whether linear memory or CUDA arrays).
 *
 * \param devPtr - Pointer to allocated pitched device memory
 * \param pitch  - Pitch for allocation
 * \param width  - Requested pitched allocation width (in bytes)
 * \param height - Requested pitched allocation height
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorMemoryAllocation
 * \notefnerr
 *
 * \sa ::cudaMalloc, ::cudaFree, ::cudaMallocArray, ::cudaFreeArray,
 * \ref ::cudaMallocHost(void**, size_t) "cudaMallocHost (C API)",
 * ::cudaFreeHost, ::cudaMalloc3D, ::cudaMalloc3DArray,
 * ::cudaHostAlloc
 */
    pub fn cudaMallocPitch(devPtr: *mut *mut ::libc::c_void,
                           pitch: *mut usize, width: usize, height: usize)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Allocate an array on the device
 *
 * Allocates a CUDA array according to the ::cudaChannelFormatDesc structure
 * \p desc and returns a handle to the new CUDA array in \p *array.
 *
 * The ::cudaChannelFormatDesc is defined as:
 * \code
    struct cudaChannelFormatDesc {
        int x, y, z, w;
    enum cudaChannelFormatKind f;
    };
    \endcode
 * where ::cudaChannelFormatKind is one of ::cudaChannelFormatKindSigned,
 * ::cudaChannelFormatKindUnsigned, or ::cudaChannelFormatKindFloat.
 *
 * The \p flags parameter enables different options to be specified that affect
 * the allocation, as follows.
 * - ::cudaArrayDefault: This flag's value is defined to be 0 and provides default array allocation
 * - ::cudaArraySurfaceLoadStore: Allocates an array that can be read from or written to using a surface reference
 * - ::cudaArrayTextureGather: This flag indicates that texture gather operations will be performed on the array.
 *
 * \p width and \p height must meet certain size requirements. See ::cudaMalloc3DArray() for more details.
 *
 * \param array  - Pointer to allocated array in device memory
 * \param desc   - Requested channel format
 * \param width  - Requested array allocation width
 * \param height - Requested array allocation height
 * \param flags  - Requested properties of allocated array
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorMemoryAllocation
 * \notefnerr
 *
 * \sa ::cudaMalloc, ::cudaMallocPitch, ::cudaFree, ::cudaFreeArray,
 * \ref ::cudaMallocHost(void**, size_t) "cudaMallocHost (C API)",
 * ::cudaFreeHost, ::cudaMalloc3D, ::cudaMalloc3DArray,
 * ::cudaHostAlloc
 */
    pub fn cudaMallocArray(array: *mut cudaArray_t,
                           desc: *const cudaChannelFormatDesc, width: usize,
                           height: usize, flags: ::libc::c_uint)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Frees memory on the device
 *
 * Frees the memory space pointed to by \p devPtr, which must have been
 * returned by a previous call to ::cudaMalloc() or ::cudaMallocPitch().
 * Otherwise, or if ::cudaFree(\p devPtr) has already been called before,
 * an error is returned. If \p devPtr is 0, no operation is performed.
 * ::cudaFree() returns ::cudaErrorInvalidDevicePointer in case of failure.
 *
 * The device version of ::cudaFree cannot be used with a \p *devPtr
 * allocated using the host API, and vice versa.
 *
 * \param devPtr - Device pointer to memory to free
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidDevicePointer,
 * ::cudaErrorInitializationError
 * \notefnerr
 *
 * \sa ::cudaMalloc, ::cudaMallocPitch, ::cudaMallocArray, ::cudaFreeArray,
 * \ref ::cudaMallocHost(void**, size_t) "cudaMallocHost (C API)",
 * ::cudaFreeHost, ::cudaMalloc3D, ::cudaMalloc3DArray,
 * ::cudaHostAlloc
 */
    pub fn cudaFree(devPtr: *mut ::libc::c_void) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Frees page-locked memory
 *
 * Frees the memory space pointed to by \p hostPtr, which must have been
 * returned by a previous call to ::cudaMallocHost() or ::cudaHostAlloc().
 *
 * \param ptr - Pointer to memory to free
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInitializationError
 * \notefnerr
 *
 * \sa ::cudaMalloc, ::cudaMallocPitch, ::cudaFree, ::cudaMallocArray,
 * ::cudaFreeArray,
 * \ref ::cudaMallocHost(void**, size_t) "cudaMallocHost (C API)",
 * ::cudaMalloc3D, ::cudaMalloc3DArray, ::cudaHostAlloc
 */
    pub fn cudaFreeHost(ptr: *mut ::libc::c_void) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Frees an array on the device
 *
 * Frees the CUDA array \p array, which must have been * returned by a
 * previous call to ::cudaMallocArray(). If ::cudaFreeArray(\p array) has
 * already been called before, ::cudaErrorInvalidValue is returned. If
 * \p devPtr is 0, no operation is performed.
 *
 * \param array - Pointer to array to free
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInitializationError
 * \notefnerr
 *
 * \sa ::cudaMalloc, ::cudaMallocPitch, ::cudaFree, ::cudaMallocArray,
 * \ref ::cudaMallocHost(void**, size_t) "cudaMallocHost (C API)",
 * ::cudaFreeHost, ::cudaHostAlloc
 */
    pub fn cudaFreeArray(array: cudaArray_t) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Frees a mipmapped array on the device
 *
 * Frees the CUDA mipmapped array \p mipmappedArray, which must have been
 * returned by a previous call to ::cudaMallocMipmappedArray().
 * If ::cudaFreeMipmappedArray(\p mipmappedArray) has already been called before,
 * ::cudaErrorInvalidValue is returned.
 *
 * \param mipmappedArray - Pointer to mipmapped array to free
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInitializationError
 * \notefnerr
 *
 * \sa ::cudaMalloc, ::cudaMallocPitch, ::cudaFree, ::cudaMallocArray,
 * \ref ::cudaMallocHost(void**, size_t) "cudaMallocHost (C API)",
 * ::cudaFreeHost, ::cudaHostAlloc
 */
    pub fn cudaFreeMipmappedArray(mipmappedArray: cudaMipmappedArray_t)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Allocates page-locked memory on the host
 *
 * Allocates \p size bytes of host memory that is page-locked and accessible
 * to the device. The driver tracks the virtual memory ranges allocated with
 * this function and automatically accelerates calls to functions such as
 * ::cudaMemcpy(). Since the memory can be accessed directly by the device, it
 * can be read or written with much higher bandwidth than pageable memory
 * obtained with functions such as ::malloc(). Allocating excessive amounts of
 * pinned memory may degrade system performance, since it reduces the amount
 * of memory available to the system for paging. As a result, this function is
 * best used sparingly to allocate staging areas for data exchange between host
 * and device.
 *
 * The \p flags parameter enables different options to be specified that affect
 * the allocation, as follows.
 * - ::cudaHostAllocDefault: This flag's value is defined to be 0 and causes
 * ::cudaHostAlloc() to emulate ::cudaMallocHost().
 * - ::cudaHostAllocPortable: The memory returned by this call will be
 * considered as pinned memory by all CUDA contexts, not just the one that
 * performed the allocation.
 * - ::cudaHostAllocMapped: Maps the allocation into the CUDA address space.
 * The device pointer to the memory may be obtained by calling
 * ::cudaHostGetDevicePointer().
 * - ::cudaHostAllocWriteCombined: Allocates the memory as write-combined (WC).
 * WC memory can be transferred across the PCI Express bus more quickly on some
 * system configurations, but cannot be read efficiently by most CPUs.  WC
 * memory is a good option for buffers that will be written by the CPU and read
 * by the device via mapped pinned memory or host->device transfers.
 *
 * All of these flags are orthogonal to one another: a developer may allocate
 * memory that is portable, mapped and/or write-combined with no restrictions.
 *
 * ::cudaSetDeviceFlags() must have been called with the ::cudaDeviceMapHost
 * flag in order for the ::cudaHostAllocMapped flag to have any effect.
 *
 * The ::cudaHostAllocMapped flag may be specified on CUDA contexts for devices
 * that do not support mapped pinned memory. The failure is deferred to
 * ::cudaHostGetDevicePointer() because the memory may be mapped into other
 * CUDA contexts via the ::cudaHostAllocPortable flag.
 *
 * Memory allocated by this function must be freed with ::cudaFreeHost().
 *
 * \param pHost - Device pointer to allocated memory
 * \param size  - Requested allocation size in bytes
 * \param flags - Requested properties of allocated memory
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorMemoryAllocation
 * \notefnerr
 *
 * \sa ::cudaSetDeviceFlags,
 * \ref ::cudaMallocHost(void**, size_t) "cudaMallocHost (C API)",
 * ::cudaFreeHost
 */
    pub fn cudaHostAlloc(pHost: *mut *mut ::libc::c_void, size: usize,
                         flags: ::libc::c_uint) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Registers an existing host memory range for use by CUDA
 *
 * Page-locks the memory range specified by \p ptr and \p size and maps it
 * for the device(s) as specified by \p flags. This memory range also is added
 * to the same tracking mechanism as ::cudaHostAlloc() to automatically accelerate
 * calls to functions such as ::cudaMemcpy(). Since the memory can be accessed
 * directly by the device, it can be read or written with much higher bandwidth
 * than pageable memory that has not been registered.  Page-locking excessive
 * amounts of memory may degrade system performance, since it reduces the amount
 * of memory available to the system for paging. As a result, this function is
 * best used sparingly to register staging areas for data exchange between
 * host and device.
 *
 * The \p flags parameter enables different options to be specified that
 * affect the allocation, as follows.
 *
 * - ::cudaHostRegisterDefault: On a system with unified virtual addressing,
 *   the memory will be both mapped and portable.  On a system with no unified
 *   virtual addressing, the memory will be neither mapped nor portable.
 *
 * - ::cudaHostRegisterPortable: The memory returned by this call will be
 *   considered as pinned memory by all CUDA contexts, not just the one that
 *   performed the allocation.
 *
 * - ::cudaHostRegisterMapped: Maps the allocation into the CUDA address
 *   space. The device pointer to the memory may be obtained by calling
 *   ::cudaHostGetDevicePointer().
 *
 * - ::cudaHostRegisterIoMemory: The passed memory pointer is treated as
 *   pointing to some memory-mapped I/O space, e.g. belonging to a
 *   third-party PCIe device, and it will marked as non cache-coherent and
 *   contiguous.
 *
 * All of these flags are orthogonal to one another: a developer may page-lock
 * memory that is portable or mapped with no restrictions.
 *
 * The CUDA context must have been created with the ::cudaMapHost flag in
 * order for the ::cudaHostRegisterMapped flag to have any effect.
 *
 * The ::cudaHostRegisterMapped flag may be specified on CUDA contexts for
 * devices that do not support mapped pinned memory. The failure is deferred
 * to ::cudaHostGetDevicePointer() because the memory may be mapped into
 * other CUDA contexts via the ::cudaHostRegisterPortable flag.
 *
 * For devices that have a non-zero value for the device attribute
 * ::cudaDevAttrCanUseHostPointerForRegisteredMem, the memory
 * can also be accessed from the device using the host pointer \p ptr.
 * The device pointer returned by ::cudaHostGetDevicePointer() may or may not
 * match the original host pointer \p ptr and depends on the devices visible to the
 * application. If all devices visible to the application have a non-zero value for the
 * device attribute, the device pointer returned by ::cudaHostGetDevicePointer()
 * will match the original pointer \p ptr. If any device visible to the application
 * has a zero value for the device attribute, the device pointer returned by
 * ::cudaHostGetDevicePointer() will not match the original host pointer \p ptr,
 * but it will be suitable for use on all devices provided Unified Virtual Addressing
 * is enabled. In such systems, it is valid to access the memory using either pointer
 * on devices that have a non-zero value for the device attribute. Note however that
 * such devices should access the memory using only of the two pointers and not both.
 *
 * The memory page-locked by this function must be unregistered with ::cudaHostUnregister().
 *
 * \param ptr   - Host pointer to memory to page-lock
 * \param size  - Size in bytes of the address range to page-lock in bytes
 * \param flags - Flags for allocation request
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorMemoryAllocation,
 * ::cudaErrorHostMemoryAlreadyRegistered
 * \notefnerr
 *
 * \sa ::cudaHostUnregister, ::cudaHostGetFlags, ::cudaHostGetDevicePointer
 */
    pub fn cudaHostRegister(ptr: *mut ::libc::c_void, size: usize,
                            flags: ::libc::c_uint) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Unregisters a memory range that was registered with cudaHostRegister
 *
 * Unmaps the memory range whose base address is specified by \p ptr, and makes
 * it pageable again.
 *
 * The base address must be the same one specified to ::cudaHostRegister().
 *
 * \param ptr - Host pointer to memory to unregister
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue
 * \notefnerr
 *
 * \sa ::cudaHostUnregister
 */
    pub fn cudaHostUnregister(ptr: *mut ::libc::c_void)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Passes back device pointer of mapped host memory allocated by
 * cudaHostAlloc or registered by cudaHostRegister
 *
 * Passes back the device pointer corresponding to the mapped, pinned host
 * buffer allocated by ::cudaHostAlloc() or registered by ::cudaHostRegister().
 *
 * ::cudaHostGetDevicePointer() will fail if the ::cudaDeviceMapHost flag was
 * not specified before deferred context creation occurred, or if called on a
 * device that does not support mapped, pinned memory.
 *
 * For devices that have a non-zero value for the device attribute
 * ::cudaDevAttrCanUseHostPointerForRegisteredMem, the memory
 * can also be accessed from the device using the host pointer \p pHost.
 * The device pointer returned by ::cudaHostGetDevicePointer() may or may not
 * match the original host pointer \p pHost and depends on the devices visible to the
 * application. If all devices visible to the application have a non-zero value for the
 * device attribute, the device pointer returned by ::cudaHostGetDevicePointer()
 * will match the original pointer \p pHost. If any device visible to the application
 * has a zero value for the device attribute, the device pointer returned by
 * ::cudaHostGetDevicePointer() will not match the original host pointer \p pHost,
 * but it will be suitable for use on all devices provided Unified Virtual Addressing
 * is enabled. In such systems, it is valid to access the memory using either pointer
 * on devices that have a non-zero value for the device attribute. Note however that
 * such devices should access the memory using only of the two pointers and not both.
 *
 * \p flags provides for future releases.  For now, it must be set to 0.
 *
 * \param pDevice - Returned device pointer for mapped memory
 * \param pHost   - Requested host pointer mapping
 * \param flags   - Flags for extensions (must be 0 for now)
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorMemoryAllocation
 * \notefnerr
 *
 * \sa ::cudaSetDeviceFlags, ::cudaHostAlloc
 */
    pub fn cudaHostGetDevicePointer(pDevice: *mut *mut ::libc::c_void,
                                    pHost: *mut ::libc::c_void,
                                    flags: ::libc::c_uint)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Passes back flags used to allocate pinned host memory allocated by
 * cudaHostAlloc
 *
 * ::cudaHostGetFlags() will fail if the input pointer does not
 * reside in an address range allocated by ::cudaHostAlloc().
 *
 * \param pFlags - Returned flags word
 * \param pHost - Host pointer
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue
 * \notefnerr
 *
 * \sa ::cudaHostAlloc
 */
    pub fn cudaHostGetFlags(pFlags: *mut ::libc::c_uint,
                            pHost: *mut ::libc::c_void)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Allocates logical 1D, 2D, or 3D memory objects on the device
 *
 * Allocates at least \p width * \p height * \p depth bytes of linear memory
 * on the device and returns a ::cudaPitchedPtr in which \p ptr is a pointer
 * to the allocated memory. The function may pad the allocation to ensure
 * hardware alignment requirements are met. The pitch returned in the \p pitch
 * field of \p pitchedDevPtr is the width in bytes of the allocation.
 *
 * The returned ::cudaPitchedPtr contains additional fields \p xsize and
 * \p ysize, the logical width and height of the allocation, which are
 * equivalent to the \p width and \p height \p extent parameters provided by
 * the programmer during allocation.
 *
 * For allocations of 2D and 3D objects, it is highly recommended that
 * programmers perform allocations using ::cudaMalloc3D() or
 * ::cudaMallocPitch(). Due to alignment restrictions in the hardware, this is
 * especially true if the application will be performing memory copies
 * involving 2D or 3D objects (whether linear memory or CUDA arrays).
 *
 * \param pitchedDevPtr  - Pointer to allocated pitched device memory
 * \param extent         - Requested allocation size (\p width field in bytes)
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorMemoryAllocation
 * \notefnerr
 *
 * \sa ::cudaMallocPitch, ::cudaFree, ::cudaMemcpy3D, ::cudaMemset3D,
 * ::cudaMalloc3DArray, ::cudaMallocArray, ::cudaFreeArray,
 * \ref ::cudaMallocHost(void**, size_t) "cudaMallocHost (C API)",
 * ::cudaFreeHost, ::cudaHostAlloc, ::make_cudaPitchedPtr, ::make_cudaExtent
 */
    pub fn cudaMalloc3D(pitchedDevPtr: *mut cudaPitchedPtr,
                        extent: cudaExtent) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Allocate an array on the device
 *
 * Allocates a CUDA array according to the ::cudaChannelFormatDesc structure
 * \p desc and returns a handle to the new CUDA array in \p *array.
 *
 * The ::cudaChannelFormatDesc is defined as:
 * \code
    struct cudaChannelFormatDesc {
        int x, y, z, w;
        enum cudaChannelFormatKind f;
    };
    \endcode
 * where ::cudaChannelFormatKind is one of ::cudaChannelFormatKindSigned,
 * ::cudaChannelFormatKindUnsigned, or ::cudaChannelFormatKindFloat.
 *
 * ::cudaMalloc3DArray() can allocate the following:
 *
 * - A 1D array is allocated if the height and depth extents are both zero.
 * - A 2D array is allocated if only the depth extent is zero.
 * - A 3D array is allocated if all three extents are non-zero.
 * - A 1D layered CUDA array is allocated if only the height extent is zero and
 * the cudaArrayLayered flag is set. Each layer is a 1D array. The number of layers is
 * determined by the depth extent.
 * - A 2D layered CUDA array is allocated if all three extents are non-zero and
 * the cudaArrayLayered flag is set. Each layer is a 2D array. The number of layers is
 * determined by the depth extent.
 * - A cubemap CUDA array is allocated if all three extents are non-zero and the
 * cudaArrayCubemap flag is set. Width must be equal to height, and depth must be six. A cubemap is
 * a special type of 2D layered CUDA array, where the six layers represent the six faces of a cube.
 * The order of the six layers in memory is the same as that listed in ::cudaGraphicsCubeFace.
 * - A cubemap layered CUDA array is allocated if all three extents are non-zero, and both,
 * cudaArrayCubemap and cudaArrayLayered flags are set. Width must be equal to height, and depth must be
 * a multiple of six. A cubemap layered CUDA array is a special type of 2D layered CUDA array that consists
 * of a collection of cubemaps. The first six layers represent the first cubemap, the next six layers form
 * the second cubemap, and so on.
 *
 *
 * The \p flags parameter enables different options to be specified that affect
 * the allocation, as follows.
 * - ::cudaArrayDefault: This flag's value is defined to be 0 and provides default array allocation
 * - ::cudaArrayLayered: Allocates a layered CUDA array, with the depth extent indicating the number of layers
 * - ::cudaArrayCubemap: Allocates a cubemap CUDA array. Width must be equal to height, and depth must be six.
 *   If the cudaArrayLayered flag is also set, depth must be a multiple of six.
 * - ::cudaArraySurfaceLoadStore: Allocates a CUDA array that could be read from or written to using a surface
 *   reference.
 * - ::cudaArrayTextureGather: This flag indicates that texture gather operations will be performed on the CUDA
 *   array. Texture gather can only be performed on 2D CUDA arrays.
 *
 * The width, height and depth extents must meet certain size requirements as listed in the following table.
 * All values are specified in elements.
 *
 * Note that 2D CUDA arrays have different size requirements if the ::cudaArrayTextureGather flag is set. In that
 * case, the valid range for (width, height, depth) is ((1,maxTexture2DGather[0]), (1,maxTexture2DGather[1]), 0).
 *
 * \xmlonly
 * <table outputclass="xmlonly">
 * <tgroup cols="3" colsep="1" rowsep="1">
 * <colspec colname="c1" colwidth="1.0*"/>
 * <colspec colname="c2" colwidth="3.0*"/>
 * <colspec colname="c3" colwidth="3.0*"/>
 * <thead>
 * <row>
 * <entry>CUDA array type</entry>
 * <entry>Valid extents that must always be met {(width range in elements),
 * (height range), (depth range)}</entry>
 * <entry>Valid extents with cudaArraySurfaceLoadStore set {(width range in
 * elements), (height range), (depth range)}</entry>
 * </row>
 * </thead>
 * <tbody>
 * <row>
 * <entry>1D</entry>
 * <entry>{ (1,maxTexture1D), 0, 0 }</entry>
 * <entry>{ (1,maxSurface1D), 0, 0 }</entry>
 * </row>
 * <row>
 * <entry>2D</entry>
 * <entry>{ (1,maxTexture2D[0]), (1,maxTexture2D[1]), 0 }</entry>
 * <entry>{ (1,maxSurface2D[0]), (1,maxSurface2D[1]), 0 }</entry>
 * </row>
 * <row>
 * <entry>3D</entry>
 * <entry>{ (1,maxTexture3D[0]), (1,maxTexture3D[1]), (1,maxTexture3D[2]) }
 * OR { (1,maxTexture3DAlt[0]), (1,maxTexture3DAlt[1]),
 * (1,maxTexture3DAlt[2]) }</entry>
 * <entry>{ (1,maxSurface3D[0]), (1,maxSurface3D[1]), (1,maxSurface3D[2]) }</entry>
 * </row>
 * <row>
 * <entry>1D Layered</entry>
 * <entry>{ (1,maxTexture1DLayered[0]), 0, (1,maxTexture1DLayered[1]) }</entry>
 * <entry>{ (1,maxSurface1DLayered[0]), 0, (1,maxSurface1DLayered[1]) }</entry>
 * </row>
 * <row>
 * <entry>2D Layered</entry>
 * <entry>{ (1,maxTexture2DLayered[0]), (1,maxTexture2DLayered[1]),
 * (1,maxTexture2DLayered[2]) }</entry>
 * <entry>{ (1,maxSurface2DLayered[0]), (1,maxSurface2DLayered[1]),
 * (1,maxSurface2DLayered[2]) }</entry>
 * </row>
 * <row>
 * <entry>Cubemap</entry>
 * <entry>{ (1,maxTextureCubemap), (1,maxTextureCubemap), 6 }</entry>
 * <entry>{ (1,maxSurfaceCubemap), (1,maxSurfaceCubemap), 6 }</entry>
 * </row>
 * <row>
 * <entry>Cubemap Layered</entry>
 * <entry>{ (1,maxTextureCubemapLayered[0]), (1,maxTextureCubemapLayered[0]),
 * (1,maxTextureCubemapLayered[1]) }</entry>
 * <entry>{ (1,maxSurfaceCubemapLayered[0]), (1,maxSurfaceCubemapLayered[0]),
 * (1,maxSurfaceCubemapLayered[1]) }</entry>
 * </row>
 * </tbody>
 * </tgroup>
 * </table>
 * \endxmlonly
 *
 * \param array  - Pointer to allocated array in device memory
 * \param desc   - Requested channel format
 * \param extent - Requested allocation size (\p width field in elements)
 * \param flags  - Flags for extensions
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorMemoryAllocation
 * \notefnerr
 *
 * \sa ::cudaMalloc3D, ::cudaMalloc, ::cudaMallocPitch, ::cudaFree,
 * ::cudaFreeArray,
 * \ref ::cudaMallocHost(void**, size_t) "cudaMallocHost (C API)",
 * ::cudaFreeHost, ::cudaHostAlloc,
 * ::make_cudaExtent
 */
    pub fn cudaMalloc3DArray(array: *mut cudaArray_t,
                             desc: *const cudaChannelFormatDesc,
                             extent: cudaExtent,
                             flags: ::libc::c_uint) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Allocate a mipmapped array on the device
 *
 * Allocates a CUDA mipmapped array according to the ::cudaChannelFormatDesc structure
 * \p desc and returns a handle to the new CUDA mipmapped array in \p *mipmappedArray.
 * \p numLevels specifies the number of mipmap levels to be allocated. This value is
 * clamped to the range [1, 1 + floor(log2(max(width, height, depth)))].
 *
 * The ::cudaChannelFormatDesc is defined as:
 * \code
    struct cudaChannelFormatDesc {
        int x, y, z, w;
        enum cudaChannelFormatKind f;
    };
    \endcode
 * where ::cudaChannelFormatKind is one of ::cudaChannelFormatKindSigned,
 * ::cudaChannelFormatKindUnsigned, or ::cudaChannelFormatKindFloat.
 *
 * ::cudaMallocMipmappedArray() can allocate the following:
 *
 * - A 1D mipmapped array is allocated if the height and depth extents are both zero.
 * - A 2D mipmapped array is allocated if only the depth extent is zero.
 * - A 3D mipmapped array is allocated if all three extents are non-zero.
 * - A 1D layered CUDA mipmapped array is allocated if only the height extent is zero and
 * the cudaArrayLayered flag is set. Each layer is a 1D mipmapped array. The number of layers is
 * determined by the depth extent.
 * - A 2D layered CUDA mipmapped array is allocated if all three extents are non-zero and
 * the cudaArrayLayered flag is set. Each layer is a 2D mipmapped array. The number of layers is
 * determined by the depth extent.
 * - A cubemap CUDA mipmapped array is allocated if all three extents are non-zero and the
 * cudaArrayCubemap flag is set. Width must be equal to height, and depth must be six.
 * The order of the six layers in memory is the same as that listed in ::cudaGraphicsCubeFace.
 * - A cubemap layered CUDA mipmapped array is allocated if all three extents are non-zero, and both,
 * cudaArrayCubemap and cudaArrayLayered flags are set. Width must be equal to height, and depth must be
 * a multiple of six. A cubemap layered CUDA mipmapped array is a special type of 2D layered CUDA mipmapped
 * array that consists of a collection of cubemap mipmapped arrays. The first six layers represent the
 * first cubemap mipmapped array, the next six layers form the second cubemap mipmapped array, and so on.
 *
 *
 * The \p flags parameter enables different options to be specified that affect
 * the allocation, as follows.
 * - ::cudaArrayDefault: This flag's value is defined to be 0 and provides default mipmapped array allocation
 * - ::cudaArrayLayered: Allocates a layered CUDA mipmapped array, with the depth extent indicating the number of layers
 * - ::cudaArrayCubemap: Allocates a cubemap CUDA mipmapped array. Width must be equal to height, and depth must be six.
 *   If the cudaArrayLayered flag is also set, depth must be a multiple of six.
 * - ::cudaArraySurfaceLoadStore: This flag indicates that individual mipmap levels of the CUDA mipmapped array
 *   will be read from or written to using a surface reference.
 * - ::cudaArrayTextureGather: This flag indicates that texture gather operations will be performed on the CUDA
 *   array. Texture gather can only be performed on 2D CUDA mipmapped arrays, and the gather operations are
 *   performed only on the most detailed mipmap level.
 *
 * The width, height and depth extents must meet certain size requirements as listed in the following table.
 * All values are specified in elements.
 *
 * \xmlonly
 * <table outputclass="xmlonly">
 * <tgroup cols="2" colsep="1" rowsep="1">
 * <colspec colname="c1" colwidth="1.0*"/>
 * <colspec colname="c2" colwidth="3.0*"/>
 * <thead>
 * <row>
 * <entry>CUDA array type</entry>
 * <entry>Valid extents {(width range in elements), (height range), (depth
 * range)}</entry>
 * </row>
 * </thead>
 * <tbody>
 * <row>
 * <entry>1D</entry>
 * <entry>{ (1,maxTexture1DMipmap), 0, 0 }</entry>
 * </row>
 * <row>
 * <entry>2D</entry>
 * <entry>{ (1,maxTexture2DMipmap[0]), (1,maxTexture2DMipmap[1]), 0 }</entry>
 * </row>
 * <row>
 * <entry>3D</entry>
 * <entry>{ (1,maxTexture3D[0]), (1,maxTexture3D[1]), (1,maxTexture3D[2]) }</entry>
 * </row>
 * <row>
 * <entry>1D Layered</entry>
 * <entry>{ (1,maxTexture1DLayered[0]), 0, (1,maxTexture1DLayered[1]) }</entry>
 * </row>
 * <row>
 * <entry>2D Layered</entry>
 * <entry>{ (1,maxTexture2DLayered[0]), (1,maxTexture2DLayered[1]),
 * (1,maxTexture2DLayered[2]) }</entry>
 * </row>
 * <row>
 * <entry>Cubemap</entry>
 * <entry>{ (1,maxTextureCubemap), (1,maxTextureCubemap), 6 }</entry>
 * </row>
 * <row>
 * <entry>Cubemap Layered</entry>
 * <entry>{ (1,maxTextureCubemapLayered[0]), (1,maxTextureCubemapLayered[0]),
 * (1,maxTextureCubemapLayered[1]) }</entry>
 * </row>
 * </tbody>
 * </tgroup>
 * </table>
 * \endxmlonly
 *
 * \param mipmappedArray  - Pointer to allocated mipmapped array in device memory
 * \param desc            - Requested channel format
 * \param extent          - Requested allocation size (\p width field in elements)
 * \param numLevels       - Number of mipmap levels to allocate
 * \param flags           - Flags for extensions
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorMemoryAllocation
 * \notefnerr
 *
 * \sa ::cudaMalloc3D, ::cudaMalloc, ::cudaMallocPitch, ::cudaFree,
 * ::cudaFreeArray,
 * \ref ::cudaMallocHost(void**, size_t) "cudaMallocHost (C API)",
 * ::cudaFreeHost, ::cudaHostAlloc,
 * ::make_cudaExtent
 */
    pub fn cudaMallocMipmappedArray(mipmappedArray: *mut cudaMipmappedArray_t,
                                    desc: *const cudaChannelFormatDesc,
                                    extent: cudaExtent,
                                    numLevels: ::libc::c_uint,
                                    flags: ::libc::c_uint)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Gets a mipmap level of a CUDA mipmapped array
 *
 * Returns in \p *levelArray a CUDA array that represents a single mipmap level
 * of the CUDA mipmapped array \p mipmappedArray.
 *
 * If \p level is greater than the maximum number of levels in this mipmapped array,
 * ::cudaErrorInvalidValue is returned.
 *
 * \param levelArray     - Returned mipmap level CUDA array
 * \param mipmappedArray - CUDA mipmapped array
 * \param level          - Mipmap level
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue
 * \notefnerr
 *
 * \sa ::cudaMalloc3D, ::cudaMalloc, ::cudaMallocPitch, ::cudaFree,
 * ::cudaFreeArray,
 * \ref ::cudaMallocHost(void**, size_t) "cudaMallocHost (C API)",
 * ::cudaFreeHost, ::cudaHostAlloc,
 * ::make_cudaExtent
 */
    pub fn cudaGetMipmappedArrayLevel(levelArray: *mut cudaArray_t,
                                      mipmappedArray:
                                          cudaMipmappedArray_const_t,
                                      level: ::libc::c_uint)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Copies data between 3D objects
 *
\code
struct cudaExtent {
  size_t width;
  size_t height;
  size_t depth;
};
struct cudaExtent make_cudaExtent(size_t w, size_t h, size_t d);

struct cudaPos {
  size_t x;
  size_t y;
  size_t z;
};
struct cudaPos make_cudaPos(size_t x, size_t y, size_t z);

struct cudaMemcpy3DParms {
  cudaArray_t           srcArray;
  struct cudaPos        srcPos;
  struct cudaPitchedPtr srcPtr;
  cudaArray_t           dstArray;
  struct cudaPos        dstPos;
  struct cudaPitchedPtr dstPtr;
  struct cudaExtent     extent;
  enum cudaMemcpyKind   kind;
};
\endcode
 *
 * ::cudaMemcpy3D() copies data betwen two 3D objects. The source and
 * destination objects may be in either host memory, device memory, or a CUDA
 * array. The source, destination, extent, and kind of copy performed is
 * specified by the ::cudaMemcpy3DParms struct which should be initialized to
 * zero before use:
\code
cudaMemcpy3DParms myParms = {0};
\endcode
 *
 * The struct passed to ::cudaMemcpy3D() must specify one of \p srcArray or
 * \p srcPtr and one of \p dstArray or \p dstPtr. Passing more than one
 * non-zero source or destination will cause ::cudaMemcpy3D() to return an
 * error.
 *
 * The \p srcPos and \p dstPos fields are optional offsets into the source and
 * destination objects and are defined in units of each object's elements. The
 * element for a host or device pointer is assumed to be <b>unsigned char</b>.
 * For CUDA arrays, positions must be in the range [0, 2048) for any
 * dimension.
 *
 * The \p extent field defines the dimensions of the transferred area in
 * elements. If a CUDA array is participating in the copy, the extent is
 * defined in terms of that array's elements. If no CUDA array is
 * participating in the copy then the extents are defined in elements of
 * <b>unsigned char</b>.
 *
 * The \p kind field defines the direction of the copy. It must be one of
 * ::cudaMemcpyHostToHost, ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
 * ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
 * ::cudaMemcpyDefault is recommended, in which case the type of transfer is
 * inferred from the pointer values. However, ::cudaMemcpyDefault is only
 * allowed on systems that support unified virtual addressing.
 *
 * If the source and destination are both arrays, ::cudaMemcpy3D() will return
 * an error if they do not have the same element size.
 *
 * The source and destination object may not overlap. If overlapping source
 * and destination objects are specified, undefined behavior will result.
 *
 * The source object must lie entirely within the region defined by \p srcPos
 * and \p extent. The destination object must lie entirely within the region
 * defined by \p dstPos and \p extent.
 *
 * ::cudaMemcpy3D() returns an error if the pitch of \p srcPtr or \p dstPtr
 * exceeds the maximum allowed. The pitch of a ::cudaPitchedPtr allocated
 * with ::cudaMalloc3D() will always be valid.
 *
 * \param p - 3D memory copy parameters
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidDevicePointer,
 * ::cudaErrorInvalidPitchValue,
 * ::cudaErrorInvalidMemcpyDirection
 * \notefnerr
 * \note_sync
 *
 * \sa ::cudaMalloc3D, ::cudaMalloc3DArray, ::cudaMemset3D, ::cudaMemcpy3DAsync,
 * ::cudaMemcpy, ::cudaMemcpy2D, ::cudaMemcpyToArray,
 * ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
 * ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
 * ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
 * ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
 * ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
 * ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync,
 * ::make_cudaExtent, ::make_cudaPos
 */
    pub fn cudaMemcpy3D(p: *const cudaMemcpy3DParms) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Copies memory between devices
 *
 * Perform a 3D memory copy according to the parameters specified in
 * \p p.  See the definition of the ::cudaMemcpy3DPeerParms structure
 * for documentation of its parameters.
 *
 * Note that this function is synchronous with respect to the host only if
 * the source or destination of the transfer is host memory.  Note also
 * that this copy is serialized with respect to all pending and future
 * asynchronous work in to the current device, the copy's source device,
 * and the copy's destination device (use ::cudaMemcpy3DPeerAsync to avoid
 * this synchronization).
 *
 * \param p - Parameters for the memory copy
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidDevice
 * \notefnerr
 * \note_sync
 *
 * \sa ::cudaMemcpy, ::cudaMemcpyPeer, ::cudaMemcpyAsync, ::cudaMemcpyPeerAsync,
 * ::cudaMemcpy3DPeerAsync
 */
    pub fn cudaMemcpy3DPeer(p: *const cudaMemcpy3DPeerParms) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Copies data between 3D objects
 *
\code
struct cudaExtent {
  size_t width;
  size_t height;
  size_t depth;
};
struct cudaExtent make_cudaExtent(size_t w, size_t h, size_t d);

struct cudaPos {
  size_t x;
  size_t y;
  size_t z;
};
struct cudaPos make_cudaPos(size_t x, size_t y, size_t z);

struct cudaMemcpy3DParms {
  cudaArray_t           srcArray;
  struct cudaPos        srcPos;
  struct cudaPitchedPtr srcPtr;
  cudaArray_t           dstArray;
  struct cudaPos        dstPos;
  struct cudaPitchedPtr dstPtr;
  struct cudaExtent     extent;
  enum cudaMemcpyKind   kind;
};
\endcode
 *
 * ::cudaMemcpy3DAsync() copies data betwen two 3D objects. The source and
 * destination objects may be in either host memory, device memory, or a CUDA
 * array. The source, destination, extent, and kind of copy performed is
 * specified by the ::cudaMemcpy3DParms struct which should be initialized to
 * zero before use:
\code
cudaMemcpy3DParms myParms = {0};
\endcode
 *
 * The struct passed to ::cudaMemcpy3DAsync() must specify one of \p srcArray
 * or \p srcPtr and one of \p dstArray or \p dstPtr. Passing more than one
 * non-zero source or destination will cause ::cudaMemcpy3DAsync() to return an
 * error.
 *
 * The \p srcPos and \p dstPos fields are optional offsets into the source and
 * destination objects and are defined in units of each object's elements. The
 * element for a host or device pointer is assumed to be <b>unsigned char</b>.
 * For CUDA arrays, positions must be in the range [0, 2048) for any
 * dimension.
 *
 * The \p extent field defines the dimensions of the transferred area in
 * elements. If a CUDA array is participating in the copy, the extent is
 * defined in terms of that array's elements. If no CUDA array is
 * participating in the copy then the extents are defined in elements of
 * <b>unsigned char</b>.
 *
 * The \p kind field defines the direction of the copy. It must be one of
 * ::cudaMemcpyHostToHost, ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
 * ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
 * ::cudaMemcpyDefault is recommended, in which case the type of transfer is
 * inferred from the pointer values. However, ::cudaMemcpyDefault is only
 * allowed on systems that support unified virtual addressing.
 *
 * If the source and destination are both arrays, ::cudaMemcpy3DAsync() will
 * return an error if they do not have the same element size.
 *
 * The source and destination object may not overlap. If overlapping source
 * and destination objects are specified, undefined behavior will result.
 *
 * The source object must lie entirely within the region defined by \p srcPos
 * and \p extent. The destination object must lie entirely within the region
 * defined by \p dstPos and \p extent.
 *
 * ::cudaMemcpy3DAsync() returns an error if the pitch of \p srcPtr or
 * \p dstPtr exceeds the maximum allowed. The pitch of a
 * ::cudaPitchedPtr allocated with ::cudaMalloc3D() will always be valid.
 *
 * ::cudaMemcpy3DAsync() is asynchronous with respect to the host, so
 * the call may return before the copy is complete. The copy can optionally
 * be associated to a stream by passing a non-zero \p stream argument. If
 * \p kind is ::cudaMemcpyHostToDevice or ::cudaMemcpyDeviceToHost and \p stream
 * is non-zero, the copy may overlap with operations in other streams.
 *
 * The device version of this function only handles device to device copies and
 * cannot be given local or shared pointers.
 *
 * \param p      - 3D memory copy parameters
 * \param stream - Stream identifier
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidDevicePointer,
 * ::cudaErrorInvalidPitchValue,
 * ::cudaErrorInvalidMemcpyDirection
 * \notefnerr
 * \note_async
 * \note_null_stream
 *
 * \sa ::cudaMalloc3D, ::cudaMalloc3DArray, ::cudaMemset3D, ::cudaMemcpy3D,
 * ::cudaMemcpy, ::cudaMemcpy2D, ::cudaMemcpyToArray,
 * ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
 * ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
 * ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
 * ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
 * ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
 * ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync,
 * ::make_cudaExtent, ::make_cudaPos
 */
    pub fn cudaMemcpy3DAsync(p: *const cudaMemcpy3DParms,
                             stream: cudaStream_t) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Copies memory between devices asynchronously.
 *
 * Perform a 3D memory copy according to the parameters specified in
 * \p p.  See the definition of the ::cudaMemcpy3DPeerParms structure
 * for documentation of its parameters.
 *
 * \param p      - Parameters for the memory copy
 * \param stream - Stream identifier
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidDevice
 * \notefnerr
 * \note_async
 * \note_null_stream
 *
 * \sa ::cudaMemcpy, ::cudaMemcpyPeer, ::cudaMemcpyAsync, ::cudaMemcpyPeerAsync,
 * ::cudaMemcpy3DPeerAsync
 */
    pub fn cudaMemcpy3DPeerAsync(p: *const cudaMemcpy3DPeerParms,
                                 stream: cudaStream_t) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Gets free and total device memory
 *
 * Returns in \p *free and \p *total respectively, the free and total amount of
 * memory available for allocation by the device in bytes.
 *
 * \param free  - Returned free memory in bytes
 * \param total - Returned total memory in bytes
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInitializationError,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorLaunchFailure
 * \notefnerr
 *
 */
    pub fn cudaMemGetInfo(free: *mut usize, total: *mut usize) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Gets info about the specified cudaArray
 *
 * Returns in \p *desc, \p *extent and \p *flags respectively, the type, shape
 * and flags of \p array.
 *
 * Any of \p *desc, \p *extent and \p *flags may be specified as NULL.
 *
 * \param desc   - Returned array type
 * \param extent - Returned array shape. 2D arrays will have depth of zero
 * \param flags  - Returned array flags
 * \param array  - The ::cudaArray to get info for
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue
 * \notefnerr
 *
 */
    pub fn cudaArrayGetInfo(desc: *mut cudaChannelFormatDesc,
                            extent: *mut cudaExtent,
                            flags: *mut ::libc::c_uint,
                            array: cudaArray_t) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Copies data between host and device
 *
 * Copies \p count bytes from the memory area pointed to by \p src to the
 * memory area pointed to by \p dst, where \p kind specifies the direction
 * of the copy, and must be one of ::cudaMemcpyHostToHost,
 * ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
 * ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
 * ::cudaMemcpyDefault is recommended, in which case the type of transfer is
 * inferred from the pointer values. However, ::cudaMemcpyDefault is only
 * allowed on systems that support unified virtual addressing. Calling
 * ::cudaMemcpy() with dst and src pointers that do not match the direction of
 * the copy results in an undefined behavior.
 *
 * \param dst   - Destination memory address
 * \param src   - Source memory address
 * \param count - Size in bytes to copy
 * \param kind  - Type of transfer
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidDevicePointer,
 * ::cudaErrorInvalidMemcpyDirection
 * \notefnerr
 *
 * \note_sync
 *
 * \sa ::cudaMemcpy2D, ::cudaMemcpyToArray,
 * ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
 * ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
 * ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
 * ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
 * ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
 * ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync
 */
    pub fn cudaMemcpy(dst: *mut ::libc::c_void,
                      src: *const ::libc::c_void, count: usize,
                      kind: cudaMemcpyKind) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Copies memory between two devices
 *
 * Copies memory from one device to memory on another device.  \p dst is the
 * base device pointer of the destination memory and \p dstDevice is the
 * destination device.  \p src is the base device pointer of the source memory
 * and \p srcDevice is the source device.  \p count specifies the number of bytes
 * to copy.
 *
 * Note that this function is asynchronous with respect to the host, but
 * serialized with respect all pending and future asynchronous work in to the
 * current device, \p srcDevice, and \p dstDevice (use ::cudaMemcpyPeerAsync
 * to avoid this synchronization).
 *
 * \param dst       - Destination device pointer
 * \param dstDevice - Destination device
 * \param src       - Source device pointer
 * \param srcDevice - Source device
 * \param count     - Size of memory copy in bytes
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidDevice
 * \notefnerr
 * \note_sync
 *
 * \sa ::cudaMemcpy, ::cudaMemcpyAsync, ::cudaMemcpyPeerAsync,
 * ::cudaMemcpy3DPeerAsync
 */
    pub fn cudaMemcpyPeer(dst: *mut ::libc::c_void,
                          dstDevice: ::libc::c_int,
                          src: *const ::libc::c_void,
                          srcDevice: ::libc::c_int, count: usize)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Copies data between host and device
 *
 * Copies \p count bytes from the memory area pointed to by \p src to the
 * CUDA array \p dst starting at the upper left corner
 * (\p wOffset, \p hOffset), where \p kind specifies the direction
 * of the copy, and must be one of ::cudaMemcpyHostToHost,
 * ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
 * ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
 * ::cudaMemcpyDefault is recommended, in which case the type of transfer is
 * inferred from the pointer values. However, ::cudaMemcpyDefault is only
 * allowed on systems that support unified virtual addressing.
 *
 * \param dst     - Destination memory address
 * \param wOffset - Destination starting X offset
 * \param hOffset - Destination starting Y offset
 * \param src     - Source memory address
 * \param count   - Size in bytes to copy
 * \param kind    - Type of transfer
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidDevicePointer,
 * ::cudaErrorInvalidMemcpyDirection
 * \notefnerr
 * \note_sync
 *
 * \sa ::cudaMemcpy, ::cudaMemcpy2D,
 * ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
 * ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
 * ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
 * ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
 * ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
 * ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync
 */
    pub fn cudaMemcpyToArray(dst: cudaArray_t, wOffset: usize, hOffset: usize,
                             src: *const ::libc::c_void, count: usize,
                             kind: cudaMemcpyKind) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Copies data between host and device
 *
 * Copies \p count bytes from the CUDA array \p src starting at the upper
 * left corner (\p wOffset, hOffset) to the memory area pointed to by \p dst,
 * where \p kind specifies the direction of the copy, and must be one of
 * ::cudaMemcpyHostToHost, ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
 * ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
 * ::cudaMemcpyDefault is recommended, in which case the type of transfer is
 * inferred from the pointer values. However, ::cudaMemcpyDefault is only
 * allowed on systems that support unified virtual addressing.
 *
 * \param dst     - Destination memory address
 * \param src     - Source memory address
 * \param wOffset - Source starting X offset
 * \param hOffset - Source starting Y offset
 * \param count   - Size in bytes to copy
 * \param kind    - Type of transfer
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidDevicePointer,
 * ::cudaErrorInvalidMemcpyDirection
 * \notefnerr
 * \note_sync
 *
 * \sa ::cudaMemcpy, ::cudaMemcpy2D, ::cudaMemcpyToArray,
 * ::cudaMemcpy2DToArray, ::cudaMemcpy2DFromArray,
 * ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
 * ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
 * ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
 * ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
 * ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync
 */
    pub fn cudaMemcpyFromArray(dst: *mut ::libc::c_void,
                               src: cudaArray_const_t, wOffset: usize,
                               hOffset: usize, count: usize,
                               kind: cudaMemcpyKind) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Copies data between host and device
 *
 * Copies \p count bytes from the CUDA array \p src starting at the upper
 * left corner (\p wOffsetSrc, \p hOffsetSrc) to the CUDA array \p dst
 * starting at the upper left corner (\p wOffsetDst, \p hOffsetDst) where
 * \p kind specifies the direction of the copy, and must be one of
 * ::cudaMemcpyHostToHost, ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
 * ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
 * ::cudaMemcpyDefault is recommended, in which case the type of transfer is
 * inferred from the pointer values. However, ::cudaMemcpyDefault is only
 * allowed on systems that support unified virtual addressing.
 *
 * \param dst        - Destination memory address
 * \param wOffsetDst - Destination starting X offset
 * \param hOffsetDst - Destination starting Y offset
 * \param src        - Source memory address
 * \param wOffsetSrc - Source starting X offset
 * \param hOffsetSrc - Source starting Y offset
 * \param count      - Size in bytes to copy
 * \param kind       - Type of transfer
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidMemcpyDirection
 * \notefnerr
 *
 * \sa ::cudaMemcpy, ::cudaMemcpy2D, ::cudaMemcpyToArray,
 * ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
 * ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
 * ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
 * ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
 * ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
 * ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync
 */
    pub fn cudaMemcpyArrayToArray(dst: cudaArray_t, wOffsetDst: usize,
                                  hOffsetDst: usize, src: cudaArray_const_t,
                                  wOffsetSrc: usize, hOffsetSrc: usize,
                                  count: usize, kind: cudaMemcpyKind)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Copies data between host and device
 *
 * Copies a matrix (\p height rows of \p width bytes each) from the memory
 * area pointed to by \p src to the memory area pointed to by \p dst, where
 * \p kind specifies the direction of the copy, and must be one of
 * ::cudaMemcpyHostToHost, ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
 * ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
 * ::cudaMemcpyDefault is recommended, in which case the type of transfer is
 * inferred from the pointer values. However, ::cudaMemcpyDefault is only
 * allowed on systems that support unified virtual addressing. \p dpitch and
 * \p spitch are the widths in memory in bytes of the 2D arrays pointed to by
 * \p dst and \p src, including any padding added to the end of each row. The
 * memory areas may not overlap. \p width must not exceed either \p dpitch or
 * \p spitch. Calling ::cudaMemcpy2D() with \p dst and \p src pointers that do
 * not match the direction of the copy results in an undefined behavior.
 * ::cudaMemcpy2D() returns an error if \p dpitch or \p spitch exceeds
 * the maximum allowed.
 *
 * \param dst    - Destination memory address
 * \param dpitch - Pitch of destination memory
 * \param src    - Source memory address
 * \param spitch - Pitch of source memory
 * \param width  - Width of matrix transfer (columns in bytes)
 * \param height - Height of matrix transfer (rows)
 * \param kind   - Type of transfer
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidPitchValue,
 * ::cudaErrorInvalidDevicePointer,
 * ::cudaErrorInvalidMemcpyDirection
 * \notefnerr
 *
 * \sa ::cudaMemcpy, ::cudaMemcpyToArray,
 * ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
 * ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
 * ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
 * ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
 * ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
 * ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync
 */
    pub fn cudaMemcpy2D(dst: *mut ::libc::c_void, dpitch: usize,
                        src: *const ::libc::c_void, spitch: usize,
                        width: usize, height: usize, kind: cudaMemcpyKind)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Copies data between host and device
 *
 * Copies a matrix (\p height rows of \p width bytes each) from the memory
 * area pointed to by \p src to the CUDA array \p dst starting at the
 * upper left corner (\p wOffset, \p hOffset) where \p kind specifies the
 * direction of the copy, and must be one of ::cudaMemcpyHostToHost,
 * ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
 * ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
 * ::cudaMemcpyDefault is recommended, in which case the type of transfer is
 * inferred from the pointer values. However, ::cudaMemcpyDefault is only
 * allowed on systems that support unified virtual addressing.
 * \p spitch is the width in memory in bytes of the 2D array pointed to by
 * \p src, including any padding added to the end of each row. \p wOffset +
 * \p width must not exceed the width of the CUDA array \p dst. \p width must
 * not exceed \p spitch. ::cudaMemcpy2DToArray() returns an error if \p spitch
 * exceeds the maximum allowed.
 *
 * \param dst     - Destination memory address
 * \param wOffset - Destination starting X offset
 * \param hOffset - Destination starting Y offset
 * \param src     - Source memory address
 * \param spitch  - Pitch of source memory
 * \param width   - Width of matrix transfer (columns in bytes)
 * \param height  - Height of matrix transfer (rows)
 * \param kind    - Type of transfer
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidDevicePointer,
 * ::cudaErrorInvalidPitchValue,
 * ::cudaErrorInvalidMemcpyDirection
 * \notefnerr
 * \note_sync
 *
 * \sa ::cudaMemcpy, ::cudaMemcpy2D, ::cudaMemcpyToArray,
 * ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
 * ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
 * ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
 * ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
 * ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
 * ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync
 */
    pub fn cudaMemcpy2DToArray(dst: cudaArray_t, wOffset: usize,
                               hOffset: usize,
                               src: *const ::libc::c_void,
                               spitch: usize, width: usize, height: usize,
                               kind: cudaMemcpyKind) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Copies data between host and device
 *
 * Copies a matrix (\p height rows of \p width bytes each) from the CUDA
 * array \p srcArray starting at the upper left corner
 * (\p wOffset, \p hOffset) to the memory area pointed to by \p dst, where
 * \p kind specifies the direction of the copy, and must be one of
 * ::cudaMemcpyHostToHost, ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
 * ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
 * ::cudaMemcpyDefault is recommended, in which case the type of transfer is
 * inferred from the pointer values. However, ::cudaMemcpyDefault is only
 * allowed on systems that support unified virtual addressing. \p dpitch is the
 * width in memory in bytes of the 2D array pointed to by \p dst, including any
 * padding added to the end of each row. \p wOffset + \p width must not exceed
 * the width of the CUDA array \p src. \p width must not exceed \p dpitch.
 * ::cudaMemcpy2DFromArray() returns an error if \p dpitch exceeds the maximum
 * allowed.
 *
 * \param dst     - Destination memory address
 * \param dpitch  - Pitch of destination memory
 * \param src     - Source memory address
 * \param wOffset - Source starting X offset
 * \param hOffset - Source starting Y offset
 * \param width   - Width of matrix transfer (columns in bytes)
 * \param height  - Height of matrix transfer (rows)
 * \param kind    - Type of transfer
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidDevicePointer,
 * ::cudaErrorInvalidPitchValue,
 * ::cudaErrorInvalidMemcpyDirection
 * \notefnerr
 * \note_sync
 *
 * \sa ::cudaMemcpy, ::cudaMemcpy2D, ::cudaMemcpyToArray,
 * ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray,
 * ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
 * ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
 * ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
 * ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
 * ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync
 */
    pub fn cudaMemcpy2DFromArray(dst: *mut ::libc::c_void,
                                 dpitch: usize, src: cudaArray_const_t,
                                 wOffset: usize, hOffset: usize, width: usize,
                                 height: usize, kind: cudaMemcpyKind)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Copies data between host and device
 *
 * Copies a matrix (\p height rows of \p width bytes each) from the CUDA
 * array \p srcArray starting at the upper left corner
 * (\p wOffsetSrc, \p hOffsetSrc) to the CUDA array \p dst starting at
 * the upper left corner (\p wOffsetDst, \p hOffsetDst), where \p kind
 * specifies the direction of the copy, and must be one of
 * ::cudaMemcpyHostToHost, ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
 * ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
 * ::cudaMemcpyDefault is recommended, in which case the type of transfer is
 * inferred from the pointer values. However, ::cudaMemcpyDefault is only
 * allowed on systems that support unified virtual addressing.
 * \p wOffsetDst + \p width must not exceed the width of the CUDA array \p dst.
 * \p wOffsetSrc + \p width must not exceed the width of the CUDA array \p src.
 *
 * \param dst        - Destination memory address
 * \param wOffsetDst - Destination starting X offset
 * \param hOffsetDst - Destination starting Y offset
 * \param src        - Source memory address
 * \param wOffsetSrc - Source starting X offset
 * \param hOffsetSrc - Source starting Y offset
 * \param width      - Width of matrix transfer (columns in bytes)
 * \param height     - Height of matrix transfer (rows)
 * \param kind       - Type of transfer
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidMemcpyDirection
 * \notefnerr
 * \note_sync
 *
 * \sa ::cudaMemcpy, ::cudaMemcpy2D, ::cudaMemcpyToArray,
 * ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
 * ::cudaMemcpyArrayToArray, ::cudaMemcpyToSymbol,
 * ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
 * ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
 * ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
 * ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync
 */
    pub fn cudaMemcpy2DArrayToArray(dst: cudaArray_t, wOffsetDst: usize,
                                    hOffsetDst: usize, src: cudaArray_const_t,
                                    wOffsetSrc: usize, hOffsetSrc: usize,
                                    width: usize, height: usize,
                                    kind: cudaMemcpyKind) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Copies data to the given symbol on the device
 *
 * Copies \p count bytes from the memory area pointed to by \p src
 * to the memory area pointed to by \p offset bytes from the start of symbol
 * \p symbol. The memory areas may not overlap. \p symbol is a variable that
 * resides in global or constant memory space. \p kind can be either
 * ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault.
 * Passing ::cudaMemcpyDefault is recommended, in which case the type of
 * transfer is inferred from the pointer values. However, ::cudaMemcpyDefault
 * is only allowed on systems that support unified virtual addressing.
 *
 * \param symbol - Device symbol address
 * \param src    - Source memory address
 * \param count  - Size in bytes to copy
 * \param offset - Offset from start of symbol in bytes
 * \param kind   - Type of transfer
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidSymbol,
 * ::cudaErrorInvalidDevicePointer,
 * ::cudaErrorInvalidMemcpyDirection
 * \notefnerr
 * \note_sync
 * \note_string_api_deprecation
 *
 * \sa ::cudaMemcpy, ::cudaMemcpy2D, ::cudaMemcpyToArray,
 * ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
 * ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray,
 * ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
 * ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
 * ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
 * ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync
 */
    pub fn cudaMemcpyToSymbol(symbol: *const ::libc::c_void,
                              src: *const ::libc::c_void,
                              count: usize, offset: usize,
                              kind: cudaMemcpyKind) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Copies data from the given symbol on the device
 *
 * Copies \p count bytes from the memory area pointed to by \p offset bytes
 * from the start of symbol \p symbol to the memory area pointed to by \p dst.
 * The memory areas may not overlap. \p symbol is a variable that
 * resides in global or constant memory space. \p kind can be either
 * ::cudaMemcpyDeviceToHost, ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault.
 * Passing ::cudaMemcpyDefault is recommended, in which case the type of
 * transfer is inferred from the pointer values. However, ::cudaMemcpyDefault
 * is only allowed on systems that support unified virtual addressing.
 *
 * \param dst    - Destination memory address
 * \param symbol - Device symbol address
 * \param count  - Size in bytes to copy
 * \param offset - Offset from start of symbol in bytes
 * \param kind   - Type of transfer
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidSymbol,
 * ::cudaErrorInvalidDevicePointer,
 * ::cudaErrorInvalidMemcpyDirection
 * \notefnerr
 * \note_sync
 * \note_string_api_deprecation
 *
 * \sa ::cudaMemcpy, ::cudaMemcpy2D, ::cudaMemcpyToArray,
 * ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
 * ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
 * ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
 * ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
 * ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
 * ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync
 */
    pub fn cudaMemcpyFromSymbol(dst: *mut ::libc::c_void,
                                symbol: *const ::libc::c_void,
                                count: usize, offset: usize,
                                kind: cudaMemcpyKind) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Copies data between host and device
 *
 * Copies \p count bytes from the memory area pointed to by \p src to the
 * memory area pointed to by \p dst, where \p kind specifies the
 * direction of the copy, and must be one of ::cudaMemcpyHostToHost,
 * ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
 * ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
 * ::cudaMemcpyDefault is recommended, in which case the type of transfer is
 * inferred from the pointer values. However, ::cudaMemcpyDefault is only
 * allowed on systems that support unified virtual addressing.
 *
 * The memory areas may not overlap. Calling ::cudaMemcpyAsync() with \p dst and
 * \p src pointers that do not match the direction of the copy results in an
 * undefined behavior.
 *
 * ::cudaMemcpyAsync() is asynchronous with respect to the host, so the call
 * may return before the copy is complete. The copy can optionally be
 * associated to a stream by passing a non-zero \p stream argument. If \p kind
 * is ::cudaMemcpyHostToDevice or ::cudaMemcpyDeviceToHost and the \p stream is
 * non-zero, the copy may overlap with operations in other streams.
 *
 * The device version of this function only handles device to device copies and
 * cannot be given local or shared pointers.
 *
 * \param dst    - Destination memory address
 * \param src    - Source memory address
 * \param count  - Size in bytes to copy
 * \param kind   - Type of transfer
 * \param stream - Stream identifier
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidDevicePointer,
 * ::cudaErrorInvalidMemcpyDirection
 * \notefnerr
 * \note_async
 * \note_null_stream
 *
 * \sa ::cudaMemcpy, ::cudaMemcpy2D, ::cudaMemcpyToArray,
 * ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
 * ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
 * ::cudaMemcpyFromSymbol, ::cudaMemcpy2DAsync,
 * ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
 * ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
 * ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync
 */
    pub fn cudaMemcpyAsync(dst: *mut ::libc::c_void,
                           src: *const ::libc::c_void, count: usize,
                           kind: cudaMemcpyKind, stream: cudaStream_t)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Copies memory between two devices asynchronously.
 *
 * Copies memory from one device to memory on another device.  \p dst is the
 * base device pointer of the destination memory and \p dstDevice is the
 * destination device.  \p src is the base device pointer of the source memory
 * and \p srcDevice is the source device.  \p count specifies the number of bytes
 * to copy.
 *
 * Note that this function is asynchronous with respect to the host and all work
 * on other devices.
 *
 * \param dst       - Destination device pointer
 * \param dstDevice - Destination device
 * \param src       - Source device pointer
 * \param srcDevice - Source device
 * \param count     - Size of memory copy in bytes
 * \param stream    - Stream identifier
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidDevice
 * \notefnerr
 * \note_async
 * \note_null_stream
 *
 * \sa ::cudaMemcpy, ::cudaMemcpyPeer, ::cudaMemcpyAsync,
 * ::cudaMemcpy3DPeerAsync
 */
    pub fn cudaMemcpyPeerAsync(dst: *mut ::libc::c_void,
                               dstDevice: ::libc::c_int,
                               src: *const ::libc::c_void,
                               srcDevice: ::libc::c_int, count: usize,
                               stream: cudaStream_t) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Copies data between host and device
 *
 * Copies \p count bytes from the memory area pointed to by \p src to the
 * CUDA array \p dst starting at the upper left corner
 * (\p wOffset, \p hOffset), where \p kind specifies the
 * direction of the copy, and must be one of ::cudaMemcpyHostToHost,
 * ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
 * ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
 * ::cudaMemcpyDefault is recommended, in which case the type of transfer is
 * inferred from the pointer values. However, ::cudaMemcpyDefault is only
 * allowed on systems that support unified virtual addressing.
 *
 * ::cudaMemcpyToArrayAsync() is asynchronous with respect to the host, so
 * the call may return before the copy is complete. The copy can optionally
 * be associated to a stream by passing a non-zero \p stream argument. If \p
 * kind is ::cudaMemcpyHostToDevice or ::cudaMemcpyDeviceToHost and \p stream
 * is non-zero, the copy may overlap with operations in other streams.
 *
 * \param dst     - Destination memory address
 * \param wOffset - Destination starting X offset
 * \param hOffset - Destination starting Y offset
 * \param src     - Source memory address
 * \param count   - Size in bytes to copy
 * \param kind    - Type of transfer
 * \param stream  - Stream identifier
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidDevicePointer,
 * ::cudaErrorInvalidMemcpyDirection
 * \notefnerr
 * \note_async
 * \note_null_stream
 *
 * \sa ::cudaMemcpy, ::cudaMemcpy2D, ::cudaMemcpyToArray,
 * ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
 * ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
 * ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
 * ::cudaMemcpy2DToArrayAsync,
 * ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
 * ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync
 */
    pub fn cudaMemcpyToArrayAsync(dst: cudaArray_t, wOffset: usize,
                                  hOffset: usize,
                                  src: *const ::libc::c_void,
                                  count: usize, kind: cudaMemcpyKind,
                                  stream: cudaStream_t) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Copies data between host and device
 *
 * Copies \p count bytes from the CUDA array \p src starting at the upper
 * left corner (\p wOffset, hOffset) to the memory area pointed to by \p dst,
 * where \p kind specifies the direction of the copy, and must be one of
 * ::cudaMemcpyHostToHost, ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
 * ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
 * ::cudaMemcpyDefault is recommended, in which case the type of transfer is
 * inferred from the pointer values. However, ::cudaMemcpyDefault is only
 * allowed on systems that support unified virtual addressing.
 *
 * ::cudaMemcpyFromArrayAsync() is asynchronous with respect to the host, so
 * the call may return before the copy is complete. The copy can optionally
 * be associated to a stream by passing a non-zero \p stream argument. If \p
 * kind is ::cudaMemcpyHostToDevice or ::cudaMemcpyDeviceToHost and \p stream
 * is non-zero, the copy may overlap with operations in other streams.
 *
 * \param dst     - Destination memory address
 * \param src     - Source memory address
 * \param wOffset - Source starting X offset
 * \param hOffset - Source starting Y offset
 * \param count   - Size in bytes to copy
 * \param kind    - Type of transfer
 * \param stream  - Stream identifier
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidDevicePointer,
 * ::cudaErrorInvalidMemcpyDirection
 * \notefnerr
 * \note_async
 * \note_null_stream
 *
 * \sa ::cudaMemcpy, ::cudaMemcpy2D, ::cudaMemcpyToArray,
 * ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
 * ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
 * ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
 * ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
 * ::cudaMemcpy2DFromArrayAsync,
 * ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync
 */
    pub fn cudaMemcpyFromArrayAsync(dst: *mut ::libc::c_void,
                                    src: cudaArray_const_t, wOffset: usize,
                                    hOffset: usize, count: usize,
                                    kind: cudaMemcpyKind,
                                    stream: cudaStream_t) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Copies data between host and device
 *
 * Copies a matrix (\p height rows of \p width bytes each) from the memory
 * area pointed to by \p src to the memory area pointed to by \p dst, where
 * \p kind specifies the direction of the copy, and must be one of
 * ::cudaMemcpyHostToHost, ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
 * ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
 * ::cudaMemcpyDefault is recommended, in which case the type of transfer is
 * inferred from the pointer values. However, ::cudaMemcpyDefault is only
 * allowed on systems that support unified virtual addressing.
 * \p dpitch and \p spitch are the widths in memory in bytes of the 2D arrays
 * pointed to by \p dst and \p src, including any padding added to the end of
 * each row. The memory areas may not overlap. \p width must not exceed either
 * \p dpitch or \p spitch.
 *
 * Calling ::cudaMemcpy2DAsync() with \p dst and \p src pointers that do not
 * match the direction of the copy results in an undefined behavior.
 * ::cudaMemcpy2DAsync() returns an error if \p dpitch or \p spitch is greater
 * than the maximum allowed.
 *
 * ::cudaMemcpy2DAsync() is asynchronous with respect to the host, so
 * the call may return before the copy is complete. The copy can optionally
 * be associated to a stream by passing a non-zero \p stream argument. If
 * \p kind is ::cudaMemcpyHostToDevice or ::cudaMemcpyDeviceToHost and
 * \p stream is non-zero, the copy may overlap with operations in other
 * streams.
 *
 * The device version of this function only handles device to device copies and
 * cannot be given local or shared pointers.
 *
 * \param dst    - Destination memory address
 * \param dpitch - Pitch of destination memory
 * \param src    - Source memory address
 * \param spitch - Pitch of source memory
 * \param width  - Width of matrix transfer (columns in bytes)
 * \param height - Height of matrix transfer (rows)
 * \param kind   - Type of transfer
 * \param stream - Stream identifier
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidPitchValue,
 * ::cudaErrorInvalidDevicePointer,
 * ::cudaErrorInvalidMemcpyDirection
 * \notefnerr
 * \note_async
 * \note_null_stream
 *
 * \sa ::cudaMemcpy, ::cudaMemcpy2D, ::cudaMemcpyToArray,
 * ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
 * ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
 * ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync,
 * ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
 * ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
 * ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync
 */
    pub fn cudaMemcpy2DAsync(dst: *mut ::libc::c_void, dpitch: usize,
                             src: *const ::libc::c_void,
                             spitch: usize, width: usize, height: usize,
                             kind: cudaMemcpyKind, stream: cudaStream_t)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Copies data between host and device
 *
 * Copies a matrix (\p height rows of \p width bytes each) from the memory
 * area pointed to by \p src to the CUDA array \p dst starting at the
 * upper left corner (\p wOffset, \p hOffset) where \p kind specifies the
 * direction of the copy, and must be one of ::cudaMemcpyHostToHost,
 * ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
 * ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
 * ::cudaMemcpyDefault is recommended, in which case the type of transfer is
 * inferred from the pointer values. However, ::cudaMemcpyDefault is only
 * allowed on systems that support unified virtual addressing.
 * \p spitch is the width in memory in bytes of the 2D array pointed to by
 * \p src, including any padding added to the end of each row. \p wOffset +
 * \p width must not exceed the width of the CUDA array \p dst. \p width must
 * not exceed \p spitch. ::cudaMemcpy2DToArrayAsync() returns an error if
 * \p spitch exceeds the maximum allowed.
 *
 * ::cudaMemcpy2DToArrayAsync() is asynchronous with respect to the host, so
 * the call may return before the copy is complete. The copy can optionally
 * be associated to a stream by passing a non-zero \p stream argument. If
 * \p kind is ::cudaMemcpyHostToDevice or ::cudaMemcpyDeviceToHost and
 * \p stream is non-zero, the copy may overlap with operations in other
 * streams.
 *
 * \param dst     - Destination memory address
 * \param wOffset - Destination starting X offset
 * \param hOffset - Destination starting Y offset
 * \param src     - Source memory address
 * \param spitch  - Pitch of source memory
 * \param width   - Width of matrix transfer (columns in bytes)
 * \param height  - Height of matrix transfer (rows)
 * \param kind    - Type of transfer
 * \param stream  - Stream identifier
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidDevicePointer,
 * ::cudaErrorInvalidPitchValue,
 * ::cudaErrorInvalidMemcpyDirection
 * \notefnerr
 * \note_async
 * \note_null_stream
 *
 * \sa ::cudaMemcpy, ::cudaMemcpy2D, ::cudaMemcpyToArray,
 * ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
 * ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
 * ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
 * ::cudaMemcpyToArrayAsync,
 * ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
 * ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync
 */
    pub fn cudaMemcpy2DToArrayAsync(dst: cudaArray_t, wOffset: usize,
                                    hOffset: usize,
                                    src: *const ::libc::c_void,
                                    spitch: usize, width: usize,
                                    height: usize, kind: cudaMemcpyKind,
                                    stream: cudaStream_t) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Copies data between host and device
 *
 * Copies a matrix (\p height rows of \p width bytes each) from the CUDA
 * array \p srcArray starting at the upper left corner
 * (\p wOffset, \p hOffset) to the memory area pointed to by \p dst, where
 * \p kind specifies the direction of the copy, and must be one of
 * ::cudaMemcpyHostToHost, ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
 * ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
 * ::cudaMemcpyDefault is recommended, in which case the type of transfer is
 * inferred from the pointer values. However, ::cudaMemcpyDefault is only
 * allowed on systems that support unified virtual addressing.
 * \p dpitch is the width in memory in bytes of the 2D
 * array pointed to by \p dst, including any padding added to the end of each
 * row. \p wOffset + \p width must not exceed the width of the CUDA array
 * \p src. \p width must not exceed \p dpitch. ::cudaMemcpy2DFromArrayAsync()
 * returns an error if \p dpitch exceeds the maximum allowed.
 *
 * ::cudaMemcpy2DFromArrayAsync() is asynchronous with respect to the host, so
 * the call may return before the copy is complete. The copy can optionally be
 * associated to a stream by passing a non-zero \p stream argument. If \p kind
 * is ::cudaMemcpyHostToDevice or ::cudaMemcpyDeviceToHost and \p stream is
 * non-zero, the copy may overlap with operations in other streams.
 *
 * \param dst     - Destination memory address
 * \param dpitch  - Pitch of destination memory
 * \param src     - Source memory address
 * \param wOffset - Source starting X offset
 * \param hOffset - Source starting Y offset
 * \param width   - Width of matrix transfer (columns in bytes)
 * \param height  - Height of matrix transfer (rows)
 * \param kind    - Type of transfer
 * \param stream  - Stream identifier
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidDevicePointer,
 * ::cudaErrorInvalidPitchValue,
 * ::cudaErrorInvalidMemcpyDirection
 * \notefnerr
 * \note_async
 * \note_null_stream
 *
 * \sa ::cudaMemcpy, ::cudaMemcpy2D, ::cudaMemcpyToArray,
 * ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
 * ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
 * ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
 * ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
 * ::cudaMemcpyFromArrayAsync,
 * ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync
 */
    pub fn cudaMemcpy2DFromArrayAsync(dst: *mut ::libc::c_void,
                                      dpitch: usize, src: cudaArray_const_t,
                                      wOffset: usize, hOffset: usize,
                                      width: usize, height: usize,
                                      kind: cudaMemcpyKind,
                                      stream: cudaStream_t) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Copies data to the given symbol on the device
 *
 * Copies \p count bytes from the memory area pointed to by \p src
 * to the memory area pointed to by \p offset bytes from the start of symbol
 * \p symbol. The memory areas may not overlap. \p symbol is a variable that
 * resides in global or constant memory space. \p kind can be either
 * ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault.
 * Passing ::cudaMemcpyDefault is recommended, in which case the type of transfer
 * is inferred from the pointer values. However, ::cudaMemcpyDefault is only
 * allowed on systems that support unified virtual addressing.
 *
 * ::cudaMemcpyToSymbolAsync() is asynchronous with respect to the host, so
 * the call may return before the copy is complete. The copy can optionally
 * be associated to a stream by passing a non-zero \p stream argument. If
 * \p kind is ::cudaMemcpyHostToDevice and \p stream is non-zero, the copy
 * may overlap with operations in other streams.
 *
 * \param symbol - Device symbol address
 * \param src    - Source memory address
 * \param count  - Size in bytes to copy
 * \param offset - Offset from start of symbol in bytes
 * \param kind   - Type of transfer
 * \param stream - Stream identifier
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidSymbol,
 * ::cudaErrorInvalidDevicePointer,
 * ::cudaErrorInvalidMemcpyDirection
 * \notefnerr
 * \note_async
 * \note_null_stream
 * \note_string_api_deprecation
 *
 * \sa ::cudaMemcpy, ::cudaMemcpy2D, ::cudaMemcpyToArray,
 * ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
 * ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
 * ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
 * ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
 * ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
 * ::cudaMemcpyFromSymbolAsync
 */
    pub fn cudaMemcpyToSymbolAsync(symbol: *const ::libc::c_void,
                                   src: *const ::libc::c_void,
                                   count: usize, offset: usize,
                                   kind: cudaMemcpyKind, stream: cudaStream_t)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Copies data from the given symbol on the device
 *
 * Copies \p count bytes from the memory area pointed to by \p offset bytes
 * from the start of symbol \p symbol to the memory area pointed to by \p dst.
 * The memory areas may not overlap. \p symbol is a variable that resides in
 * global or constant memory space. \p kind can be either
 * ::cudaMemcpyDeviceToHost, ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault.
 * Passing ::cudaMemcpyDefault is recommended, in which case the type of transfer
 * is inferred from the pointer values. However, ::cudaMemcpyDefault is only
 * allowed on systems that support unified virtual addressing.
 *
 * ::cudaMemcpyFromSymbolAsync() is asynchronous with respect to the host, so
 * the call may return before the copy is complete. The copy can optionally be
 * associated to a stream by passing a non-zero \p stream argument. If \p kind
 * is ::cudaMemcpyDeviceToHost and \p stream is non-zero, the copy may overlap
 * with operations in other streams.
 *
 * \param dst    - Destination memory address
 * \param symbol - Device symbol address
 * \param count  - Size in bytes to copy
 * \param offset - Offset from start of symbol in bytes
 * \param kind   - Type of transfer
 * \param stream - Stream identifier
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidSymbol,
 * ::cudaErrorInvalidDevicePointer,
 * ::cudaErrorInvalidMemcpyDirection
 * \notefnerr
 * \note_async
 * \note_null_stream
 * \note_string_api_deprecation
 *
 * \sa ::cudaMemcpy, ::cudaMemcpy2D, ::cudaMemcpyToArray,
 * ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
 * ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
 * ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
 * ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
 * ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
 * ::cudaMemcpyToSymbolAsync
 */
    pub fn cudaMemcpyFromSymbolAsync(dst: *mut ::libc::c_void,
                                     symbol: *const ::libc::c_void,
                                     count: usize, offset: usize,
                                     kind: cudaMemcpyKind,
                                     stream: cudaStream_t) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Initializes or sets device memory to a value
 *
 * Fills the first \p count bytes of the memory area pointed to by \p devPtr
 * with the constant byte value \p value.
 *
 * Note that this function is asynchronous with respect to the host unless
 * \p devPtr refers to pinned host memory.
 *
 * \param devPtr - Pointer to device memory
 * \param value  - Value to set for each byte of specified memory
 * \param count  - Size in bytes to set
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidDevicePointer
 * \notefnerr
 * \note_memset
 *
 * \sa ::cudaMemset2D, ::cudaMemset3D, ::cudaMemsetAsync,
 * ::cudaMemset2DAsync, ::cudaMemset3DAsync
 */
    pub fn cudaMemset(devPtr: *mut ::libc::c_void,
                      value: ::libc::c_int, count: usize)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Initializes or sets device memory to a value
 *
 * Sets to the specified value \p value a matrix (\p height rows of \p width
 * bytes each) pointed to by \p dstPtr. \p pitch is the width in bytes of the
 * 2D array pointed to by \p dstPtr, including any padding added to the end
 * of each row. This function performs fastest when the pitch is one that has
 * been passed back by ::cudaMallocPitch().
 *
 * Note that this function is asynchronous with respect to the host unless
 * \p devPtr refers to pinned host memory.
 *
 * \param devPtr - Pointer to 2D device memory
 * \param pitch  - Pitch in bytes of 2D device memory
 * \param value  - Value to set for each byte of specified memory
 * \param width  - Width of matrix set (columns in bytes)
 * \param height - Height of matrix set (rows)
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidDevicePointer
 * \notefnerr
 * \note_memset
 *
 * \sa ::cudaMemset, ::cudaMemset3D, ::cudaMemsetAsync,
 * ::cudaMemset2DAsync, ::cudaMemset3DAsync
 */
    pub fn cudaMemset2D(devPtr: *mut ::libc::c_void, pitch: usize,
                        value: ::libc::c_int, width: usize,
                        height: usize) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Initializes or sets device memory to a value
 *
 * Initializes each element of a 3D array to the specified value \p value.
 * The object to initialize is defined by \p pitchedDevPtr. The \p pitch field
 * of \p pitchedDevPtr is the width in memory in bytes of the 3D array pointed
 * to by \p pitchedDevPtr, including any padding added to the end of each row.
 * The \p xsize field specifies the logical width of each row in bytes, while
 * the \p ysize field specifies the height of each 2D slice in rows.
 *
 * The extents of the initialized region are specified as a \p width in bytes,
 * a \p height in rows, and a \p depth in slices.
 *
 * Extents with \p width greater than or equal to the \p xsize of
 * \p pitchedDevPtr may perform significantly faster than extents narrower
 * than the \p xsize. Secondarily, extents with \p height equal to the
 * \p ysize of \p pitchedDevPtr will perform faster than when the \p height is
 * shorter than the \p ysize.
 *
 * This function performs fastest when the \p pitchedDevPtr has been allocated
 * by ::cudaMalloc3D().
 *
 * Note that this function is asynchronous with respect to the host unless
 * \p pitchedDevPtr refers to pinned host memory.
 *
 * \param pitchedDevPtr - Pointer to pitched device memory
 * \param value         - Value to set for each byte of specified memory
 * \param extent        - Size parameters for where to set device memory (\p width field in bytes)
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidDevicePointer
 * \notefnerr
 * \note_memset
 *
 * \sa ::cudaMemset, ::cudaMemset2D,
 * ::cudaMemsetAsync, ::cudaMemset2DAsync, ::cudaMemset3DAsync,
 * ::cudaMalloc3D, ::make_cudaPitchedPtr,
 * ::make_cudaExtent
 */
    pub fn cudaMemset3D(pitchedDevPtr: cudaPitchedPtr,
                        value: ::libc::c_int, extent: cudaExtent)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Initializes or sets device memory to a value
 *
 * Fills the first \p count bytes of the memory area pointed to by \p devPtr
 * with the constant byte value \p value.
 *
 * ::cudaMemsetAsync() is asynchronous with respect to the host, so
 * the call may return before the memset is complete. The operation can optionally
 * be associated to a stream by passing a non-zero \p stream argument.
 * If \p stream is non-zero, the operation may overlap with operations in other streams.
 *
 * The device version of this function only handles device to device copies and
 * cannot be given local or shared pointers.
 *
 * \param devPtr - Pointer to device memory
 * \param value  - Value to set for each byte of specified memory
 * \param count  - Size in bytes to set
 * \param stream - Stream identifier
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidDevicePointer
 * \notefnerr
 * \note_memset
 * \note_null_stream
 *
 * \sa ::cudaMemset, ::cudaMemset2D, ::cudaMemset3D,
 * ::cudaMemset2DAsync, ::cudaMemset3DAsync
 */
    pub fn cudaMemsetAsync(devPtr: *mut ::libc::c_void,
                           value: ::libc::c_int, count: usize,
                           stream: cudaStream_t) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Initializes or sets device memory to a value
 *
 * Sets to the specified value \p value a matrix (\p height rows of \p width
 * bytes each) pointed to by \p dstPtr. \p pitch is the width in bytes of the
 * 2D array pointed to by \p dstPtr, including any padding added to the end
 * of each row. This function performs fastest when the pitch is one that has
 * been passed back by ::cudaMallocPitch().
 *
 * ::cudaMemset2DAsync() is asynchronous with respect to the host, so
 * the call may return before the memset is complete. The operation can optionally
 * be associated to a stream by passing a non-zero \p stream argument.
 * If \p stream is non-zero, the operation may overlap with operations in other streams.
 *
 * The device version of this function only handles device to device copies and
 * cannot be given local or shared pointers.
 *
 * \param devPtr - Pointer to 2D device memory
 * \param pitch  - Pitch in bytes of 2D device memory
 * \param value  - Value to set for each byte of specified memory
 * \param width  - Width of matrix set (columns in bytes)
 * \param height - Height of matrix set (rows)
 * \param stream - Stream identifier
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidDevicePointer
 * \notefnerr
 * \note_memset
 * \note_null_stream
 *
 * \sa ::cudaMemset, ::cudaMemset2D, ::cudaMemset3D,
 * ::cudaMemsetAsync, ::cudaMemset3DAsync
 */
    pub fn cudaMemset2DAsync(devPtr: *mut ::libc::c_void,
                             pitch: usize, value: ::libc::c_int,
                             width: usize, height: usize,
                             stream: cudaStream_t) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Initializes or sets device memory to a value
 *
 * Initializes each element of a 3D array to the specified value \p value.
 * The object to initialize is defined by \p pitchedDevPtr. The \p pitch field
 * of \p pitchedDevPtr is the width in memory in bytes of the 3D array pointed
 * to by \p pitchedDevPtr, including any padding added to the end of each row.
 * The \p xsize field specifies the logical width of each row in bytes, while
 * the \p ysize field specifies the height of each 2D slice in rows.
 *
 * The extents of the initialized region are specified as a \p width in bytes,
 * a \p height in rows, and a \p depth in slices.
 *
 * Extents with \p width greater than or equal to the \p xsize of
 * \p pitchedDevPtr may perform significantly faster than extents narrower
 * than the \p xsize. Secondarily, extents with \p height equal to the
 * \p ysize of \p pitchedDevPtr will perform faster than when the \p height is
 * shorter than the \p ysize.
 *
 * This function performs fastest when the \p pitchedDevPtr has been allocated
 * by ::cudaMalloc3D().
 *
 * ::cudaMemset3DAsync() is asynchronous with respect to the host, so
 * the call may return before the memset is complete. The operation can optionally
 * be associated to a stream by passing a non-zero \p stream argument.
 * If \p stream is non-zero, the operation may overlap with operations in other streams.
 *
 * The device version of this function only handles device to device copies and
 * cannot be given local or shared pointers.
 *
 * \param pitchedDevPtr - Pointer to pitched device memory
 * \param value         - Value to set for each byte of specified memory
 * \param extent        - Size parameters for where to set device memory (\p width field in bytes)
 * \param stream - Stream identifier
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidDevicePointer
 * \notefnerr
 * \note_memset
 * \note_null_stream
 *
 * \sa ::cudaMemset, ::cudaMemset2D, ::cudaMemset3D,
 * ::cudaMemsetAsync, ::cudaMemset2DAsync,
 * ::cudaMalloc3D, ::make_cudaPitchedPtr,
 * ::make_cudaExtent
 */
    pub fn cudaMemset3DAsync(pitchedDevPtr: cudaPitchedPtr,
                             value: ::libc::c_int, extent: cudaExtent,
                             stream: cudaStream_t) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Finds the address associated with a CUDA symbol
 *
 * Returns in \p *devPtr the address of symbol \p symbol on the device.
 * \p symbol is a variable that resides in global or constant memory space.
 * If \p symbol cannot be found, or if \p symbol is not declared in the
 * global or constant memory space, \p *devPtr is unchanged and the error
 * ::cudaErrorInvalidSymbol is returned.
 *
 * \param devPtr - Return device pointer associated with symbol
 * \param symbol - Device symbol address
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidSymbol
 * \notefnerr
 * \note_string_api_deprecation
 *
 * \sa \ref ::cudaGetSymbolAddress(void**, const T&) "cudaGetSymbolAddress (C++ API)",
 * \ref ::cudaGetSymbolSize(size_t*, const void*) "cudaGetSymbolSize (C API)"
 */
    pub fn cudaGetSymbolAddress(devPtr: *mut *mut ::libc::c_void,
                                symbol: *const ::libc::c_void)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Finds the size of the object associated with a CUDA symbol
 *
 * Returns in \p *size the size of symbol \p symbol. \p symbol is a variable that
 * resides in global or constant memory space. If \p symbol cannot be found, or
 * if \p symbol is not declared in global or constant memory space, \p *size is
 * unchanged and the error ::cudaErrorInvalidSymbol is returned.
 *
 * \param size   - Size of object associated with symbol
 * \param symbol - Device symbol address
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidSymbol
 * \notefnerr
 * \note_string_api_deprecation
 *
 * \sa \ref ::cudaGetSymbolAddress(void**, const void*) "cudaGetSymbolAddress (C API)",
 * \ref ::cudaGetSymbolSize(size_t*, const T&) "cudaGetSymbolSize (C++ API)"
 */
    pub fn cudaGetSymbolSize(size: *mut usize,
                             symbol: *const ::libc::c_void)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Prefetches memory to the specified destination device
 *
 * Prefetches memory to the specified destination device.  \p devPtr is the
 * base device pointer of the memory to be prefetched and \p dstDevice is the
 * destination device. \p count specifies the number of bytes to copy. \p stream
 * is the stream in which the operation is enqueued. The memory range must refer
 * to managed memory allocated via ::cudaMallocManaged or declared via __managed__ variables.
 *
 * Passing in cudaCpuDeviceId for \p dstDevice will prefetch the data to host memory. If
 * \p dstDevice is a GPU, then the device attribute ::cudaDevAttrConcurrentManagedAccess
 * must be non-zero. Additionally, \p stream must be associated with a device that has a
 * non-zero value for the device attribute ::cudaDevAttrConcurrentManagedAccess.
 *
 * The start address and end address of the memory range will be rounded down and rounded up
 * respectively to be aligned to CPU page size before the prefetch operation is enqueued
 * in the stream.
 *
 * If no physical memory has been allocated for this region, then this memory region
 * will be populated and mapped on the destination device. If there's insufficient
 * memory to prefetch the desired region, the Unified Memory driver may evict pages from other
 * ::cudaMallocManaged allocations to host memory in order to make room. Device memory
 * allocated using ::cudaMalloc or ::cudaMallocArray will not be evicted.
 *
 * By default, any mappings to the previous location of the migrated pages are removed and
 * mappings for the new location are only setup on \p dstDevice. The exact behavior however
 * also depends on the settings applied to this memory range via ::cudaMemAdvise as described
 * below:
 *
 * If ::cudaMemAdviseSetReadMostly was set on any subset of this memory range,
 * then that subset will create a read-only copy of the pages on \p dstDevice.
 *
 * If ::cudaMemAdviseSetPreferredLocation was called on any subset of this memory
 * range, then the pages will be migrated to \p dstDevice even if \p dstDevice is not the
 * preferred location of any pages in the memory range.
 *
 * If ::cudaMemAdviseSetAccessedBy was called on any subset of this memory range,
 * then mappings to those pages from all the appropriate processors are updated to
 * refer to the new location if establishing such a mapping is possible. Otherwise,
 * those mappings are cleared.
 *
 * Note that this API is not required for functionality and only serves to improve performance
 * by allowing the application to migrate data to a suitable location before it is accessed.
 * Memory accesses to this range are always coherent and are allowed even when the data is
 * actively being migrated.
 *
 * Note that this function is asynchronous with respect to the host and all work
 * on other devices.
 *
 * \param devPtr    - Pointer to be prefetched
 * \param count     - Size in bytes
 * \param dstDevice - Destination device to prefetch to
 * \param stream    - Stream to enqueue prefetch operation
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidDevice
 * \notefnerr
 * \note_async
 * \note_null_stream
 *
 * \sa ::cudaMemcpy, ::cudaMemcpyPeer, ::cudaMemcpyAsync,
 * ::cudaMemcpy3DPeerAsync, ::cudaMemAdvise
 */
    pub fn cudaMemPrefetchAsync(devPtr: *const ::libc::c_void,
                                count: usize,
                                dstDevice: ::libc::c_int,
                                stream: cudaStream_t) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Advise about the usage of a given memory range
 *
 * Advise the Unified Memory subsystem about the usage pattern for the memory range
 * starting at \p devPtr with a size of \p count bytes. The start address and end address of the memory
 * range will be rounded down and rounded up respectively to be aligned to CPU page size before the
 * advice is applied. The memory range must refer to managed memory allocated via ::cudaMallocManaged
 * or declared via __managed__ variables.
 *
 * The \p advice parameter can take the following values:
 * - ::cudaMemAdviseSetReadMostly: This implies that the data is mostly going to be read
 * from and only occasionally written to. Any read accesses from any processor to this region will create a
 * read-only copy of at least the accessed pages in that processor's memory. Additionally, if ::cudaMemPrefetchAsync
 * is called on this region, it will create a read-only copy of the data on the destination processor.
 * If any processor writes to this region, all copies of the corresponding page will be invalidated
 * except for the one where the write occurred. The \p device argument is ignored for this advice.
 * Note that for a page to be read-duplicated, the accessing processor must either be the CPU or a GPU
 * that has a non-zero value for the device attribute ::cudaDevAttrConcurrentManagedAccess.
 * Also, if a context is created on a device that does not have the device attribute
 * ::cudaDevAttrConcurrentManagedAccess set, then read-duplication will not occur until
 * all such contexts are destroyed.
 * - ::cudaMemAdviceUnsetReadMostly: Undoes the effect of ::cudaMemAdviceReadMostly and also prevents the
 * Unified Memory driver from attempting heuristic read-duplication on the memory range. Any read-duplicated
 * copies of the data will be collapsed into a single copy. The location for the collapsed
 * copy will be the preferred location if the page has a preferred location and one of the read-duplicated
 * copies was resident at that location. Otherwise, the location chosen is arbitrary.
 * - ::cudaMemAdviseSetPreferredLocation: This advice sets the preferred location for the
 * data to be the memory belonging to \p device. Passing in cudaCpuDeviceId for \p device sets the
 * preferred location as host memory. If \p device is a GPU, then it must have a non-zero value for the
 * device attribute ::cudaDevAttrConcurrentManagedAccess. Setting the preferred location
 * does not cause data to migrate to that location immediately. Instead, it guides the migration policy
 * when a fault occurs on that memory region. If the data is already in its preferred location and the
 * faulting processor can establish a mapping without requiring the data to be migrated, then
 * data migration will be avoided. On the other hand, if the data is not in its preferred location
 * or if a direct mapping cannot be established, then it will be migrated to the processor accessing
 * it. It is important to note that setting the preferred location does not prevent data prefetching
 * done using ::cudaMemPrefetchAsync.
 * Having a preferred location can override the page thrash detection and resolution logic in the Unified
 * Memory driver. Normally, if a page is detected to be constantly thrashing between for example host and device
 * memory, the page may eventually be pinned to host memory by the Unified Memory driver. But
 * if the preferred location is set as device memory, then the page will continue to thrash indefinitely.
 * If ::cudaMemAdviseSetReadMostly is also set on this memory region or any subset of it, then the
 * policies associated with that advice will override the policies of this advice.
 * - ::cudaMemAdviseUnsetPreferredLocation: Undoes the effect of ::cudaMemAdviseSetPreferredLocation
 * and changes the preferred location to none.
 * - ::cudaMemAdviseSetAccessedBy: This advice implies that the data will be accessed by \p device.
 * Passing in ::cudaCpuDeviceId for \p device will set the advice for the CPU. If \p device is a GPU, then
 * the device attribute ::cudaDevAttrConcurrentManagedAccess must be non-zero.
 * This advice does not cause data migration and has no impact on the location of the data per se. Instead,
 * it causes the data to always be mapped in the specified processor's page tables, as long as the
 * location of the data permits a mapping to be established. If the data gets migrated for any reason,
 * the mappings are updated accordingly.
 * This advice is recommended in scenarios where data locality is not important, but avoiding faults is.
 * Consider for example a system containing multiple GPUs with peer-to-peer access enabled, where the
 * data located on one GPU is occasionally accessed by peer GPUs. In such scenarios, migrating data
 * over to the other GPUs is not as important because the accesses are infrequent and the overhead of
 * migration may be too high. But preventing faults can still help improve performance, and so having
 * a mapping set up in advance is useful. Note that on CPU access of this data, the data may be migrated
 * to host memory because the CPU typically cannot access device memory directly. Any GPU that had the
 * ::cudaMemAdviceSetAccessedBy flag set for this data will now have its mapping updated to point to the
 * page in host memory.
 * If ::cudaMemAdviseSetReadMostly is also set on this memory region or any subset of it, then the
 * policies associated with that advice will override the policies of this advice. Additionally, if the
 * preferred location of this memory region or any subset of it is also \p device, then the policies
 * associated with ::cudaMemAdviseSetPreferredLocation will override the policies of this advice.
 * - ::cudaMemAdviseUnsetAccessedBy: Undoes the effect of ::cudaMemAdviseSetAccessedBy. Any mappings to
 * the data from \p device may be removed at any time causing accesses to result in non-fatal page faults.
 *
 * \param devPtr - Pointer to memory to set the advice for
 * \param count  - Size in bytes of the memory range
 * \param advice - Advice to be applied for the specified memory range
 * \param device - Device to apply the advice for
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidDevice
 * \notefnerr
 * \note_async
 * \note_null_stream
 *
 * \sa ::cudaMemcpy, ::cudaMemcpyPeer, ::cudaMemcpyAsync,
 * ::cudaMemcpy3DPeerAsync, ::cudaMemPrefetchAsync
 */
    pub fn cudaMemAdvise(devPtr: *const ::libc::c_void, count: usize,
                         advice: cudaMemoryAdvise,
                         device: ::libc::c_int) -> cudaError_t;
}
extern "C" {
    /**
* \brief Query an attribute of a given memory range
*
* Query an attribute about the memory range starting at \p devPtr with a size of \p count bytes. The
* memory range must refer to managed memory allocated via ::cudaMallocManaged or declared via
* __managed__ variables.
*
* The \p attribute parameter can take the following values:
* - ::cudaMemRangeAttributeReadMostly: If this attribute is specified, \p data will be interpreted
* as a 32-bit integer, and \p dataSize must be 4. The result returned will be 1 if all pages in the given
* memory range have read-duplication enabled, or 0 otherwise.
* - ::cudaMemRangeAttributePreferredLocation: If this attribute is specified, \p data will be
* interpreted as a 32-bit integer, and \p dataSize must be 4. The result returned will be a GPU device
* id if all pages in the memory range have that GPU as their preferred location, or it will be cudaCpuDeviceId
* if all pages in the memory range have the CPU as their preferred location, or it will be cudaInvalidDeviceId
* if either all the pages don't have the same preferred location or some of the pages don't have a
* preferred location at all. Note that the actual location of the pages in the memory range at the time of
* the query may be different from the preferred location.
* - ::cudaMemRangeAttributeAccessedBy: If this attribute is specified, \p data will be interpreted
* as an array of 32-bit integers, and \p dataSize must be a non-zero multiple of 4. The result returned
* will be a list of device ids that had ::cudaMemAdviceSetAccessedBy set for that entire memory range.
* If any device does not have that advice set for the entire memory range, that device will not be included.
* If \p data is larger than the number of devices that have that advice set for that memory range,
* cudaInvalidDeviceId will be returned in all the extra space provided. For ex., if \p dataSize is 12
* (i.e. \p data has 3 elements) and only device 0 has the advice set, then the result returned will be
* { 0, cudaInvalidDeviceId, cudaInvalidDeviceId }. If \p data is smaller than the number of devices that have
* that advice set, then only as many devices will be returned as can fit in the array. There is no
* guarantee on which specific devices will be returned, however.
* - ::cudaMemRangeAttributeLastPrefetchLocation: If this attribute is specified, \p data will be
* interpreted as a 32-bit integer, and \p dataSize must be 4. The result returned will be the last location
* to which all pages in the memory range were prefetched explicitly via ::cudaMemPrefetchAsync. This will either be
* a GPU id or cudaCpuDeviceId depending on whether the last location for prefetch was a GPU or the CPU
* respectively. If any page in the memory range was never explicitly prefetched or if all pages were not
* prefetched to the same location, cudaInvalidDeviceId will be returned. Note that this simply returns the
* last location that the applicaton requested to prefetch the memory range to. It gives no indication as to
* whether the prefetch operation to that location has completed or even begun.
*
* \param data      - A pointers to a memory location where the result
*                    of each attribute query will be written to.
* \param dataSize  - Array containing the size of data
* \param attribute - The attribute to query
* \param devPtr    - Start of the range to query
* \param count     - Size of the range to query
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue
 * \notefnerr
 * \note_async
 * \note_null_stream
 *
 * \sa ::cudaMemRangeGetAttributes, ::cudaMemPrefetchAsync,
 * ::cudaMemAdvise
 */
    pub fn cudaMemRangeGetAttribute(data: *mut ::libc::c_void,
                                    dataSize: usize,
                                    attribute: cudaMemRangeAttribute,
                                    devPtr: *const ::libc::c_void,
                                    count: usize) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Query attributes of a given memory range.
 *
 * Query attributes of the memory range starting at \p devPtr with a size of \p count bytes. The
 * memory range must refer to managed memory allocated via ::cudaMallocManaged or declared via
 * __managed__ variables. The \p attributes array will be interpreted to have \p numAttributes
 * entries. The \p dataSizes array will also be interpreted to have \p numAttributes entries.
 * The results of the query will be stored in \p data.
 *
 * The list of supported attributes are given below. Please refer to ::cudaMemRangeGetAttribute for
 * attribute descriptions and restrictions.
 *
 * - ::cudaMemRangeAttributeReadMostly
 * - ::cudaMemRangeAttributePreferredLocation
 * - ::cudaMemRangeAttributeAccessedBy
 * - ::cudaMemRangeAttributeLastPrefetchLocation
 *
 * \param data          - A two-dimensional array containing pointers to memory
 *                        locations where the result of each attribute query will be written to.
 * \param dataSizes     - Array containing the sizes of each result
 * \param attributes    - An array of attributes to query
 *                        (numAttributes and the number of attributes in this array should match)
 * \param numAttributes - Number of attributes to query
 * \param devPtr        - Start of the range to query
 * \param count         - Size of the range to query
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue
 * \notefnerr
 *
 * \sa ::cudaMemRangeGetAttribute, ::cudaMemAdvise
 * ::cudaMemPrefetchAsync
 */
    pub fn cudaMemRangeGetAttributes(data: *mut *mut ::libc::c_void,
                                     dataSizes: *mut usize,
                                     attributes: *mut cudaMemRangeAttribute,
                                     numAttributes: usize,
                                     devPtr: *const ::libc::c_void,
                                     count: usize) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Returns attributes about a specified pointer
 *
 * Returns in \p *attributes the attributes of the pointer \p ptr.
 * If pointer was not allocated in, mapped by or registered with context
 * supporting unified addressing ::cudaErrorInvalidValue is returned.
 *
 * The ::cudaPointerAttributes structure is defined as:
 * \code
    struct cudaPointerAttributes {
        enum cudaMemoryType memoryType;
        int device;
        void *devicePointer;
        void *hostPointer;
        int isManaged;
    }
    \endcode
 * In this structure, the individual fields mean
 *
 * - \ref ::cudaPointerAttributes::memoryType "memoryType" identifies the physical
 *   location of the memory associated with pointer \p ptr.  It can be
 *   ::cudaMemoryTypeHost for host memory or ::cudaMemoryTypeDevice for device
 *   memory.
 *
 * - \ref ::cudaPointerAttributes::device "device" is the device against which
 *   \p ptr was allocated.  If \p ptr has memory type ::cudaMemoryTypeDevice
 *   then this identifies the device on which the memory referred to by \p ptr
 *   physically resides.  If \p ptr has memory type ::cudaMemoryTypeHost then this
 *   identifies the device which was current when the allocation was made
 *   (and if that device is deinitialized then this allocation will vanish
 *   with that device's state).
 *
 * - \ref ::cudaPointerAttributes::devicePointer "devicePointer" is
 *   the device pointer alias through which the memory referred to by \p ptr
 *   may be accessed on the current device.
 *   If the memory referred to by \p ptr cannot be accessed directly by the
 *   current device then this is NULL.
 *
 * - \ref ::cudaPointerAttributes::hostPointer "hostPointer" is
 *   the host pointer alias through which the memory referred to by \p ptr
 *   may be accessed on the host.
 *   If the memory referred to by \p ptr cannot be accessed directly by the
 *   host then this is NULL.
 *
 * - \ref ::cudaPointerAttributes::isManaged "isManaged" indicates if
 *   the pointer \p ptr points to managed memory or not.
 *
 * \param attributes - Attributes for the specified pointer
 * \param ptr        - Pointer to get attributes for
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidDevice,
 * ::cudaErrorInvalidValue
 *
 * \sa ::cudaGetDeviceCount, ::cudaGetDevice, ::cudaSetDevice,
 * ::cudaChooseDevice
 */
    pub fn cudaPointerGetAttributes(attributes: *mut cudaPointerAttributes,
                                    ptr: *const ::libc::c_void)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Queries if a device may directly access a peer device's memory.
 *
 * Returns in \p *canAccessPeer a value of 1 if device \p device is capable of
 * directly accessing memory from \p peerDevice and 0 otherwise.  If direct
 * access of \p peerDevice from \p device is possible, then access may be
 * enabled by calling ::cudaDeviceEnablePeerAccess().
 *
 * \param canAccessPeer - Returned access capability
 * \param device        - Device from which allocations on \p peerDevice are to
 *                        be directly accessed.
 * \param peerDevice    - Device on which the allocations to be directly accessed
 *                        by \p device reside.
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidDevice
 * \notefnerr
 *
 * \sa ::cudaDeviceEnablePeerAccess,
 * ::cudaDeviceDisablePeerAccess
 */
    pub fn cudaDeviceCanAccessPeer(canAccessPeer: *mut ::libc::c_int,
                                   device: ::libc::c_int,
                                   peerDevice: ::libc::c_int)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Enables direct access to memory allocations on a peer device.
 *
 * On success, all allocations from \p peerDevice will immediately be accessible by
 * the current device.  They will remain accessible until access is explicitly
 * disabled using ::cudaDeviceDisablePeerAccess() or either device is reset using
 * ::cudaDeviceReset().
 *
 * Note that access granted by this call is unidirectional and that in order to access
 * memory on the current device from \p peerDevice, a separate symmetric call
 * to ::cudaDeviceEnablePeerAccess() is required.
 *
 * Each device can support a system-wide maximum of eight peer connections.
 *
 * Peer access is not supported in 32 bit applications.
 *
 * Returns ::cudaErrorInvalidDevice if ::cudaDeviceCanAccessPeer() indicates
 * that the current device cannot directly access memory from \p peerDevice.
 *
 * Returns ::cudaErrorPeerAccessAlreadyEnabled if direct access of
 * \p peerDevice from the current device has already been enabled.
 *
 * Returns ::cudaErrorInvalidValue if \p flags is not 0.
 *
 * \param peerDevice  - Peer device to enable direct access to from the current device
 * \param flags       - Reserved for future use and must be set to 0
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidDevice,
 * ::cudaErrorPeerAccessAlreadyEnabled,
 * ::cudaErrorInvalidValue
 * \notefnerr
 *
 * \sa ::cudaDeviceCanAccessPeer,
 * ::cudaDeviceDisablePeerAccess
 */
    pub fn cudaDeviceEnablePeerAccess(peerDevice: ::libc::c_int,
                                      flags: ::libc::c_uint)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Disables direct access to memory allocations on a peer device.
 *
 * Returns ::cudaErrorPeerAccessNotEnabled if direct access to memory on
 * \p peerDevice has not yet been enabled from the current device.
 *
 * \param peerDevice - Peer device to disable direct access to
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorPeerAccessNotEnabled,
 * ::cudaErrorInvalidDevice
 * \notefnerr
 *
 * \sa ::cudaDeviceCanAccessPeer,
 * ::cudaDeviceEnablePeerAccess
 */
    pub fn cudaDeviceDisablePeerAccess(peerDevice: ::libc::c_int)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Unregisters a graphics resource for access by CUDA
 *
 * Unregisters the graphics resource \p resource so it is not accessible by
 * CUDA unless registered again.
 *
 * If \p resource is invalid then ::cudaErrorInvalidResourceHandle is
 * returned.
 *
 * \param resource - Resource to unregister
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidResourceHandle,
 * ::cudaErrorUnknown
 * \notefnerr
 *
 * \sa
 * ::cudaGraphicsD3D9RegisterResource,
 * ::cudaGraphicsD3D10RegisterResource,
 * ::cudaGraphicsD3D11RegisterResource,
 * ::cudaGraphicsGLRegisterBuffer,
 * ::cudaGraphicsGLRegisterImage
 */
    pub fn cudaGraphicsUnregisterResource(resource: cudaGraphicsResource_t)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Set usage flags for mapping a graphics resource
 *
 * Set \p flags for mapping the graphics resource \p resource.
 *
 * Changes to \p flags will take effect the next time \p resource is mapped.
 * The \p flags argument may be any of the following:
 * - ::cudaGraphicsMapFlagsNone: Specifies no hints about how \p resource will
 *     be used. It is therefore assumed that CUDA may read from or write to \p resource.
 * - ::cudaGraphicsMapFlagsReadOnly: Specifies that CUDA will not write to \p resource.
 * - ::cudaGraphicsMapFlagsWriteDiscard: Specifies CUDA will not read from \p resource and will
 *   write over the entire contents of \p resource, so none of the data
 *   previously stored in \p resource will be preserved.
 *
 * If \p resource is presently mapped for access by CUDA then ::cudaErrorUnknown is returned.
 * If \p flags is not one of the above values then ::cudaErrorInvalidValue is returned.
 *
 * \param resource - Registered resource to set flags for
 * \param flags    - Parameters for resource mapping
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidResourceHandle,
 * ::cudaErrorUnknown,
 * \notefnerr
 *
 * \sa
 * ::cudaGraphicsMapResources
 */
    pub fn cudaGraphicsResourceSetMapFlags(resource: cudaGraphicsResource_t,
                                           flags: ::libc::c_uint)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Map graphics resources for access by CUDA
 *
 * Maps the \p count graphics resources in \p resources for access by CUDA.
 *
 * The resources in \p resources may be accessed by CUDA until they
 * are unmapped. The graphics API from which \p resources were registered
 * should not access any resources while they are mapped by CUDA. If an
 * application does so, the results are undefined.
 *
 * This function provides the synchronization guarantee that any graphics calls
 * issued before ::cudaGraphicsMapResources() will complete before any subsequent CUDA
 * work issued in \p stream begins.
 *
 * If \p resources contains any duplicate entries then ::cudaErrorInvalidResourceHandle
 * is returned. If any of \p resources are presently mapped for access by
 * CUDA then ::cudaErrorUnknown is returned.
 *
 * \param count     - Number of resources to map
 * \param resources - Resources to map for CUDA
 * \param stream    - Stream for synchronization
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidResourceHandle,
 * ::cudaErrorUnknown
 * \note_null_stream
 * \notefnerr
 *
 * \sa
 * ::cudaGraphicsResourceGetMappedPointer,
 * ::cudaGraphicsSubResourceGetMappedArray,
 * ::cudaGraphicsUnmapResources
 */
    pub fn cudaGraphicsMapResources(count: ::libc::c_int,
                                    resources: *mut cudaGraphicsResource_t,
                                    stream: cudaStream_t) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Unmap graphics resources.
 *
 * Unmaps the \p count graphics resources in \p resources.
 *
 * Once unmapped, the resources in \p resources may not be accessed by CUDA
 * until they are mapped again.
 *
 * This function provides the synchronization guarantee that any CUDA work issued
 * in \p stream before ::cudaGraphicsUnmapResources() will complete before any
 * subsequently issued graphics work begins.
 *
 * If \p resources contains any duplicate entries then ::cudaErrorInvalidResourceHandle
 * is returned. If any of \p resources are not presently mapped for access by
 * CUDA then ::cudaErrorUnknown is returned.
 *
 * \param count     - Number of resources to unmap
 * \param resources - Resources to unmap
 * \param stream    - Stream for synchronization
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidResourceHandle,
 * ::cudaErrorUnknown
 * \note_null_stream
 * \notefnerr
 *
 * \sa
 * ::cudaGraphicsMapResources
 */
    pub fn cudaGraphicsUnmapResources(count: ::libc::c_int,
                                      resources: *mut cudaGraphicsResource_t,
                                      stream: cudaStream_t) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Get an device pointer through which to access a mapped graphics resource.
 *
 * Returns in \p *devPtr a pointer through which the mapped graphics resource
 * \p resource may be accessed.
 * Returns in \p *size the size of the memory in bytes which may be accessed from that pointer.
 * The value set in \p devPtr may change every time that \p resource is mapped.
 *
 * If \p resource is not a buffer then it cannot be accessed via a pointer and
 * ::cudaErrorUnknown is returned.
 * If \p resource is not mapped then ::cudaErrorUnknown is returned.
 * *
 * \param devPtr     - Returned pointer through which \p resource may be accessed
 * \param size       - Returned size of the buffer accessible starting at \p *devPtr
 * \param resource   - Mapped resource to access
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidResourceHandle,
 * ::cudaErrorUnknown
 * \notefnerr
 *
 * \sa
 * ::cudaGraphicsMapResources,
 * ::cudaGraphicsSubResourceGetMappedArray
 */
    pub fn cudaGraphicsResourceGetMappedPointer(devPtr:
                                                    *mut *mut ::libc::c_void,
                                                size: *mut usize,
                                                resource:
                                                    cudaGraphicsResource_t)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Get an array through which to access a subresource of a mapped graphics resource.
 *
 * Returns in \p *array an array through which the subresource of the mapped
 * graphics resource \p resource which corresponds to array index \p arrayIndex
 * and mipmap level \p mipLevel may be accessed.  The value set in \p array may
 * change every time that \p resource is mapped.
 *
 * If \p resource is not a texture then it cannot be accessed via an array and
 * ::cudaErrorUnknown is returned.
 * If \p arrayIndex is not a valid array index for \p resource then
 * ::cudaErrorInvalidValue is returned.
 * If \p mipLevel is not a valid mipmap level for \p resource then
 * ::cudaErrorInvalidValue is returned.
 * If \p resource is not mapped then ::cudaErrorUnknown is returned.
 *
 * \param array       - Returned array through which a subresource of \p resource may be accessed
 * \param resource    - Mapped resource to access
 * \param arrayIndex  - Array index for array textures or cubemap face
 *                      index as defined by ::cudaGraphicsCubeFace for
 *                      cubemap textures for the subresource to access
 * \param mipLevel    - Mipmap level for the subresource to access
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidResourceHandle,
 * ::cudaErrorUnknown
 * \notefnerr
 *
 * \sa ::cudaGraphicsResourceGetMappedPointer
 */
    pub fn cudaGraphicsSubResourceGetMappedArray(array: *mut cudaArray_t,
                                                 resource:
                                                     cudaGraphicsResource_t,
                                                 arrayIndex:
                                                     ::libc::c_uint,
                                                 mipLevel:
                                                     ::libc::c_uint)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Get a mipmapped array through which to access a mapped graphics resource.
 *
 * Returns in \p *mipmappedArray a mipmapped array through which the mapped
 * graphics resource \p resource may be accessed. The value set in \p mipmappedArray may
 * change every time that \p resource is mapped.
 *
 * If \p resource is not a texture then it cannot be accessed via an array and
 * ::cudaErrorUnknown is returned.
 * If \p resource is not mapped then ::cudaErrorUnknown is returned.
 *
 * \param mipmappedArray - Returned mipmapped array through which \p resource may be accessed
 * \param resource       - Mapped resource to access
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidResourceHandle,
 * ::cudaErrorUnknown
 * \notefnerr
 *
 * \sa ::cudaGraphicsResourceGetMappedPointer
 */
    pub fn cudaGraphicsResourceGetMappedMipmappedArray(mipmappedArray:
                                                           *mut cudaMipmappedArray_t,
                                                       resource:
                                                           cudaGraphicsResource_t)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Get the channel descriptor of an array
 *
 * Returns in \p *desc the channel descriptor of the CUDA array \p array.
 *
 * \param desc  - Channel format
 * \param array - Memory array on device
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue
 * \notefnerr
 *
 * \sa \ref ::cudaCreateChannelDesc(int, int, int, int, cudaChannelFormatKind) "cudaCreateChannelDesc (C API)",
 * ::cudaGetTextureReference,
 * \ref ::cudaBindTexture(size_t*, const struct textureReference*, const void*, const struct cudaChannelFormatDesc*, size_t) "cudaBindTexture (C API)",
 * \ref ::cudaBindTexture2D(size_t*, const struct textureReference*, const void*, const struct cudaChannelFormatDesc*, size_t, size_t, size_t) "cudaBindTexture2D (C API)",
 * \ref ::cudaBindTextureToArray(const struct textureReference*, cudaArray_const_t, const struct cudaChannelFormatDesc*) "cudaBindTextureToArray (C API)",
 * \ref ::cudaUnbindTexture(const struct textureReference*) "cudaUnbindTexture (C API)",
 * \ref ::cudaGetTextureAlignmentOffset(size_t*, const struct textureReference*) "cudaGetTextureAlignmentOffset (C API)"
 */
    pub fn cudaGetChannelDesc(desc: *mut cudaChannelFormatDesc,
                              array: cudaArray_const_t) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Returns a channel descriptor using the specified format
 *
 * Returns a channel descriptor with format \p f and number of bits of each
 * component \p x, \p y, \p z, and \p w.  The ::cudaChannelFormatDesc is
 * defined as:
 * \code
  struct cudaChannelFormatDesc {
    int x, y, z, w;
    enum cudaChannelFormatKind f;
  };
 * \endcode
 *
 * where ::cudaChannelFormatKind is one of ::cudaChannelFormatKindSigned,
 * ::cudaChannelFormatKindUnsigned, or ::cudaChannelFormatKindFloat.
 *
 * \param x - X component
 * \param y - Y component
 * \param z - Z component
 * \param w - W component
 * \param f - Channel format
 *
 * \return
 * Channel descriptor with format \p f
 *
 * \sa \ref ::cudaCreateChannelDesc(void) "cudaCreateChannelDesc (C++ API)",
 * ::cudaGetChannelDesc, ::cudaGetTextureReference,
 * \ref ::cudaBindTexture(size_t*, const struct textureReference*, const void*, const struct cudaChannelFormatDesc*, size_t) "cudaBindTexture (C API)",
 * \ref ::cudaBindTexture2D(size_t*, const struct textureReference*, const void*, const struct cudaChannelFormatDesc*, size_t, size_t, size_t) "cudaBindTexture2D (C API)",
 * \ref ::cudaBindTextureToArray(const struct textureReference*, cudaArray_const_t, const struct cudaChannelFormatDesc*) "cudaBindTextureToArray (C API)",
 * \ref ::cudaUnbindTexture(const struct textureReference*) "cudaUnbindTexture (C API)",
 * \ref ::cudaGetTextureAlignmentOffset(size_t*, const struct textureReference*) "cudaGetTextureAlignmentOffset (C API)"
 */
    pub fn cudaCreateChannelDesc(x: ::libc::c_int,
                                 y: ::libc::c_int,
                                 z: ::libc::c_int,
                                 w: ::libc::c_int,
                                 f: cudaChannelFormatKind)
     -> cudaChannelFormatDesc;
}
extern "C" {
    /**
 * \brief Binds a memory area to a texture
 *
 * Binds \p size bytes of the memory area pointed to by \p devPtr to the
 * texture reference \p texref. \p desc describes how the memory is interpreted
 * when fetching values from the texture. Any memory previously bound to
 * \p texref is unbound.
 *
 * Since the hardware enforces an alignment requirement on texture base
 * addresses,
 * \ref ::cudaBindTexture(size_t*, const struct textureReference*, const void*, const struct cudaChannelFormatDesc*, size_t) "cudaBindTexture()"
 * returns in \p *offset a byte offset that
 * must be applied to texture fetches in order to read from the desired memory.
 * This offset must be divided by the texel size and passed to kernels that
 * read from the texture so they can be applied to the ::tex1Dfetch() function.
 * If the device memory pointer was returned from ::cudaMalloc(), the offset is
 * guaranteed to be 0 and NULL may be passed as the \p offset parameter.
 *
 * The total number of elements (or texels) in the linear address range
 * cannot exceed ::cudaDeviceProp::maxTexture1DLinear[0].
 * The number of elements is computed as (\p size / elementSize),
 * where elementSize is determined from \p desc.
 *
 * \param offset - Offset in bytes
 * \param texref - Texture to bind
 * \param devPtr - Memory area on device
 * \param desc   - Channel format
 * \param size   - Size of the memory area pointed to by devPtr
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidDevicePointer,
 * ::cudaErrorInvalidTexture
 * \notefnerr
 *
 * \sa \ref ::cudaCreateChannelDesc(int, int, int, int, cudaChannelFormatKind) "cudaCreateChannelDesc (C API)",
 * ::cudaGetChannelDesc, ::cudaGetTextureReference,
 * \ref ::cudaBindTexture(size_t*, const struct texture< T, dim, readMode>&, const void*, const struct cudaChannelFormatDesc&, size_t) "cudaBindTexture (C++ API)",
 * \ref ::cudaBindTexture2D(size_t*, const struct textureReference*, const void*, const struct cudaChannelFormatDesc*, size_t, size_t, size_t) "cudaBindTexture2D (C API)",
 * \ref ::cudaBindTextureToArray(const struct textureReference*, cudaArray_const_t, const struct cudaChannelFormatDesc*) "cudaBindTextureToArray (C API)",
 * \ref ::cudaUnbindTexture(const struct textureReference*) "cudaUnbindTexture (C API)",
 * \ref ::cudaGetTextureAlignmentOffset(size_t*, const struct textureReference*) "cudaGetTextureAlignmentOffset (C API)"
 */
    pub fn cudaBindTexture(offset: *mut usize,
                           texref: *const textureReference,
                           devPtr: *const ::libc::c_void,
                           desc: *const cudaChannelFormatDesc, size: usize)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Binds a 2D memory area to a texture
 *
 * Binds the 2D memory area pointed to by \p devPtr to the
 * texture reference \p texref. The size of the area is constrained by
 * \p width in texel units, \p height in texel units, and \p pitch in byte
 * units. \p desc describes how the memory is interpreted when fetching values
 * from the texture. Any memory previously bound to \p texref is unbound.
 *
 * Since the hardware enforces an alignment requirement on texture base
 * addresses, ::cudaBindTexture2D() returns in \p *offset a byte offset that
 * must be applied to texture fetches in order to read from the desired memory.
 * This offset must be divided by the texel size and passed to kernels that
 * read from the texture so they can be applied to the ::tex2D() function.
 * If the device memory pointer was returned from ::cudaMalloc(), the offset is
 * guaranteed to be 0 and NULL may be passed as the \p offset parameter.
 *
 * \p width and \p height, which are specified in elements (or texels), cannot
 * exceed ::cudaDeviceProp::maxTexture2DLinear[0] and ::cudaDeviceProp::maxTexture2DLinear[1]
 * respectively. \p pitch, which is specified in bytes, cannot exceed
 * ::cudaDeviceProp::maxTexture2DLinear[2].
 *
 * The driver returns ::cudaErrorInvalidValue if \p pitch is not a multiple of
 * ::cudaDeviceProp::texturePitchAlignment.
 *
 * \param offset - Offset in bytes
 * \param texref - Texture reference to bind
 * \param devPtr - 2D memory area on device
 * \param desc   - Channel format
 * \param width  - Width in texel units
 * \param height - Height in texel units
 * \param pitch  - Pitch in bytes
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidDevicePointer,
 * ::cudaErrorInvalidTexture
 * \notefnerr
 *
 * \sa \ref ::cudaCreateChannelDesc(int, int, int, int, cudaChannelFormatKind) "cudaCreateChannelDesc (C API)",
 * ::cudaGetChannelDesc, ::cudaGetTextureReference,
 * \ref ::cudaBindTexture(size_t*, const struct textureReference*, const void*, const struct cudaChannelFormatDesc*, size_t) "cudaBindTexture (C API)",
 * \ref ::cudaBindTexture2D(size_t*, const struct texture< T, dim, readMode>&, const void*, const struct cudaChannelFormatDesc&, size_t, size_t, size_t) "cudaBindTexture2D (C++ API)",
 * \ref ::cudaBindTexture2D(size_t*, const struct texture<T, dim, readMode>&, const void*, size_t, size_t, size_t) "cudaBindTexture2D (C++ API, inherited channel descriptor)",
 * \ref ::cudaBindTextureToArray(const struct textureReference*, cudaArray_const_t, const struct cudaChannelFormatDesc*) "cudaBindTextureToArray (C API)",
 * \ref ::cudaUnbindTexture(const struct textureReference*) "cudaBindTextureToArray (C API)",
 * \ref ::cudaGetTextureAlignmentOffset(size_t*, const struct textureReference*) "cudaGetTextureAlignmentOffset (C API)"
 */
    pub fn cudaBindTexture2D(offset: *mut usize,
                             texref: *const textureReference,
                             devPtr: *const ::libc::c_void,
                             desc: *const cudaChannelFormatDesc, width: usize,
                             height: usize, pitch: usize) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Binds an array to a texture
 *
 * Binds the CUDA array \p array to the texture reference \p texref.
 * \p desc describes how the memory is interpreted when fetching values from
 * the texture. Any CUDA array previously bound to \p texref is unbound.
 *
 * \param texref - Texture to bind
 * \param array  - Memory array on device
 * \param desc   - Channel format
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidDevicePointer,
 * ::cudaErrorInvalidTexture
 * \notefnerr
 *
 * \sa \ref ::cudaCreateChannelDesc(int, int, int, int, cudaChannelFormatKind) "cudaCreateChannelDesc (C API)",
 * ::cudaGetChannelDesc, ::cudaGetTextureReference,
 * \ref ::cudaBindTexture(size_t*, const struct textureReference*, const void*, const struct cudaChannelFormatDesc*, size_t) "cudaBindTexture (C API)",
 * \ref ::cudaBindTexture2D(size_t*, const struct textureReference*, const void*, const struct cudaChannelFormatDesc*, size_t, size_t, size_t) "cudaBindTexture2D (C API)",
 * \ref ::cudaBindTextureToArray(const struct texture< T, dim, readMode>&, cudaArray_const_t, const struct cudaChannelFormatDesc&) "cudaBindTextureToArray (C++ API)",
 * \ref ::cudaUnbindTexture(const struct textureReference*) "cudaUnbindTexture (C API)",
 * \ref ::cudaGetTextureAlignmentOffset(size_t*, const struct textureReference*) "cudaGetTextureAlignmentOffset (C API)"
 */
    pub fn cudaBindTextureToArray(texref: *const textureReference,
                                  array: cudaArray_const_t,
                                  desc: *const cudaChannelFormatDesc)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Binds a mipmapped array to a texture
 *
 * Binds the CUDA mipmapped array \p mipmappedArray to the texture reference \p texref.
 * \p desc describes how the memory is interpreted when fetching values from
 * the texture. Any CUDA mipmapped array previously bound to \p texref is unbound.
 *
 * \param texref         - Texture to bind
 * \param mipmappedArray - Memory mipmapped array on device
 * \param desc           - Channel format
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidDevicePointer,
 * ::cudaErrorInvalidTexture
 * \notefnerr
 *
 * \sa \ref ::cudaCreateChannelDesc(int, int, int, int, cudaChannelFormatKind) "cudaCreateChannelDesc (C API)",
 * ::cudaGetChannelDesc, ::cudaGetTextureReference,
 * \ref ::cudaBindTexture(size_t*, const struct textureReference*, const void*, const struct cudaChannelFormatDesc*, size_t) "cudaBindTexture (C API)",
 * \ref ::cudaBindTexture2D(size_t*, const struct textureReference*, const void*, const struct cudaChannelFormatDesc*, size_t, size_t, size_t) "cudaBindTexture2D (C API)",
 * \ref ::cudaBindTextureToArray(const struct texture< T, dim, readMode>&, cudaArray_const_t, const struct cudaChannelFormatDesc&) "cudaBindTextureToArray (C++ API)",
 * \ref ::cudaUnbindTexture(const struct textureReference*) "cudaUnbindTexture (C API)",
 * \ref ::cudaGetTextureAlignmentOffset(size_t*, const struct textureReference*) "cudaGetTextureAlignmentOffset (C API)"
 */
    pub fn cudaBindTextureToMipmappedArray(texref: *const textureReference,
                                           mipmappedArray:
                                               cudaMipmappedArray_const_t,
                                           desc: *const cudaChannelFormatDesc)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Unbinds a texture
 *
 * Unbinds the texture bound to \p texref.
 *
 * \param texref - Texture to unbind
 *
 * \return
 * ::cudaSuccess
 * \notefnerr
 *
 * \sa \ref ::cudaCreateChannelDesc(int, int, int, int, cudaChannelFormatKind) "cudaCreateChannelDesc (C API)",
 * ::cudaGetChannelDesc, ::cudaGetTextureReference,
 * \ref ::cudaBindTexture(size_t*, const struct textureReference*, const void*, const struct cudaChannelFormatDesc*, size_t) "cudaBindTexture (C API)",
 * \ref ::cudaBindTexture2D(size_t*, const struct textureReference*, const void*, const struct cudaChannelFormatDesc*, size_t, size_t, size_t) "cudaBindTexture2D (C API)",
 * \ref ::cudaBindTextureToArray(const struct textureReference*, cudaArray_const_t, const struct cudaChannelFormatDesc*) "cudaBindTextureToArray (C API)",
 * \ref ::cudaUnbindTexture(const struct texture< T, dim, readMode>&) "cudaUnbindTexture (C++ API)",
 * \ref ::cudaGetTextureAlignmentOffset(size_t*, const struct textureReference*) "cudaGetTextureAlignmentOffset (C API)"
 */
    pub fn cudaUnbindTexture(texref: *const textureReference) -> cudaError_t;
}
extern "C" {
    /**
 * \brief Get the alignment offset of a texture
 *
 * Returns in \p *offset the offset that was returned when texture reference
 * \p texref was bound.
 *
 * \param offset - Offset of texture reference in bytes
 * \param texref - Texture to get offset of
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidTexture,
 * ::cudaErrorInvalidTextureBinding
 * \notefnerr
 *
 * \sa \ref ::cudaCreateChannelDesc(int, int, int, int, cudaChannelFormatKind) "cudaCreateChannelDesc (C API)",
 * ::cudaGetChannelDesc, ::cudaGetTextureReference,
 * \ref ::cudaBindTexture(size_t*, const struct textureReference*, const void*, const struct cudaChannelFormatDesc*, size_t) "cudaBindTexture (C API)",
 * \ref ::cudaBindTexture2D(size_t*, const struct textureReference*, const void*, const struct cudaChannelFormatDesc*, size_t, size_t, size_t) "cudaBindTexture2D (C API)",
 * \ref ::cudaBindTextureToArray(const struct textureReference*, cudaArray_const_t, const struct cudaChannelFormatDesc*) "cudaBindTextureToArray (C API)",
 * \ref ::cudaUnbindTexture(const struct textureReference*) "cudaUnbindTexture (C API)",
 * \ref ::cudaGetTextureAlignmentOffset(size_t*, const struct texture< T, dim, readMode>&) "cudaGetTextureAlignmentOffset (C++ API)"
 */
    pub fn cudaGetTextureAlignmentOffset(offset: *mut usize,
                                         texref: *const textureReference)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Get the texture reference associated with a symbol
 *
 * Returns in \p *texref the structure associated to the texture reference
 * defined by symbol \p symbol.
 *
 * \param texref - Texture reference associated with symbol
 * \param symbol - Texture to get reference for
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidTexture
 * \notefnerr
 * \note_string_api_deprecation_50
 *
 * \sa \ref ::cudaCreateChannelDesc(int, int, int, int, cudaChannelFormatKind) "cudaCreateChannelDesc (C API)",
 * ::cudaGetChannelDesc,
 * \ref ::cudaGetTextureAlignmentOffset(size_t*, const struct textureReference*) "cudaGetTextureAlignmentOffset (C API)",
 * \ref ::cudaBindTexture(size_t*, const struct textureReference*, const void*, const struct cudaChannelFormatDesc*, size_t) "cudaBindTexture (C API)",
 * \ref ::cudaBindTexture2D(size_t*, const struct textureReference*, const void*, const struct cudaChannelFormatDesc*, size_t, size_t, size_t) "cudaBindTexture2D (C API)",
 * \ref ::cudaBindTextureToArray(const struct textureReference*, cudaArray_const_t, const struct cudaChannelFormatDesc*) "cudaBindTextureToArray (C API)",
 * \ref ::cudaUnbindTexture(const struct textureReference*) "cudaUnbindTexture (C API)"
 */
    pub fn cudaGetTextureReference(texref: *mut *const textureReference,
                                   symbol: *const ::libc::c_void)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Binds an array to a surface
 *
 * Binds the CUDA array \p array to the surface reference \p surfref.
 * \p desc describes how the memory is interpreted when fetching values from
 * the surface. Any CUDA array previously bound to \p surfref is unbound.
 *
 * \param surfref - Surface to bind
 * \param array  - Memory array on device
 * \param desc   - Channel format
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue,
 * ::cudaErrorInvalidSurface
 * \notefnerr
 *
 * \sa \ref ::cudaBindSurfaceToArray(const struct surface< T, dim>&, cudaArray_const_t, const struct cudaChannelFormatDesc&) "cudaBindSurfaceToArray (C++ API)",
 * \ref ::cudaBindSurfaceToArray(const struct surface< T, dim>&, cudaArray_const_t) "cudaBindSurfaceToArray (C++ API, inherited channel descriptor)",
 * ::cudaGetSurfaceReference
 */
    pub fn cudaBindSurfaceToArray(surfref: *const surfaceReference,
                                  array: cudaArray_const_t,
                                  desc: *const cudaChannelFormatDesc)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Get the surface reference associated with a symbol
 *
 * Returns in \p *surfref the structure associated to the surface reference
 * defined by symbol \p symbol.
 *
 * \param surfref - Surface reference associated with symbol
 * \param symbol - Surface to get reference for
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidSurface
 * \notefnerr
 * \note_string_api_deprecation_50
 *
 * \sa \ref ::cudaBindSurfaceToArray(const struct surfaceReference*, cudaArray_const_t, const struct cudaChannelFormatDesc*) "cudaBindSurfaceToArray (C API)"
 */
    pub fn cudaGetSurfaceReference(surfref: *mut *const surfaceReference,
                                   symbol: *const ::libc::c_void)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Creates a texture object
 *
 * Creates a texture object and returns it in \p pTexObject. \p pResDesc describes
 * the data to texture from. \p pTexDesc describes how the data should be sampled.
 * \p pResViewDesc is an optional argument that specifies an alternate format for
 * the data described by \p pResDesc, and also describes the subresource region
 * to restrict access to when texturing. \p pResViewDesc can only be specified if
 * the type of resource is a CUDA array or a CUDA mipmapped array.
 *
 * Texture objects are only supported on devices of compute capability 3.0 or higher.
 * Additionally, a texture object is an opaque value, and, as such, should only be
 * accessed through CUDA API calls.
 *
 * The ::cudaResourceDesc structure is defined as:
 * \code
        struct cudaResourceDesc {
	        enum cudaResourceType resType;

	        union {
		        struct {
			        cudaArray_t array;
		        } array;
                struct {
                    cudaMipmappedArray_t mipmap;
                } mipmap;
		        struct {
			        void *devPtr;
			        struct cudaChannelFormatDesc desc;
			        size_t sizeInBytes;
		        } linear;
		        struct {
			        void *devPtr;
			        struct cudaChannelFormatDesc desc;
			        size_t width;
			        size_t height;
			        size_t pitchInBytes;
		        } pitch2D;
	        } res;
        };
 * \endcode
 * where:
 * - ::cudaResourceDesc::resType specifies the type of resource to texture from.
 * CUresourceType is defined as:
 * \code
        enum cudaResourceType {
            cudaResourceTypeArray          = 0x00,
            cudaResourceTypeMipmappedArray = 0x01,
            cudaResourceTypeLinear         = 0x02,
            cudaResourceTypePitch2D        = 0x03
        };
 * \endcode
 *
 * \par
 * If ::cudaResourceDesc::resType is set to ::cudaResourceTypeArray, ::cudaResourceDesc::res::array::array
 * must be set to a valid CUDA array handle.
 *
 * \par
 * If ::cudaResourceDesc::resType is set to ::cudaResourceTypeMipmappedArray, ::cudaResourceDesc::res::mipmap::mipmap
 * must be set to a valid CUDA mipmapped array handle and ::cudaTextureDesc::normalizedCoords must be set to true.
 *
 * \par
 * If ::cudaResourceDesc::resType is set to ::cudaResourceTypeLinear, ::cudaResourceDesc::res::linear::devPtr
 * must be set to a valid device pointer, that is aligned to ::cudaDeviceProp::textureAlignment.
 * ::cudaResourceDesc::res::linear::desc describes the format and the number of components per array element. ::cudaResourceDesc::res::linear::sizeInBytes
 * specifies the size of the array in bytes. The total number of elements in the linear address range cannot exceed
 * ::cudaDeviceProp::maxTexture1DLinear. The number of elements is computed as (sizeInBytes / sizeof(desc)).
 *
 * \par
 * If ::cudaResourceDesc::resType is set to ::cudaResourceTypePitch2D, ::cudaResourceDesc::res::pitch2D::devPtr
 * must be set to a valid device pointer, that is aligned to ::cudaDeviceProp::textureAlignment.
 * ::cudaResourceDesc::res::pitch2D::desc describes the format and the number of components per array element. ::cudaResourceDesc::res::pitch2D::width
 * and ::cudaResourceDesc::res::pitch2D::height specify the width and height of the array in elements, and cannot exceed
 * ::cudaDeviceProp::maxTexture2DLinear[0] and ::cudaDeviceProp::maxTexture2DLinear[1] respectively.
 * ::cudaResourceDesc::res::pitch2D::pitchInBytes specifies the pitch between two rows in bytes and has to be aligned to
 * ::cudaDeviceProp::texturePitchAlignment. Pitch cannot exceed ::cudaDeviceProp::maxTexture2DLinear[2].
 *
 *
 * The ::cudaTextureDesc struct is defined as
 * \code
        struct cudaTextureDesc {
            enum cudaTextureAddressMode addressMode[3];
            enum cudaTextureFilterMode  filterMode;
            enum cudaTextureReadMode    readMode;
            int                         sRGB;
            float                       borderColor[4];
            int                         normalizedCoords;
            unsigned int                maxAnisotropy;
            enum cudaTextureFilterMode  mipmapFilterMode;
            float                       mipmapLevelBias;
            float                       minMipmapLevelClamp;
            float                       maxMipmapLevelClamp;
        };
 * \endcode
 * where
 * - ::cudaTextureDesc::addressMode specifies the addressing mode for each dimension of the texture data. ::cudaTextureAddressMode is defined as:
 *   \code
        enum cudaTextureAddressMode {
            cudaAddressModeWrap   = 0,
            cudaAddressModeClamp  = 1,
            cudaAddressModeMirror = 2,
            cudaAddressModeBorder = 3
        };
 *   \endcode
 *   This is ignored if ::cudaResourceDesc::resType is ::cudaResourceTypeLinear. Also, if ::cudaTextureDesc::normalizedCoords
 *   is set to zero, ::cudaAddressModeWrap and ::cudaAddressModeMirror won't be supported and will be switched to ::cudaAddressModeClamp.
 *
 * - ::cudaTextureDesc::filterMode specifies the filtering mode to be used when fetching from the texture. ::cudaTextureFilterMode is defined as:
 *   \code
        enum cudaTextureFilterMode {
            cudaFilterModePoint  = 0,
            cudaFilterModeLinear = 1
        };
 *   \endcode
 *   This is ignored if ::cudaResourceDesc::resType is ::cudaResourceTypeLinear.
 *
 * - ::cudaTextureDesc::readMode specifies whether integer data should be converted to floating point or not. ::cudaTextureReadMode is defined as:
 *   \code
        enum cudaTextureReadMode {
            cudaReadModeElementType     = 0,
            cudaReadModeNormalizedFloat = 1
        };
 *   \endcode
 *   Note that this applies only to 8-bit and 16-bit integer formats. 32-bit integer format would not be promoted, regardless of
 *   whether or not this ::cudaTextureDesc::readMode is set ::cudaReadModeNormalizedFloat is specified.
 *
 * - ::cudaTextureDesc::sRGB specifies whether sRGB to linear conversion should be performed during texture fetch.
 *
 * - ::cudaTextureDesc::borderColor specifies the float values of color. where:
 *   ::cudaTextureDesc::borderColor[0] contains value of 'R',
 *   ::cudaTextureDesc::borderColor[1] contains value of 'G',
 *   ::cudaTextureDesc::borderColor[2] contains value of 'B',
 *   ::cudaTextureDesc::borderColor[3] contains value of 'A'
 *   Note that application using integer border color values will need to <reinterpret_cast> these values to float.
 *   The values are set only when the addressing mode specified by ::cudaTextureDesc::addressMode is cudaAddressModeBorder.
 *
 * - ::cudaTextureDesc::normalizedCoords specifies whether the texture coordinates will be normalized or not.
 *
 * - ::cudaTextureDesc::maxAnisotropy specifies the maximum anistropy ratio to be used when doing anisotropic filtering. This value will be
 *   clamped to the range [1,16].
 *
 * - ::cudaTextureDesc::mipmapFilterMode specifies the filter mode when the calculated mipmap level lies between two defined mipmap levels.
 *
 * - ::cudaTextureDesc::mipmapLevelBias specifies the offset to be applied to the calculated mipmap level.
 *
 * - ::cudaTextureDesc::minMipmapLevelClamp specifies the lower end of the mipmap level range to clamp access to.
 *
 * - ::cudaTextureDesc::maxMipmapLevelClamp specifies the upper end of the mipmap level range to clamp access to.
 *
 *
 * The ::cudaResourceViewDesc struct is defined as
 * \code
        struct cudaResourceViewDesc {
            enum cudaResourceViewFormat format;
            size_t                      width;
            size_t                      height;
            size_t                      depth;
            unsigned int                firstMipmapLevel;
            unsigned int                lastMipmapLevel;
            unsigned int                firstLayer;
            unsigned int                lastLayer;
        };
 * \endcode
 * where:
 * - ::cudaResourceViewDesc::format specifies how the data contained in the CUDA array or CUDA mipmapped array should
 *   be interpreted. Note that this can incur a change in size of the texture data. If the resource view format is a block
 *   compressed format, then the underlying CUDA array or CUDA mipmapped array has to have a 32-bit unsigned integer format
 *   with 2 or 4 channels, depending on the block compressed format. For ex., BC1 and BC4 require the underlying CUDA array to have
 *   a 32-bit unsigned int with 2 channels. The other BC formats require the underlying resource to have the same 32-bit unsigned int
 *   format but with 4 channels.
 *
 * - ::cudaResourceViewDesc::width specifies the new width of the texture data. If the resource view format is a block
 *   compressed format, this value has to be 4 times the original width of the resource. For non block compressed formats,
 *   this value has to be equal to that of the original resource.
 *
 * - ::cudaResourceViewDesc::height specifies the new height of the texture data. If the resource view format is a block
 *   compressed format, this value has to be 4 times the original height of the resource. For non block compressed formats,
 *   this value has to be equal to that of the original resource.
 *
 * - ::cudaResourceViewDesc::depth specifies the new depth of the texture data. This value has to be equal to that of the
 *   original resource.
 *
 * - ::cudaResourceViewDesc::firstMipmapLevel specifies the most detailed mipmap level. This will be the new mipmap level zero.
 *   For non-mipmapped resources, this value has to be zero.::cudaTextureDesc::minMipmapLevelClamp and ::cudaTextureDesc::maxMipmapLevelClamp
 *   will be relative to this value. For ex., if the firstMipmapLevel is set to 2, and a minMipmapLevelClamp of 1.2 is specified,
 *   then the actual minimum mipmap level clamp will be 3.2.
 *
 * - ::cudaResourceViewDesc::lastMipmapLevel specifies the least detailed mipmap level. For non-mipmapped resources, this value
 *   has to be zero.
 *
 * - ::cudaResourceViewDesc::firstLayer specifies the first layer index for layered textures. This will be the new layer zero.
 *   For non-layered resources, this value has to be zero.
 *
 * - ::cudaResourceViewDesc::lastLayer specifies the last layer index for layered textures. For non-layered resources,
 *   this value has to be zero.
 *
 *
 * \param pTexObject   - Texture object to create
 * \param pResDesc     - Resource descriptor
 * \param pTexDesc     - Texture descriptor
 * \param pResViewDesc - Resource view descriptor
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue
 *
 * \sa ::cudaDestroyTextureObject
 */
    pub fn cudaCreateTextureObject(pTexObject: *mut cudaTextureObject_t,
                                   pResDesc: *const cudaResourceDesc,
                                   pTexDesc: *const cudaTextureDesc,
                                   pResViewDesc: *const cudaResourceViewDesc)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Destroys a texture object
 *
 * Destroys the texture object specified by \p texObject.
 *
 * \param texObject - Texture object to destroy
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue
 *
 * \sa ::cudaCreateTextureObject
 */
    pub fn cudaDestroyTextureObject(texObject: cudaTextureObject_t)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Returns a texture object's resource descriptor
 *
 * Returns the resource descriptor for the texture object specified by \p texObject.
 *
 * \param pResDesc  - Resource descriptor
 * \param texObject - Texture object
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue
 *
 * \sa ::cudaCreateTextureObject
 */
    pub fn cudaGetTextureObjectResourceDesc(pResDesc: *mut cudaResourceDesc,
                                            texObject: cudaTextureObject_t)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Returns a texture object's texture descriptor
 *
 * Returns the texture descriptor for the texture object specified by \p texObject.
 *
 * \param pTexDesc  - Texture descriptor
 * \param texObject - Texture object
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue
 *
 * \sa ::cudaCreateTextureObject
 */
    pub fn cudaGetTextureObjectTextureDesc(pTexDesc: *mut cudaTextureDesc,
                                           texObject: cudaTextureObject_t)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Returns a texture object's resource view descriptor
 *
 * Returns the resource view descriptor for the texture object specified by \p texObject.
 * If no resource view was specified, ::cudaErrorInvalidValue is returned.
 *
 * \param pResViewDesc - Resource view descriptor
 * \param texObject    - Texture object
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue
 *
 * \sa ::cudaCreateTextureObject
 */
    pub fn cudaGetTextureObjectResourceViewDesc(pResViewDesc:
                                                    *mut cudaResourceViewDesc,
                                                texObject:
                                                    cudaTextureObject_t)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Creates a surface object
 *
 * Creates a surface object and returns it in \p pSurfObject. \p pResDesc describes
 * the data to perform surface load/stores on. ::cudaResourceDesc::resType must be
 * ::cudaResourceTypeArray and  ::cudaResourceDesc::res::array::array
 * must be set to a valid CUDA array handle.
 *
 * Surface objects are only supported on devices of compute capability 3.0 or higher.
 * Additionally, a surface object is an opaque value, and, as such, should only be
 * accessed through CUDA API calls.
 *
 * \param pSurfObject - Surface object to create
 * \param pResDesc    - Resource descriptor
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue
 *
 * \sa ::cudaDestroySurfaceObject
 */
    pub fn cudaCreateSurfaceObject(pSurfObject: *mut cudaSurfaceObject_t,
                                   pResDesc: *const cudaResourceDesc)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Destroys a surface object
 *
 * Destroys the surface object specified by \p surfObject.
 *
 * \param surfObject - Surface object to destroy
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue
 *
 * \sa ::cudaCreateSurfaceObject
 */
    pub fn cudaDestroySurfaceObject(surfObject: cudaSurfaceObject_t)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Returns a surface object's resource descriptor
 * Returns the resource descriptor for the surface object specified by \p surfObject.
 *
 * \param pResDesc   - Resource descriptor
 * \param surfObject - Surface object
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue
 *
 * \sa ::cudaCreateSurfaceObject
 */
    pub fn cudaGetSurfaceObjectResourceDesc(pResDesc: *mut cudaResourceDesc,
                                            surfObject: cudaSurfaceObject_t)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Returns the CUDA driver version
 *
 * Returns in \p *driverVersion the version number of the installed CUDA
 * driver. If no driver is installed, then 0 is returned as the driver
 * version (via \p driverVersion). This function automatically returns
 * ::cudaErrorInvalidValue if the \p driverVersion argument is NULL.
 *
 * \param driverVersion - Returns the CUDA driver version.
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue
 * \notefnerr
 *
 * \sa ::cudaRuntimeGetVersion
 */
    pub fn cudaDriverGetVersion(driverVersion: *mut ::libc::c_int)
     -> cudaError_t;
}
extern "C" {
    /**
 * \brief Returns the CUDA Runtime version
 *
 * Returns in \p *runtimeVersion the version number of the installed CUDA
 * Runtime. This function automatically returns ::cudaErrorInvalidValue if
 * the \p runtimeVersion argument is NULL.
 *
 * \param runtimeVersion - Returns the CUDA Runtime version.
 *
 * \return
 * ::cudaSuccess,
 * ::cudaErrorInvalidValue
 *
 * \sa ::cudaDriverGetVersion
 */
    pub fn cudaRuntimeGetVersion(runtimeVersion: *mut ::libc::c_int)
     -> cudaError_t;
}
extern "C" {
    /** \cond impl_private */
    pub fn cudaGetExportTable(ppExportTable:
                                  *mut *const ::libc::c_void,
                              pExportTableId: *const cudaUUID_t)
     -> cudaError_t;
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudnnContext([u8; 0]);
pub type cudnnHandle_t = *mut cudnnContext;
extern "C" {
    pub fn cudnnGetVersion() -> usize;
}
pub const CUDNN_STATUS_SUCCESS: _bindgen_ty_2 =
    _bindgen_ty_2::CUDNN_STATUS_SUCCESS;
pub const CUDNN_STATUS_NOT_INITIALIZED: _bindgen_ty_2 =
    _bindgen_ty_2::CUDNN_STATUS_NOT_INITIALIZED;
pub const CUDNN_STATUS_ALLOC_FAILED: _bindgen_ty_2 =
    _bindgen_ty_2::CUDNN_STATUS_ALLOC_FAILED;
pub const CUDNN_STATUS_BAD_PARAM: _bindgen_ty_2 =
    _bindgen_ty_2::CUDNN_STATUS_BAD_PARAM;
pub const CUDNN_STATUS_INTERNAL_ERROR: _bindgen_ty_2 =
    _bindgen_ty_2::CUDNN_STATUS_INTERNAL_ERROR;
pub const CUDNN_STATUS_INVALID_VALUE: _bindgen_ty_2 =
    _bindgen_ty_2::CUDNN_STATUS_INVALID_VALUE;
pub const CUDNN_STATUS_ARCH_MISMATCH: _bindgen_ty_2 =
    _bindgen_ty_2::CUDNN_STATUS_ARCH_MISMATCH;
pub const CUDNN_STATUS_MAPPING_ERROR: _bindgen_ty_2 =
    _bindgen_ty_2::CUDNN_STATUS_MAPPING_ERROR;
pub const CUDNN_STATUS_EXECUTION_FAILED: _bindgen_ty_2 =
    _bindgen_ty_2::CUDNN_STATUS_EXECUTION_FAILED;
pub const CUDNN_STATUS_NOT_SUPPORTED: _bindgen_ty_2 =
    _bindgen_ty_2::CUDNN_STATUS_NOT_SUPPORTED;
pub const CUDNN_STATUS_LICENSE_ERROR: _bindgen_ty_2 =
    _bindgen_ty_2::CUDNN_STATUS_LICENSE_ERROR;
#[repr(u32)]
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum _bindgen_ty_2 {
    CUDNN_STATUS_SUCCESS = 0,
    CUDNN_STATUS_NOT_INITIALIZED = 1,
    CUDNN_STATUS_ALLOC_FAILED = 2,
    CUDNN_STATUS_BAD_PARAM = 3,
    CUDNN_STATUS_INTERNAL_ERROR = 4,
    CUDNN_STATUS_INVALID_VALUE = 5,
    CUDNN_STATUS_ARCH_MISMATCH = 6,
    CUDNN_STATUS_MAPPING_ERROR = 7,
    CUDNN_STATUS_EXECUTION_FAILED = 8,
    CUDNN_STATUS_NOT_SUPPORTED = 9,
    CUDNN_STATUS_LICENSE_ERROR = 10,
}
pub use self::_bindgen_ty_2 as cudnnStatus_t;
extern "C" {
    pub fn cudnnGetErrorString(status: cudnnStatus_t)
     -> *const ::libc::c_char;
}
extern "C" {
    pub fn cudnnCreate(handle: *mut cudnnHandle_t) -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnDestroy(handle: cudnnHandle_t) -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnSetStream(handle: cudnnHandle_t, streamId: cudaStream_t)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnGetStream(handle: cudnnHandle_t, streamId: *mut cudaStream_t)
     -> cudnnStatus_t;
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudnnTensorStruct([u8; 0]);
pub type cudnnTensorDescriptor_t = *mut cudnnTensorStruct;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudnnConvolutionStruct([u8; 0]);
pub type cudnnConvolutionDescriptor_t = *mut cudnnConvolutionStruct;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudnnPoolingStruct([u8; 0]);
pub type cudnnPoolingDescriptor_t = *mut cudnnPoolingStruct;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudnnFilterStruct([u8; 0]);
pub type cudnnFilterDescriptor_t = *mut cudnnFilterStruct;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudnnLRNStruct([u8; 0]);
pub type cudnnLRNDescriptor_t = *mut cudnnLRNStruct;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudnnActivationStruct([u8; 0]);
pub type cudnnActivationDescriptor_t = *mut cudnnActivationStruct;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudnnSpatialTransformerStruct([u8; 0]);
pub type cudnnSpatialTransformerDescriptor_t =
    *mut cudnnSpatialTransformerStruct;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudnnOpTensorStruct([u8; 0]);
pub type cudnnOpTensorDescriptor_t = *mut cudnnOpTensorStruct;
pub const CUDNN_DATA_FLOAT: _bindgen_ty_3 = _bindgen_ty_3::CUDNN_DATA_FLOAT;
pub const CUDNN_DATA_DOUBLE: _bindgen_ty_3 = _bindgen_ty_3::CUDNN_DATA_DOUBLE;
pub const CUDNN_DATA_HALF: _bindgen_ty_3 = _bindgen_ty_3::CUDNN_DATA_HALF;
#[repr(u32)]
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum _bindgen_ty_3 {
    CUDNN_DATA_FLOAT = 0,
    CUDNN_DATA_DOUBLE = 1,
    CUDNN_DATA_HALF = 2,
}
pub use self::_bindgen_ty_3 as cudnnDataType_t;
pub const CUDNN_NOT_PROPAGATE_NAN: _bindgen_ty_4 =
    _bindgen_ty_4::CUDNN_NOT_PROPAGATE_NAN;
pub const CUDNN_PROPAGATE_NAN: _bindgen_ty_4 =
    _bindgen_ty_4::CUDNN_PROPAGATE_NAN;
#[repr(u32)]
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum _bindgen_ty_4 {
    CUDNN_NOT_PROPAGATE_NAN = 0,
    CUDNN_PROPAGATE_NAN = 1,
}
pub use self::_bindgen_ty_4 as cudnnNanPropagation_t;
extern "C" {
    pub fn cudnnCreateTensorDescriptor(tensorDesc:
                                           *mut cudnnTensorDescriptor_t)
     -> cudnnStatus_t;
}
pub const CUDNN_TENSOR_NCHW: _bindgen_ty_5 = _bindgen_ty_5::CUDNN_TENSOR_NCHW;
pub const CUDNN_TENSOR_NHWC: _bindgen_ty_5 = _bindgen_ty_5::CUDNN_TENSOR_NHWC;
#[repr(u32)]
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum _bindgen_ty_5 { CUDNN_TENSOR_NCHW = 0, CUDNN_TENSOR_NHWC = 1, }
pub use self::_bindgen_ty_5 as cudnnTensorFormat_t;
extern "C" {
    pub fn cudnnSetTensor4dDescriptor(tensorDesc: cudnnTensorDescriptor_t,
                                      format: cudnnTensorFormat_t,
                                      dataType: cudnnDataType_t,
                                      n: ::libc::c_int,
                                      c: ::libc::c_int,
                                      h: ::libc::c_int,
                                      w: ::libc::c_int)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnSetTensor4dDescriptorEx(tensorDesc: cudnnTensorDescriptor_t,
                                        dataType: cudnnDataType_t,
                                        n: ::libc::c_int,
                                        c: ::libc::c_int,
                                        h: ::libc::c_int,
                                        w: ::libc::c_int,
                                        nStride: ::libc::c_int,
                                        cStride: ::libc::c_int,
                                        hStride: ::libc::c_int,
                                        wStride: ::libc::c_int)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnGetTensor4dDescriptor(tensorDesc: cudnnTensorDescriptor_t,
                                      dataType: *mut cudnnDataType_t,
                                      n: *mut ::libc::c_int,
                                      c: *mut ::libc::c_int,
                                      h: *mut ::libc::c_int,
                                      w: *mut ::libc::c_int,
                                      nStride: *mut ::libc::c_int,
                                      cStride: *mut ::libc::c_int,
                                      hStride: *mut ::libc::c_int,
                                      wStride: *mut ::libc::c_int)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnSetTensorNdDescriptor(tensorDesc: cudnnTensorDescriptor_t,
                                      dataType: cudnnDataType_t,
                                      nbDims: ::libc::c_int,
                                      dimA: *const ::libc::c_int,
                                      strideA: *const ::libc::c_int)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnGetTensorNdDescriptor(tensorDesc: cudnnTensorDescriptor_t,
                                      nbDimsRequested: ::libc::c_int,
                                      dataType: *mut cudnnDataType_t,
                                      nbDims: *mut ::libc::c_int,
                                      dimA: *mut ::libc::c_int,
                                      strideA: *mut ::libc::c_int)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnDestroyTensorDescriptor(tensorDesc: cudnnTensorDescriptor_t)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnTransformTensor(handle: cudnnHandle_t,
                                alpha: *const ::libc::c_void,
                                xDesc: cudnnTensorDescriptor_t,
                                x: *const ::libc::c_void,
                                beta: *const ::libc::c_void,
                                yDesc: cudnnTensorDescriptor_t,
                                y: *mut ::libc::c_void)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnAddTensor(handle: cudnnHandle_t,
                          alpha: *const ::libc::c_void,
                          aDesc: cudnnTensorDescriptor_t,
                          A: *const ::libc::c_void,
                          beta: *const ::libc::c_void,
                          cDesc: cudnnTensorDescriptor_t,
                          C: *mut ::libc::c_void) -> cudnnStatus_t;
}
pub const CUDNN_OP_TENSOR_ADD: _bindgen_ty_6 =
    _bindgen_ty_6::CUDNN_OP_TENSOR_ADD;
pub const CUDNN_OP_TENSOR_MUL: _bindgen_ty_6 =
    _bindgen_ty_6::CUDNN_OP_TENSOR_MUL;
pub const CUDNN_OP_TENSOR_MIN: _bindgen_ty_6 =
    _bindgen_ty_6::CUDNN_OP_TENSOR_MIN;
pub const CUDNN_OP_TENSOR_MAX: _bindgen_ty_6 =
    _bindgen_ty_6::CUDNN_OP_TENSOR_MAX;
#[repr(u32)]
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum _bindgen_ty_6 {
    CUDNN_OP_TENSOR_ADD = 0,
    CUDNN_OP_TENSOR_MUL = 1,
    CUDNN_OP_TENSOR_MIN = 2,
    CUDNN_OP_TENSOR_MAX = 3,
}
pub use self::_bindgen_ty_6 as cudnnOpTensorOp_t;
extern "C" {
    pub fn cudnnCreateOpTensorDescriptor(opTensorDesc:
                                             *mut cudnnOpTensorDescriptor_t)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnSetOpTensorDescriptor(opTensorDesc: cudnnOpTensorDescriptor_t,
                                      opTensorOp: cudnnOpTensorOp_t,
                                      opTensorCompType: cudnnDataType_t,
                                      opTensorNanOpt: cudnnNanPropagation_t)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnGetOpTensorDescriptor(opTensorDesc: cudnnOpTensorDescriptor_t,
                                      opTensorOp: *mut cudnnOpTensorOp_t,
                                      opTensorCompType: *mut cudnnDataType_t,
                                      opTensorNanOpt:
                                          *mut cudnnNanPropagation_t)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnDestroyOpTensorDescriptor(opTensorDesc:
                                              cudnnOpTensorDescriptor_t)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnOpTensor(handle: cudnnHandle_t,
                         opTensorDesc: cudnnOpTensorDescriptor_t,
                         alpha1: *const ::libc::c_void,
                         aDesc: cudnnTensorDescriptor_t,
                         A: *const ::libc::c_void,
                         alpha2: *const ::libc::c_void,
                         bDesc: cudnnTensorDescriptor_t,
                         B: *const ::libc::c_void,
                         beta: *const ::libc::c_void,
                         cDesc: cudnnTensorDescriptor_t,
                         C: *mut ::libc::c_void) -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnSetTensor(handle: cudnnHandle_t,
                          yDesc: cudnnTensorDescriptor_t,
                          y: *mut ::libc::c_void,
                          valuePtr: *const ::libc::c_void)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnScaleTensor(handle: cudnnHandle_t,
                            yDesc: cudnnTensorDescriptor_t,
                            y: *mut ::libc::c_void,
                            alpha: *const ::libc::c_void)
     -> cudnnStatus_t;
}
pub const CUDNN_CONVOLUTION: _bindgen_ty_7 = _bindgen_ty_7::CUDNN_CONVOLUTION;
pub const CUDNN_CROSS_CORRELATION: _bindgen_ty_7 =
    _bindgen_ty_7::CUDNN_CROSS_CORRELATION;
#[repr(u32)]
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum _bindgen_ty_7 { CUDNN_CONVOLUTION = 0, CUDNN_CROSS_CORRELATION = 1, }
pub use self::_bindgen_ty_7 as cudnnConvolutionMode_t;
extern "C" {
    pub fn cudnnCreateFilterDescriptor(filterDesc:
                                           *mut cudnnFilterDescriptor_t)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnSetFilter4dDescriptor(filterDesc: cudnnFilterDescriptor_t,
                                      dataType: cudnnDataType_t,
                                      format: cudnnTensorFormat_t,
                                      k: ::libc::c_int,
                                      c: ::libc::c_int,
                                      h: ::libc::c_int,
                                      w: ::libc::c_int)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnGetFilter4dDescriptor(filterDesc: cudnnFilterDescriptor_t,
                                      dataType: *mut cudnnDataType_t,
                                      format: *mut cudnnTensorFormat_t,
                                      k: *mut ::libc::c_int,
                                      c: *mut ::libc::c_int,
                                      h: *mut ::libc::c_int,
                                      w: *mut ::libc::c_int)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnSetFilterNdDescriptor(filterDesc: cudnnFilterDescriptor_t,
                                      dataType: cudnnDataType_t,
                                      format: cudnnTensorFormat_t,
                                      nbDims: ::libc::c_int,
                                      filterDimA:
                                          *const ::libc::c_int)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnGetFilterNdDescriptor(filterDesc: cudnnFilterDescriptor_t,
                                      nbDimsRequested: ::libc::c_int,
                                      dataType: *mut cudnnDataType_t,
                                      format: *mut cudnnTensorFormat_t,
                                      nbDims: *mut ::libc::c_int,
                                      filterDimA: *mut ::libc::c_int)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnDestroyFilterDescriptor(filterDesc: cudnnFilterDescriptor_t)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnCreateConvolutionDescriptor(convDesc:
                                                *mut cudnnConvolutionDescriptor_t)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnSetConvolution2dDescriptor(convDesc:
                                               cudnnConvolutionDescriptor_t,
                                           pad_h: ::libc::c_int,
                                           pad_w: ::libc::c_int,
                                           u: ::libc::c_int,
                                           v: ::libc::c_int,
                                           upscalex: ::libc::c_int,
                                           upscaley: ::libc::c_int,
                                           mode: cudnnConvolutionMode_t)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnSetConvolution2dDescriptor_v5(convDesc:
                                                  cudnnConvolutionDescriptor_t,
                                              pad_h: ::libc::c_int,
                                              pad_w: ::libc::c_int,
                                              u: ::libc::c_int,
                                              v: ::libc::c_int,
                                              upscalex: ::libc::c_int,
                                              upscaley: ::libc::c_int,
                                              mode: cudnnConvolutionMode_t,
                                              dataType: cudnnDataType_t)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnGetConvolution2dDescriptor(convDesc:
                                               cudnnConvolutionDescriptor_t,
                                           pad_h: *mut ::libc::c_int,
                                           pad_w: *mut ::libc::c_int,
                                           u: *mut ::libc::c_int,
                                           v: *mut ::libc::c_int,
                                           upscalex:
                                               *mut ::libc::c_int,
                                           upscaley:
                                               *mut ::libc::c_int,
                                           mode: *mut cudnnConvolutionMode_t)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnGetConvolution2dDescriptor_v5(convDesc:
                                                  cudnnConvolutionDescriptor_t,
                                              pad_h:
                                                  *mut ::libc::c_int,
                                              pad_w:
                                                  *mut ::libc::c_int,
                                              u: *mut ::libc::c_int,
                                              v: *mut ::libc::c_int,
                                              upscalex:
                                                  *mut ::libc::c_int,
                                              upscaley:
                                                  *mut ::libc::c_int,
                                              mode:
                                                  *mut cudnnConvolutionMode_t,
                                              dataType: *mut cudnnDataType_t)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnGetConvolution2dForwardOutputDim(convDesc:
                                                     cudnnConvolutionDescriptor_t,
                                                 inputTensorDesc:
                                                     cudnnTensorDescriptor_t,
                                                 filterDesc:
                                                     cudnnFilterDescriptor_t,
                                                 n:
                                                     *mut ::libc::c_int,
                                                 c:
                                                     *mut ::libc::c_int,
                                                 h:
                                                     *mut ::libc::c_int,
                                                 w:
                                                     *mut ::libc::c_int)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnSetConvolutionNdDescriptor(convDesc:
                                               cudnnConvolutionDescriptor_t,
                                           arrayLength: ::libc::c_int,
                                           padA: *const ::libc::c_int,
                                           filterStrideA:
                                               *const ::libc::c_int,
                                           upscaleA:
                                               *const ::libc::c_int,
                                           mode: cudnnConvolutionMode_t,
                                           dataType: cudnnDataType_t)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnGetConvolutionNdDescriptor(convDesc:
                                               cudnnConvolutionDescriptor_t,
                                           arrayLengthRequested:
                                               ::libc::c_int,
                                           arrayLength:
                                               *mut ::libc::c_int,
                                           padA: *mut ::libc::c_int,
                                           strideA:
                                               *mut ::libc::c_int,
                                           upscaleA:
                                               *mut ::libc::c_int,
                                           mode: *mut cudnnConvolutionMode_t,
                                           dataType: *mut cudnnDataType_t)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnGetConvolutionNdForwardOutputDim(convDesc:
                                                     cudnnConvolutionDescriptor_t,
                                                 inputTensorDesc:
                                                     cudnnTensorDescriptor_t,
                                                 filterDesc:
                                                     cudnnFilterDescriptor_t,
                                                 nbDims:
                                                     ::libc::c_int,
                                                 tensorOuputDimA:
                                                     *mut ::libc::c_int)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnDestroyConvolutionDescriptor(convDesc:
                                                 cudnnConvolutionDescriptor_t)
     -> cudnnStatus_t;
}
pub const CUDNN_CONVOLUTION_FWD_NO_WORKSPACE: _bindgen_ty_8 =
    _bindgen_ty_8::CUDNN_CONVOLUTION_FWD_NO_WORKSPACE;
pub const CUDNN_CONVOLUTION_FWD_PREFER_FASTEST: _bindgen_ty_8 =
    _bindgen_ty_8::CUDNN_CONVOLUTION_FWD_PREFER_FASTEST;
pub const CUDNN_CONVOLUTION_FWD_SPECIFY_WORKSPACE_LIMIT: _bindgen_ty_8 =
    _bindgen_ty_8::CUDNN_CONVOLUTION_FWD_SPECIFY_WORKSPACE_LIMIT;
#[repr(u32)]
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum _bindgen_ty_8 {
    CUDNN_CONVOLUTION_FWD_NO_WORKSPACE = 0,
    CUDNN_CONVOLUTION_FWD_PREFER_FASTEST = 1,
    CUDNN_CONVOLUTION_FWD_SPECIFY_WORKSPACE_LIMIT = 2,
}
pub use self::_bindgen_ty_8 as cudnnConvolutionFwdPreference_t;
pub const CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM: _bindgen_ty_9 =
    _bindgen_ty_9::CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM;
pub const CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM: _bindgen_ty_9 =
    _bindgen_ty_9::CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM;
pub const CUDNN_CONVOLUTION_FWD_ALGO_GEMM: _bindgen_ty_9 =
    _bindgen_ty_9::CUDNN_CONVOLUTION_FWD_ALGO_GEMM;
pub const CUDNN_CONVOLUTION_FWD_ALGO_DIRECT: _bindgen_ty_9 =
    _bindgen_ty_9::CUDNN_CONVOLUTION_FWD_ALGO_DIRECT;
pub const CUDNN_CONVOLUTION_FWD_ALGO_FFT: _bindgen_ty_9 =
    _bindgen_ty_9::CUDNN_CONVOLUTION_FWD_ALGO_FFT;
pub const CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING: _bindgen_ty_9 =
    _bindgen_ty_9::CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING;
pub const CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD: _bindgen_ty_9 =
    _bindgen_ty_9::CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD;
pub const CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED: _bindgen_ty_9 =
    _bindgen_ty_9::CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED;
#[repr(u32)]
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum _bindgen_ty_9 {
    CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM = 0,
    CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM = 1,
    CUDNN_CONVOLUTION_FWD_ALGO_GEMM = 2,
    CUDNN_CONVOLUTION_FWD_ALGO_DIRECT = 3,
    CUDNN_CONVOLUTION_FWD_ALGO_FFT = 4,
    CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING = 5,
    CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD = 6,
    CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED = 7,
}
pub use self::_bindgen_ty_9 as cudnnConvolutionFwdAlgo_t;
#[repr(C)]
#[derive(Debug, Copy)]
pub struct _bindgen_ty_10 {
    pub algo: cudnnConvolutionFwdAlgo_t,
    pub status: cudnnStatus_t,
    pub time: f32,
    pub memory: usize,
}
#[test]
fn bindgen_test_layout__bindgen_ty_10() {
    assert_eq!(::std::mem::size_of::<_bindgen_ty_10>() , 24usize);
    assert_eq!(::std::mem::align_of::<_bindgen_ty_10>() , 8usize);
}
impl Clone for _bindgen_ty_10 {
    fn clone(&self) -> Self { *self }
}

pub type cudnnConvolutionFwdAlgoPerf_t = _bindgen_ty_10;

impl Default for cudnnConvolutionFwdAlgoPerf_t {
    fn default() -> Self {
        cudnnConvolutionFwdAlgoPerf_t {
            algo : CUDNN_CONVOLUTION_FWD_ALGO_DIRECT,
            status : CUDNN_STATUS_NOT_INITIALIZED,
            time : 0.0 as f32,
            memory : 0,
        }
    }
}

extern "C" {
    pub fn cudnnFindConvolutionForwardAlgorithm(handle: cudnnHandle_t,
                                                xDesc:
                                                    cudnnTensorDescriptor_t,
                                                wDesc:
                                                    cudnnFilterDescriptor_t,
                                                convDesc:
                                                    cudnnConvolutionDescriptor_t,
                                                yDesc:
                                                    cudnnTensorDescriptor_t,
                                                requestedAlgoCount:
                                                    ::libc::c_int,
                                                returnedAlgoCount:
                                                    *mut ::libc::c_int,
                                                perfResults:
                                                    *mut cudnnConvolutionFwdAlgoPerf_t)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnFindConvolutionForwardAlgorithmEx(handle: cudnnHandle_t,
                                                  xDesc:
                                                      cudnnTensorDescriptor_t,
                                                  x:
                                                      *const ::libc::c_void,
                                                  wDesc:
                                                      cudnnFilterDescriptor_t,
                                                  w:
                                                      *const ::libc::c_void,
                                                  convDesc:
                                                      cudnnConvolutionDescriptor_t,
                                                  yDesc:
                                                      cudnnTensorDescriptor_t,
                                                  y:
                                                      *mut ::libc::c_void,
                                                  requestedAlgoCount:
                                                      ::libc::c_int,
                                                  returnedAlgoCount:
                                                      *mut ::libc::c_int,
                                                  perfResults:
                                                      *mut cudnnConvolutionFwdAlgoPerf_t,
                                                  workSpace:
                                                      *mut ::libc::c_void,
                                                  workSpaceSizeInBytes: usize)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnGetConvolutionForwardAlgorithm(handle: cudnnHandle_t,
                                               xDesc: cudnnTensorDescriptor_t,
                                               wDesc: cudnnFilterDescriptor_t,
                                               convDesc:
                                                   cudnnConvolutionDescriptor_t,
                                               yDesc: cudnnTensorDescriptor_t,
                                               preference:
                                                   cudnnConvolutionFwdPreference_t,
                                               memoryLimitInBytes: usize,
                                               algo:
                                                   *mut cudnnConvolutionFwdAlgo_t)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnGetConvolutionForwardWorkspaceSize(handle: cudnnHandle_t,
                                                   xDesc:
                                                       cudnnTensorDescriptor_t,
                                                   wDesc:
                                                       cudnnFilterDescriptor_t,
                                                   convDesc:
                                                       cudnnConvolutionDescriptor_t,
                                                   yDesc:
                                                       cudnnTensorDescriptor_t,
                                                   algo:
                                                       cudnnConvolutionFwdAlgo_t,
                                                   sizeInBytes: *mut usize)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnConvolutionForward(handle: cudnnHandle_t,
                                   alpha: *const ::libc::c_void,
                                   xDesc: cudnnTensorDescriptor_t,
                                   x: *const ::libc::c_void,
                                   wDesc: cudnnFilterDescriptor_t,
                                   w: *const ::libc::c_void,
                                   convDesc: cudnnConvolutionDescriptor_t,
                                   algo: cudnnConvolutionFwdAlgo_t,
                                   workSpace: *mut ::libc::c_void,
                                   workSpaceSizeInBytes: usize,
                                   beta: *const ::libc::c_void,
                                   yDesc: cudnnTensorDescriptor_t,
                                   y: *mut ::libc::c_void)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnConvolutionBackwardBias(handle: cudnnHandle_t,
                                        alpha: *const ::libc::c_void,
                                        dyDesc: cudnnTensorDescriptor_t,
                                        dy: *const ::libc::c_void,
                                        beta: *const ::libc::c_void,
                                        dbDesc: cudnnTensorDescriptor_t,
                                        db: *mut ::libc::c_void)
     -> cudnnStatus_t;
}
pub const CUDNN_CONVOLUTION_BWD_FILTER_NO_WORKSPACE: _bindgen_ty_11 =
    _bindgen_ty_11::CUDNN_CONVOLUTION_BWD_FILTER_NO_WORKSPACE;
pub const CUDNN_CONVOLUTION_BWD_FILTER_PREFER_FASTEST: _bindgen_ty_11 =
    _bindgen_ty_11::CUDNN_CONVOLUTION_BWD_FILTER_PREFER_FASTEST;
pub const CUDNN_CONVOLUTION_BWD_FILTER_SPECIFY_WORKSPACE_LIMIT: _bindgen_ty_11
          =
    _bindgen_ty_11::CUDNN_CONVOLUTION_BWD_FILTER_SPECIFY_WORKSPACE_LIMIT;
#[repr(u32)]
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum _bindgen_ty_11 {
    CUDNN_CONVOLUTION_BWD_FILTER_NO_WORKSPACE = 0,
    CUDNN_CONVOLUTION_BWD_FILTER_PREFER_FASTEST = 1,
    CUDNN_CONVOLUTION_BWD_FILTER_SPECIFY_WORKSPACE_LIMIT = 2,
}
pub use self::_bindgen_ty_11 as cudnnConvolutionBwdFilterPreference_t;
pub const CUDNN_CONVOLUTION_BWD_FILTER_ALGO_0: _bindgen_ty_12 =
    _bindgen_ty_12::CUDNN_CONVOLUTION_BWD_FILTER_ALGO_0;
pub const CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1: _bindgen_ty_12 =
    _bindgen_ty_12::CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1;
pub const CUDNN_CONVOLUTION_BWD_FILTER_ALGO_FFT: _bindgen_ty_12 =
    _bindgen_ty_12::CUDNN_CONVOLUTION_BWD_FILTER_ALGO_FFT;
pub const CUDNN_CONVOLUTION_BWD_FILTER_ALGO_3: _bindgen_ty_12 =
    _bindgen_ty_12::CUDNN_CONVOLUTION_BWD_FILTER_ALGO_3;
pub const CUDNN_CONVOLUTION_BWD_FILTER_ALGO_WINOGRAD_NONFUSED: _bindgen_ty_12
          =
    _bindgen_ty_12::CUDNN_CONVOLUTION_BWD_FILTER_ALGO_WINOGRAD_NONFUSED;
#[repr(u32)]
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum _bindgen_ty_12 {
    CUDNN_CONVOLUTION_BWD_FILTER_ALGO_0 = 0,
    CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1 = 1,
    CUDNN_CONVOLUTION_BWD_FILTER_ALGO_FFT = 2,
    CUDNN_CONVOLUTION_BWD_FILTER_ALGO_3 = 3,
    CUDNN_CONVOLUTION_BWD_FILTER_ALGO_WINOGRAD_NONFUSED = 5,
}
pub use self::_bindgen_ty_12 as cudnnConvolutionBwdFilterAlgo_t;
#[repr(C)]
#[derive(Debug, Copy)]
pub struct _bindgen_ty_13 {
    pub algo: cudnnConvolutionBwdFilterAlgo_t,
    pub status: cudnnStatus_t,
    pub time: f32,
    pub memory: usize,
}
#[test]
fn bindgen_test_layout__bindgen_ty_13() {
    assert_eq!(::std::mem::size_of::<_bindgen_ty_13>() , 24usize);
    assert_eq!(::std::mem::align_of::<_bindgen_ty_13>() , 8usize);
}
impl Clone for _bindgen_ty_13 {
    fn clone(&self) -> Self { *self }
}
pub type cudnnConvolutionBwdFilterAlgoPerf_t = _bindgen_ty_13;

impl Default for cudnnConvolutionBwdFilterAlgoPerf_t {
    fn default() -> Self {
        cudnnConvolutionBwdFilterAlgoPerf_t {
            algo : CUDNN_CONVOLUTION_BWD_FILTER_ALGO_0,
            status : CUDNN_STATUS_NOT_INITIALIZED,
            time : 0.0 as f32,
            memory : 0,
        }
    }
}

extern "C" {
    pub fn cudnnFindConvolutionBackwardFilterAlgorithm(handle: cudnnHandle_t,
                                                       xDesc:
                                                           cudnnTensorDescriptor_t,
                                                       dyDesc:
                                                           cudnnTensorDescriptor_t,
                                                       convDesc:
                                                           cudnnConvolutionDescriptor_t,
                                                       dwDesc:
                                                           cudnnFilterDescriptor_t,
                                                       requestedAlgoCount:
                                                           ::libc::c_int,
                                                       returnedAlgoCount:
                                                           *mut ::libc::c_int,
                                                       perfResults:
                                                           *mut cudnnConvolutionBwdFilterAlgoPerf_t)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnFindConvolutionBackwardFilterAlgorithmEx(handle:
                                                             cudnnHandle_t,
                                                         xDesc:
                                                             cudnnTensorDescriptor_t,
                                                         x:
                                                             *const ::libc::c_void,
                                                         dyDesc:
                                                             cudnnTensorDescriptor_t,
                                                         y:
                                                             *const ::libc::c_void,
                                                         convDesc:
                                                             cudnnConvolutionDescriptor_t,
                                                         dwDesc:
                                                             cudnnFilterDescriptor_t,
                                                         dw:
                                                             *mut ::libc::c_void,
                                                         requestedAlgoCount:
                                                             ::libc::c_int,
                                                         returnedAlgoCount:
                                                             *mut ::libc::c_int,
                                                         perfResults:
                                                             *mut cudnnConvolutionBwdFilterAlgoPerf_t,
                                                         workSpace:
                                                             *mut ::libc::c_void,
                                                         workSpaceSizeInBytes:
                                                             usize)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnGetConvolutionBackwardFilterAlgorithm(handle: cudnnHandle_t,
                                                      xDesc:
                                                          cudnnTensorDescriptor_t,
                                                      dyDesc:
                                                          cudnnTensorDescriptor_t,
                                                      convDesc:
                                                          cudnnConvolutionDescriptor_t,
                                                      dwDesc:
                                                          cudnnFilterDescriptor_t,
                                                      preference:
                                                          cudnnConvolutionBwdFilterPreference_t,
                                                      memoryLimitInBytes:
                                                          usize,
                                                      algo:
                                                          *mut cudnnConvolutionBwdFilterAlgo_t)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnGetConvolutionBackwardFilterWorkspaceSize(handle:
                                                              cudnnHandle_t,
                                                          xDesc:
                                                              cudnnTensorDescriptor_t,
                                                          dyDesc:
                                                              cudnnTensorDescriptor_t,
                                                          convDesc:
                                                              cudnnConvolutionDescriptor_t,
                                                          gradDesc:
                                                              cudnnFilterDescriptor_t,
                                                          algo:
                                                              cudnnConvolutionBwdFilterAlgo_t,
                                                          sizeInBytes:
                                                              *mut usize)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnConvolutionBackwardFilter(handle: cudnnHandle_t,
                                          alpha:
                                              *const ::libc::c_void,
                                          xDesc: cudnnTensorDescriptor_t,
                                          x: *const ::libc::c_void,
                                          dyDesc: cudnnTensorDescriptor_t,
                                          dy: *const ::libc::c_void,
                                          convDesc:
                                              cudnnConvolutionDescriptor_t,
                                          algo:
                                              cudnnConvolutionBwdFilterAlgo_t,
                                          workSpace:
                                              *mut ::libc::c_void,
                                          workSpaceSizeInBytes: usize,
                                          beta: *const ::libc::c_void,
                                          dwDesc: cudnnFilterDescriptor_t,
                                          dw: *mut ::libc::c_void)
     -> cudnnStatus_t;
}
pub const CUDNN_CONVOLUTION_BWD_DATA_NO_WORKSPACE: _bindgen_ty_14 =
    _bindgen_ty_14::CUDNN_CONVOLUTION_BWD_DATA_NO_WORKSPACE;
pub const CUDNN_CONVOLUTION_BWD_DATA_PREFER_FASTEST: _bindgen_ty_14 =
    _bindgen_ty_14::CUDNN_CONVOLUTION_BWD_DATA_PREFER_FASTEST;
pub const CUDNN_CONVOLUTION_BWD_DATA_SPECIFY_WORKSPACE_LIMIT: _bindgen_ty_14 =
    _bindgen_ty_14::CUDNN_CONVOLUTION_BWD_DATA_SPECIFY_WORKSPACE_LIMIT;
#[repr(u32)]
/*********************************************************/
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum _bindgen_ty_14 {
    CUDNN_CONVOLUTION_BWD_DATA_NO_WORKSPACE = 0,
    CUDNN_CONVOLUTION_BWD_DATA_PREFER_FASTEST = 1,
    CUDNN_CONVOLUTION_BWD_DATA_SPECIFY_WORKSPACE_LIMIT = 2,
}
pub use self::_bindgen_ty_14 as cudnnConvolutionBwdDataPreference_t;
pub const CUDNN_CONVOLUTION_BWD_DATA_ALGO_0: _bindgen_ty_15 =
    _bindgen_ty_15::CUDNN_CONVOLUTION_BWD_DATA_ALGO_0;
pub const CUDNN_CONVOLUTION_BWD_DATA_ALGO_1: _bindgen_ty_15 =
    _bindgen_ty_15::CUDNN_CONVOLUTION_BWD_DATA_ALGO_1;
pub const CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT: _bindgen_ty_15 =
    _bindgen_ty_15::CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT;
pub const CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT_TILING: _bindgen_ty_15 =
    _bindgen_ty_15::CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT_TILING;
pub const CUDNN_CONVOLUTION_BWD_DATA_ALGO_WINOGRAD: _bindgen_ty_15 =
    _bindgen_ty_15::CUDNN_CONVOLUTION_BWD_DATA_ALGO_WINOGRAD;
pub const CUDNN_CONVOLUTION_BWD_DATA_ALGO_WINOGRAD_NONFUSED: _bindgen_ty_15 =
    _bindgen_ty_15::CUDNN_CONVOLUTION_BWD_DATA_ALGO_WINOGRAD_NONFUSED;
#[repr(u32)]
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum _bindgen_ty_15 {
    CUDNN_CONVOLUTION_BWD_DATA_ALGO_0 = 0,
    CUDNN_CONVOLUTION_BWD_DATA_ALGO_1 = 1,
    CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT = 2,
    CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT_TILING = 3,
    CUDNN_CONVOLUTION_BWD_DATA_ALGO_WINOGRAD = 4,
    CUDNN_CONVOLUTION_BWD_DATA_ALGO_WINOGRAD_NONFUSED = 5,
}
pub use self::_bindgen_ty_15 as cudnnConvolutionBwdDataAlgo_t;
#[repr(C)]
#[derive(Debug, Copy)]
pub struct _bindgen_ty_16 {
    pub algo: cudnnConvolutionBwdDataAlgo_t,
    pub status: cudnnStatus_t,
    pub time: f32,
    pub memory: usize,
}
#[test]
fn bindgen_test_layout__bindgen_ty_16() {
    assert_eq!(::std::mem::size_of::<_bindgen_ty_16>() , 24usize);
    assert_eq!(::std::mem::align_of::<_bindgen_ty_16>() , 8usize);
}
impl Clone for _bindgen_ty_16 {
    fn clone(&self) -> Self { *self }
}
pub type cudnnConvolutionBwdDataAlgoPerf_t = _bindgen_ty_16;

impl Default for cudnnConvolutionBwdDataAlgoPerf_t {
    fn default() -> Self {
        cudnnConvolutionBwdDataAlgoPerf_t {
            algo : CUDNN_CONVOLUTION_BWD_DATA_ALGO_0,
            status : CUDNN_STATUS_NOT_INITIALIZED,
            time : 0.0 as f32,
            memory : 0,
        }
    }
}

extern "C" {
    pub fn cudnnFindConvolutionBackwardDataAlgorithm(handle: cudnnHandle_t,
                                                     wDesc:
                                                         cudnnFilterDescriptor_t,
                                                     dyDesc:
                                                         cudnnTensorDescriptor_t,
                                                     convDesc:
                                                         cudnnConvolutionDescriptor_t,
                                                     dxDesc:
                                                         cudnnTensorDescriptor_t,
                                                     requestedAlgoCount:
                                                         ::libc::c_int,
                                                     returnedAlgoCount:
                                                         *mut ::libc::c_int,
                                                     perfResults:
                                                         *mut cudnnConvolutionBwdDataAlgoPerf_t)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnFindConvolutionBackwardDataAlgorithmEx(handle: cudnnHandle_t,
                                                       wDesc:
                                                           cudnnFilterDescriptor_t,
                                                       w:
                                                           *const ::libc::c_void,
                                                       dyDesc:
                                                           cudnnTensorDescriptor_t,
                                                       dy:
                                                           *const ::libc::c_void,
                                                       convDesc:
                                                           cudnnConvolutionDescriptor_t,
                                                       dxDesc:
                                                           cudnnTensorDescriptor_t,
                                                       dx:
                                                           *mut ::libc::c_void,
                                                       requestedAlgoCount:
                                                           ::libc::c_int,
                                                       returnedAlgoCount:
                                                           *mut ::libc::c_int,
                                                       perfResults:
                                                           *mut cudnnConvolutionBwdDataAlgoPerf_t,
                                                       workSpace:
                                                           *mut ::libc::c_void,
                                                       workSpaceSizeInBytes:
                                                           usize)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnGetConvolutionBackwardDataAlgorithm(handle: cudnnHandle_t,
                                                    wDesc:
                                                        cudnnFilterDescriptor_t,
                                                    dyDesc:
                                                        cudnnTensorDescriptor_t,
                                                    convDesc:
                                                        cudnnConvolutionDescriptor_t,
                                                    dxDesc:
                                                        cudnnTensorDescriptor_t,
                                                    preference:
                                                        cudnnConvolutionBwdDataPreference_t,
                                                    memoryLimitInBytes: usize,
                                                    algo:
                                                        *mut cudnnConvolutionBwdDataAlgo_t)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnGetConvolutionBackwardDataWorkspaceSize(handle: cudnnHandle_t,
                                                        wDesc:
                                                            cudnnFilterDescriptor_t,
                                                        dyDesc:
                                                            cudnnTensorDescriptor_t,
                                                        convDesc:
                                                            cudnnConvolutionDescriptor_t,
                                                        dxDesc:
                                                            cudnnTensorDescriptor_t,
                                                        algo:
                                                            cudnnConvolutionBwdDataAlgo_t,
                                                        sizeInBytes:
                                                            *mut usize)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnConvolutionBackwardData(handle: cudnnHandle_t,
                                        alpha: *const ::libc::c_void,
                                        wDesc: cudnnFilterDescriptor_t,
                                        w: *const ::libc::c_void,
                                        dyDesc: cudnnTensorDescriptor_t,
                                        dy: *const ::libc::c_void,
                                        convDesc:
                                            cudnnConvolutionDescriptor_t,
                                        algo: cudnnConvolutionBwdDataAlgo_t,
                                        workSpace:
                                            *mut ::libc::c_void,
                                        workSpaceSizeInBytes: usize,
                                        beta: *const ::libc::c_void,
                                        dxDesc: cudnnTensorDescriptor_t,
                                        dx: *mut ::libc::c_void)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnIm2Col(handle: cudnnHandle_t, xDesc: cudnnTensorDescriptor_t,
                       x: *const ::libc::c_void,
                       wDesc: cudnnFilterDescriptor_t,
                       convDesc: cudnnConvolutionDescriptor_t,
                       colBuffer: *mut ::libc::c_void)
     -> cudnnStatus_t;
}
pub const CUDNN_SOFTMAX_FAST: _bindgen_ty_17 =
    _bindgen_ty_17::CUDNN_SOFTMAX_FAST;
pub const CUDNN_SOFTMAX_ACCURATE: _bindgen_ty_17 =
    _bindgen_ty_17::CUDNN_SOFTMAX_ACCURATE;
pub const CUDNN_SOFTMAX_LOG: _bindgen_ty_17 =
    _bindgen_ty_17::CUDNN_SOFTMAX_LOG;
#[repr(u32)]
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum _bindgen_ty_17 {
    CUDNN_SOFTMAX_FAST = 0,
    CUDNN_SOFTMAX_ACCURATE = 1,
    CUDNN_SOFTMAX_LOG = 2,
}
pub use self::_bindgen_ty_17 as cudnnSoftmaxAlgorithm_t;
pub const CUDNN_SOFTMAX_MODE_INSTANCE: _bindgen_ty_18 =
    _bindgen_ty_18::CUDNN_SOFTMAX_MODE_INSTANCE;
pub const CUDNN_SOFTMAX_MODE_CHANNEL: _bindgen_ty_18 =
    _bindgen_ty_18::CUDNN_SOFTMAX_MODE_CHANNEL;
#[repr(u32)]
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum _bindgen_ty_18 {
    CUDNN_SOFTMAX_MODE_INSTANCE = 0,
    CUDNN_SOFTMAX_MODE_CHANNEL = 1,
}
pub use self::_bindgen_ty_18 as cudnnSoftmaxMode_t;
extern "C" {
    pub fn cudnnSoftmaxForward(handle: cudnnHandle_t,
                               algo: cudnnSoftmaxAlgorithm_t,
                               mode: cudnnSoftmaxMode_t,
                               alpha: *const ::libc::c_void,
                               xDesc: cudnnTensorDescriptor_t,
                               x: *const ::libc::c_void,
                               beta: *const ::libc::c_void,
                               yDesc: cudnnTensorDescriptor_t,
                               y: *mut ::libc::c_void)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnSoftmaxBackward(handle: cudnnHandle_t,
                                algo: cudnnSoftmaxAlgorithm_t,
                                mode: cudnnSoftmaxMode_t,
                                alpha: *const ::libc::c_void,
                                yDesc: cudnnTensorDescriptor_t,
                                y: *const ::libc::c_void,
                                dyDesc: cudnnTensorDescriptor_t,
                                dy: *const ::libc::c_void,
                                beta: *const ::libc::c_void,
                                dxDesc: cudnnTensorDescriptor_t,
                                dx: *mut ::libc::c_void)
     -> cudnnStatus_t;
}
pub const CUDNN_POOLING_MAX: _bindgen_ty_19 =
    _bindgen_ty_19::CUDNN_POOLING_MAX;
pub const CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING: _bindgen_ty_19 =
    _bindgen_ty_19::CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING;
pub const CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING: _bindgen_ty_19 =
    _bindgen_ty_19::CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING;
#[repr(u32)]
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum _bindgen_ty_19 {
    CUDNN_POOLING_MAX = 0,
    CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING = 1,
    CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING = 2,
}
pub use self::_bindgen_ty_19 as cudnnPoolingMode_t;
extern "C" {
    pub fn cudnnCreatePoolingDescriptor(poolingDesc:
                                            *mut cudnnPoolingDescriptor_t)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnSetPooling2dDescriptor(poolingDesc: cudnnPoolingDescriptor_t,
                                       mode: cudnnPoolingMode_t,
                                       maxpoolingNanOpt:
                                           cudnnNanPropagation_t,
                                       windowHeight: ::libc::c_int,
                                       windowWidth: ::libc::c_int,
                                       verticalPadding: ::libc::c_int,
                                       horizontalPadding:
                                           ::libc::c_int,
                                       verticalStride: ::libc::c_int,
                                       horizontalStride:
                                           ::libc::c_int)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnGetPooling2dDescriptor(poolingDesc: cudnnPoolingDescriptor_t,
                                       mode: *mut cudnnPoolingMode_t,
                                       maxpoolingNanOpt:
                                           *mut cudnnNanPropagation_t,
                                       windowHeight:
                                           *mut ::libc::c_int,
                                       windowWidth:
                                           *mut ::libc::c_int,
                                       verticalPadding:
                                           *mut ::libc::c_int,
                                       horizontalPadding:
                                           *mut ::libc::c_int,
                                       verticalStride:
                                           *mut ::libc::c_int,
                                       horizontalStride:
                                           *mut ::libc::c_int)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnSetPoolingNdDescriptor(poolingDesc: cudnnPoolingDescriptor_t,
                                       mode: cudnnPoolingMode_t,
                                       maxpoolingNanOpt:
                                           cudnnNanPropagation_t,
                                       nbDims: ::libc::c_int,
                                       windowDimA:
                                           *const ::libc::c_int,
                                       paddingA: *const ::libc::c_int,
                                       strideA: *const ::libc::c_int)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnGetPoolingNdDescriptor(poolingDesc: cudnnPoolingDescriptor_t,
                                       nbDimsRequested: ::libc::c_int,
                                       mode: *mut cudnnPoolingMode_t,
                                       maxpoolingNanOpt:
                                           *mut cudnnNanPropagation_t,
                                       nbDims: *mut ::libc::c_int,
                                       windowDimA: *mut ::libc::c_int,
                                       paddingA: *mut ::libc::c_int,
                                       strideA: *mut ::libc::c_int)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnGetPoolingNdForwardOutputDim(poolingDesc:
                                                 cudnnPoolingDescriptor_t,
                                             inputTensorDesc:
                                                 cudnnTensorDescriptor_t,
                                             nbDims: ::libc::c_int,
                                             outputTensorDimA:
                                                 *mut ::libc::c_int)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnGetPooling2dForwardOutputDim(poolingDesc:
                                                 cudnnPoolingDescriptor_t,
                                             inputTensorDesc:
                                                 cudnnTensorDescriptor_t,
                                             n: *mut ::libc::c_int,
                                             c: *mut ::libc::c_int,
                                             h: *mut ::libc::c_int,
                                             w: *mut ::libc::c_int)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnDestroyPoolingDescriptor(poolingDesc:
                                             cudnnPoolingDescriptor_t)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnPoolingForward(handle: cudnnHandle_t,
                               poolingDesc: cudnnPoolingDescriptor_t,
                               alpha: *const ::libc::c_void,
                               xDesc: cudnnTensorDescriptor_t,
                               x: *const ::libc::c_void,
                               beta: *const ::libc::c_void,
                               yDesc: cudnnTensorDescriptor_t,
                               y: *mut ::libc::c_void)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnPoolingBackward(handle: cudnnHandle_t,
                                poolingDesc: cudnnPoolingDescriptor_t,
                                alpha: *const ::libc::c_void,
                                yDesc: cudnnTensorDescriptor_t,
                                y: *const ::libc::c_void,
                                dyDesc: cudnnTensorDescriptor_t,
                                dy: *const ::libc::c_void,
                                xDesc: cudnnTensorDescriptor_t,
                                x: *const ::libc::c_void,
                                beta: *const ::libc::c_void,
                                dxDesc: cudnnTensorDescriptor_t,
                                dx: *mut ::libc::c_void)
     -> cudnnStatus_t;
}
pub const CUDNN_ACTIVATION_SIGMOID: _bindgen_ty_20 =
    _bindgen_ty_20::CUDNN_ACTIVATION_SIGMOID;
pub const CUDNN_ACTIVATION_RELU: _bindgen_ty_20 =
    _bindgen_ty_20::CUDNN_ACTIVATION_RELU;
pub const CUDNN_ACTIVATION_TANH: _bindgen_ty_20 =
    _bindgen_ty_20::CUDNN_ACTIVATION_TANH;
pub const CUDNN_ACTIVATION_CLIPPED_RELU: _bindgen_ty_20 =
    _bindgen_ty_20::CUDNN_ACTIVATION_CLIPPED_RELU;
#[repr(u32)]
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum _bindgen_ty_20 {
    CUDNN_ACTIVATION_SIGMOID = 0,
    CUDNN_ACTIVATION_RELU = 1,
    CUDNN_ACTIVATION_TANH = 2,
    CUDNN_ACTIVATION_CLIPPED_RELU = 3,
}
pub use self::_bindgen_ty_20 as cudnnActivationMode_t;
extern "C" {
    pub fn cudnnCreateActivationDescriptor(activationDesc:
                                               *mut cudnnActivationDescriptor_t)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnSetActivationDescriptor(activationDesc:
                                            cudnnActivationDescriptor_t,
                                        mode: cudnnActivationMode_t,
                                        reluNanOpt: cudnnNanPropagation_t,
                                        reluCeiling: f64) -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnGetActivationDescriptor(activationDesc:
                                            cudnnActivationDescriptor_t,
                                        mode: *mut cudnnActivationMode_t,
                                        reluNanOpt:
                                            *mut cudnnNanPropagation_t,
                                        reluCeiling: *mut f64)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnDestroyActivationDescriptor(activationDesc:
                                                cudnnActivationDescriptor_t)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnActivationForward(handle: cudnnHandle_t,
                                  activationDesc: cudnnActivationDescriptor_t,
                                  alpha: *const ::libc::c_void,
                                  xDesc: cudnnTensorDescriptor_t,
                                  x: *const ::libc::c_void,
                                  beta: *const ::libc::c_void,
                                  yDesc: cudnnTensorDescriptor_t,
                                  y: *mut ::libc::c_void)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnActivationBackward(handle: cudnnHandle_t,
                                   activationDesc:
                                       cudnnActivationDescriptor_t,
                                   alpha: *const ::libc::c_void,
                                   yDesc: cudnnTensorDescriptor_t,
                                   y: *const ::libc::c_void,
                                   dyDesc: cudnnTensorDescriptor_t,
                                   dy: *const ::libc::c_void,
                                   xDesc: cudnnTensorDescriptor_t,
                                   x: *const ::libc::c_void,
                                   beta: *const ::libc::c_void,
                                   dxDesc: cudnnTensorDescriptor_t,
                                   dx: *mut ::libc::c_void)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnCreateLRNDescriptor(normDesc: *mut cudnnLRNDescriptor_t)
     -> cudnnStatus_t;
}
pub const CUDNN_LRN_CROSS_CHANNEL_DIM1: _bindgen_ty_21 =
    _bindgen_ty_21::CUDNN_LRN_CROSS_CHANNEL_DIM1;
#[repr(u32)]
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum _bindgen_ty_21 { CUDNN_LRN_CROSS_CHANNEL_DIM1 = 0, }
pub use self::_bindgen_ty_21 as cudnnLRNMode_t;
extern "C" {
    pub fn cudnnSetLRNDescriptor(normDesc: cudnnLRNDescriptor_t,
                                 lrnN: ::libc::c_uint, lrnAlpha: f64,
                                 lrnBeta: f64, lrnK: f64) -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnGetLRNDescriptor(normDesc: cudnnLRNDescriptor_t,
                                 lrnN: *mut ::libc::c_uint,
                                 lrnAlpha: *mut f64, lrnBeta: *mut f64,
                                 lrnK: *mut f64) -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnDestroyLRNDescriptor(lrnDesc: cudnnLRNDescriptor_t)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnLRNCrossChannelForward(handle: cudnnHandle_t,
                                       normDesc: cudnnLRNDescriptor_t,
                                       lrnMode: cudnnLRNMode_t,
                                       alpha: *const ::libc::c_void,
                                       xDesc: cudnnTensorDescriptor_t,
                                       x: *const ::libc::c_void,
                                       beta: *const ::libc::c_void,
                                       yDesc: cudnnTensorDescriptor_t,
                                       y: *mut ::libc::c_void)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnLRNCrossChannelBackward(handle: cudnnHandle_t,
                                        normDesc: cudnnLRNDescriptor_t,
                                        lrnMode: cudnnLRNMode_t,
                                        alpha: *const ::libc::c_void,
                                        yDesc: cudnnTensorDescriptor_t,
                                        y: *const ::libc::c_void,
                                        dyDesc: cudnnTensorDescriptor_t,
                                        dy: *const ::libc::c_void,
                                        xDesc: cudnnTensorDescriptor_t,
                                        x: *const ::libc::c_void,
                                        beta: *const ::libc::c_void,
                                        dxDesc: cudnnTensorDescriptor_t,
                                        dx: *mut ::libc::c_void)
     -> cudnnStatus_t;
}
pub const CUDNN_DIVNORM_PRECOMPUTED_MEANS: _bindgen_ty_22 =
    _bindgen_ty_22::CUDNN_DIVNORM_PRECOMPUTED_MEANS;
#[repr(u32)]
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum _bindgen_ty_22 { CUDNN_DIVNORM_PRECOMPUTED_MEANS = 0, }
pub use self::_bindgen_ty_22 as cudnnDivNormMode_t;
extern "C" {
    pub fn cudnnDivisiveNormalizationForward(handle: cudnnHandle_t,
                                             normDesc: cudnnLRNDescriptor_t,
                                             mode: cudnnDivNormMode_t,
                                             alpha:
                                                 *const ::libc::c_void,
                                             xDesc: cudnnTensorDescriptor_t,
                                             x: *const ::libc::c_void,
                                             means:
                                                 *const ::libc::c_void,
                                             temp:
                                                 *mut ::libc::c_void,
                                             temp2:
                                                 *mut ::libc::c_void,
                                             beta:
                                                 *const ::libc::c_void,
                                             yDesc: cudnnTensorDescriptor_t,
                                             y: *mut ::libc::c_void)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnDivisiveNormalizationBackward(handle: cudnnHandle_t,
                                              normDesc: cudnnLRNDescriptor_t,
                                              mode: cudnnDivNormMode_t,
                                              alpha:
                                                  *const ::libc::c_void,
                                              xDesc: cudnnTensorDescriptor_t,
                                              x:
                                                  *const ::libc::c_void,
                                              means:
                                                  *const ::libc::c_void,
                                              dy:
                                                  *const ::libc::c_void,
                                              temp:
                                                  *mut ::libc::c_void,
                                              temp2:
                                                  *mut ::libc::c_void,
                                              beta:
                                                  *const ::libc::c_void,
                                              dXdMeansDesc:
                                                  cudnnTensorDescriptor_t,
                                              dx: *mut ::libc::c_void,
                                              dMeans:
                                                  *mut ::libc::c_void)
     -> cudnnStatus_t;
}
pub const CUDNN_BATCHNORM_PER_ACTIVATION: _bindgen_ty_23 =
    _bindgen_ty_23::CUDNN_BATCHNORM_PER_ACTIVATION;
pub const CUDNN_BATCHNORM_SPATIAL: _bindgen_ty_23 =
    _bindgen_ty_23::CUDNN_BATCHNORM_SPATIAL;
#[repr(u32)]
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum _bindgen_ty_23 {
    CUDNN_BATCHNORM_PER_ACTIVATION = 0,
    CUDNN_BATCHNORM_SPATIAL = 1,
}
pub use self::_bindgen_ty_23 as cudnnBatchNormMode_t;
extern "C" {
    pub fn cudnnDeriveBNTensorDescriptor(derivedBnDesc:
                                             cudnnTensorDescriptor_t,
                                         xDesc: cudnnTensorDescriptor_t,
                                         mode: cudnnBatchNormMode_t)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnBatchNormalizationForwardTraining(handle: cudnnHandle_t,
                                                  mode: cudnnBatchNormMode_t,
                                                  alpha:
                                                      *const ::libc::c_void,
                                                  beta:
                                                      *const ::libc::c_void,
                                                  xDesc:
                                                      cudnnTensorDescriptor_t,
                                                  x:
                                                      *const ::libc::c_void,
                                                  yDesc:
                                                      cudnnTensorDescriptor_t,
                                                  y:
                                                      *mut ::libc::c_void,
                                                  bnScaleBiasMeanVarDesc:
                                                      cudnnTensorDescriptor_t,
                                                  bnScale:
                                                      *const ::libc::c_void,
                                                  bnBias:
                                                      *const ::libc::c_void,
                                                  exponentialAverageFactor:
                                                      f64,
                                                  resultRunningMean:
                                                      *mut ::libc::c_void,
                                                  resultRunningVariance:
                                                      *mut ::libc::c_void,
                                                  epsilon: f64,
                                                  resultSaveMean:
                                                      *mut ::libc::c_void,
                                                  resultSaveInvVariance:
                                                      *mut ::libc::c_void)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnBatchNormalizationForwardInference(handle: cudnnHandle_t,
                                                   mode: cudnnBatchNormMode_t,
                                                   alpha:
                                                       *const ::libc::c_void,
                                                   beta:
                                                       *const ::libc::c_void,
                                                   xDesc:
                                                       cudnnTensorDescriptor_t,
                                                   x:
                                                       *const ::libc::c_void,
                                                   yDesc:
                                                       cudnnTensorDescriptor_t,
                                                   y:
                                                       *mut ::libc::c_void,
                                                   bnScaleBiasMeanVarDesc:
                                                       cudnnTensorDescriptor_t,
                                                   bnScale:
                                                       *const ::libc::c_void,
                                                   bnBias:
                                                       *const ::libc::c_void,
                                                   estimatedMean:
                                                       *const ::libc::c_void,
                                                   estimatedVariance:
                                                       *const ::libc::c_void,
                                                   epsilon: f64)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnBatchNormalizationBackward(handle: cudnnHandle_t,
                                           mode: cudnnBatchNormMode_t,
                                           alphaDataDiff:
                                               *const ::libc::c_void,
                                           betaDataDiff:
                                               *const ::libc::c_void,
                                           alphaParamDiff:
                                               *const ::libc::c_void,
                                           betaParamDiff:
                                               *const ::libc::c_void,
                                           xDesc: cudnnTensorDescriptor_t,
                                           x: *const ::libc::c_void,
                                           dyDesc: cudnnTensorDescriptor_t,
                                           dy: *const ::libc::c_void,
                                           dxDesc: cudnnTensorDescriptor_t,
                                           dx: *mut ::libc::c_void,
                                           dBnScaleBiasDesc:
                                               cudnnTensorDescriptor_t,
                                           bnScale:
                                               *const ::libc::c_void,
                                           dBnScaleResult:
                                               *mut ::libc::c_void,
                                           dBnBiasResult:
                                               *mut ::libc::c_void,
                                           epsilon: f64,
                                           savedMean:
                                               *const ::libc::c_void,
                                           savedInvVariance:
                                               *const ::libc::c_void)
     -> cudnnStatus_t;
}
pub const CUDNN_SAMPLER_BILINEAR: _bindgen_ty_24 =
    _bindgen_ty_24::CUDNN_SAMPLER_BILINEAR;
#[repr(u32)]
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum _bindgen_ty_24 { CUDNN_SAMPLER_BILINEAR = 0, }
pub use self::_bindgen_ty_24 as cudnnSamplerType_t;
extern "C" {
    pub fn cudnnCreateSpatialTransformerDescriptor(stDesc:
                                                       *mut cudnnSpatialTransformerDescriptor_t)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnSetSpatialTransformerNdDescriptor(stDesc:
                                                      cudnnSpatialTransformerDescriptor_t,
                                                  samplerType:
                                                      cudnnSamplerType_t,
                                                  dataType: cudnnDataType_t,
                                                  nbDims:
                                                      ::libc::c_int,
                                                  dimA:
                                                      *const ::libc::c_int)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnDestroySpatialTransformerDescriptor(stDesc:
                                                        cudnnSpatialTransformerDescriptor_t)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnSpatialTfGridGeneratorForward(handle: cudnnHandle_t,
                                              stDesc:
                                                  cudnnSpatialTransformerDescriptor_t,
                                              theta:
                                                  *const ::libc::c_void,
                                              grid:
                                                  *mut ::libc::c_void)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnSpatialTfGridGeneratorBackward(handle: cudnnHandle_t,
                                               stDesc:
                                                   cudnnSpatialTransformerDescriptor_t,
                                               dgrid:
                                                   *const ::libc::c_void,
                                               dtheta:
                                                   *mut ::libc::c_void)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnSpatialTfSamplerForward(handle: cudnnHandle_t,
                                        stDesc:
                                            cudnnSpatialTransformerDescriptor_t,
                                        alpha: *const ::libc::c_void,
                                        xDesc: cudnnTensorDescriptor_t,
                                        x: *const ::libc::c_void,
                                        grid: *const ::libc::c_void,
                                        beta: *const ::libc::c_void,
                                        yDesc: cudnnTensorDescriptor_t,
                                        y: *mut ::libc::c_void)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnSpatialTfSamplerBackward(handle: cudnnHandle_t,
                                         stDesc:
                                             cudnnSpatialTransformerDescriptor_t,
                                         alpha: *const ::libc::c_void,
                                         xDesc: cudnnTensorDescriptor_t,
                                         x: *const ::libc::c_void,
                                         beta: *const ::libc::c_void,
                                         dxDesc: cudnnTensorDescriptor_t,
                                         dx: *mut ::libc::c_void,
                                         alphaDgrid:
                                             *const ::libc::c_void,
                                         dyDesc: cudnnTensorDescriptor_t,
                                         dy: *const ::libc::c_void,
                                         grid: *const ::libc::c_void,
                                         betaDgrid:
                                             *const ::libc::c_void,
                                         dgrid: *mut ::libc::c_void)
     -> cudnnStatus_t;
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudnnDropoutStruct([u8; 0]);
pub type cudnnDropoutDescriptor_t = *mut cudnnDropoutStruct;
extern "C" {
    pub fn cudnnCreateDropoutDescriptor(dropoutDesc:
                                            *mut cudnnDropoutDescriptor_t)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnDestroyDropoutDescriptor(dropoutDesc:
                                             cudnnDropoutDescriptor_t)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnDropoutGetStatesSize(handle: cudnnHandle_t,
                                     sizeInBytes: *mut usize)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnDropoutGetReserveSpaceSize(xdesc: cudnnTensorDescriptor_t,
                                           sizeInBytes: *mut usize)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnSetDropoutDescriptor(dropoutDesc: cudnnDropoutDescriptor_t,
                                     handle: cudnnHandle_t, dropout: f32,
                                     states: *mut ::libc::c_void,
                                     stateSizeInBytes: usize,
                                     seed: ::libc::c_ulonglong)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnDropoutForward(handle: cudnnHandle_t,
                               dropoutDesc: cudnnDropoutDescriptor_t,
                               xdesc: cudnnTensorDescriptor_t,
                               x: *const ::libc::c_void,
                               ydesc: cudnnTensorDescriptor_t,
                               y: *mut ::libc::c_void,
                               reserveSpace: *mut ::libc::c_void,
                               reserveSpaceSizeInBytes: usize)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnDropoutBackward(handle: cudnnHandle_t,
                                dropoutDesc: cudnnDropoutDescriptor_t,
                                dydesc: cudnnTensorDescriptor_t,
                                dy: *const ::libc::c_void,
                                dxdesc: cudnnTensorDescriptor_t,
                                dx: *mut ::libc::c_void,
                                reserveSpace: *mut ::libc::c_void,
                                reserveSpaceSizeInBytes: usize)
     -> cudnnStatus_t;
}
pub const CUDNN_RNN_RELU: _bindgen_ty_25 = _bindgen_ty_25::CUDNN_RNN_RELU;
pub const CUDNN_RNN_TANH: _bindgen_ty_25 = _bindgen_ty_25::CUDNN_RNN_TANH;
pub const CUDNN_LSTM: _bindgen_ty_25 = _bindgen_ty_25::CUDNN_LSTM;
pub const CUDNN_GRU: _bindgen_ty_25 = _bindgen_ty_25::CUDNN_GRU;
#[repr(u32)]
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum _bindgen_ty_25 {
    CUDNN_RNN_RELU = 0,
    CUDNN_RNN_TANH = 1,
    CUDNN_LSTM = 2,
    CUDNN_GRU = 3,
}
pub use self::_bindgen_ty_25 as cudnnRNNMode_t;
pub const CUDNN_UNIDIRECTIONAL: _bindgen_ty_26 =
    _bindgen_ty_26::CUDNN_UNIDIRECTIONAL;
pub const CUDNN_BIDIRECTIONAL: _bindgen_ty_26 =
    _bindgen_ty_26::CUDNN_BIDIRECTIONAL;
#[repr(u32)]
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum _bindgen_ty_26 { CUDNN_UNIDIRECTIONAL = 0, CUDNN_BIDIRECTIONAL = 1, }
pub use self::_bindgen_ty_26 as cudnnDirectionMode_t;
pub const CUDNN_LINEAR_INPUT: _bindgen_ty_27 =
    _bindgen_ty_27::CUDNN_LINEAR_INPUT;
pub const CUDNN_SKIP_INPUT: _bindgen_ty_27 = _bindgen_ty_27::CUDNN_SKIP_INPUT;
#[repr(u32)]
#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]
pub enum _bindgen_ty_27 { CUDNN_LINEAR_INPUT = 0, CUDNN_SKIP_INPUT = 1, }
pub use self::_bindgen_ty_27 as cudnnRNNInputMode_t;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudnnRNNStruct([u8; 0]);
pub type cudnnRNNDescriptor_t = *mut cudnnRNNStruct;
extern "C" {
    pub fn cudnnCreateRNNDescriptor(rnnDesc: *mut cudnnRNNDescriptor_t)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnDestroyRNNDescriptor(rnnDesc: cudnnRNNDescriptor_t)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnSetRNNDescriptor(rnnDesc: cudnnRNNDescriptor_t,
                                 hiddenSize: ::libc::c_int,
                                 numLayers: ::libc::c_int,
                                 dropoutDesc: cudnnDropoutDescriptor_t,
                                 inputMode: cudnnRNNInputMode_t,
                                 direction: cudnnDirectionMode_t,
                                 mode: cudnnRNNMode_t,
                                 dataType: cudnnDataType_t) -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnGetRNNWorkspaceSize(handle: cudnnHandle_t,
                                    rnnDesc: cudnnRNNDescriptor_t,
                                    seqLength: ::libc::c_int,
                                    xDesc: *const cudnnTensorDescriptor_t,
                                    sizeInBytes: *mut usize) -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnGetRNNTrainingReserveSize(handle: cudnnHandle_t,
                                          rnnDesc: cudnnRNNDescriptor_t,
                                          seqLength: ::libc::c_int,
                                          xDesc:
                                              *const cudnnTensorDescriptor_t,
                                          sizeInBytes: *mut usize)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnGetRNNParamsSize(handle: cudnnHandle_t,
                                 rnnDesc: cudnnRNNDescriptor_t,
                                 xDesc: cudnnTensorDescriptor_t,
                                 sizeInBytes: *mut usize,
                                 dataType: cudnnDataType_t) -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnGetRNNLinLayerMatrixParams(handle: cudnnHandle_t,
                                           rnnDesc: cudnnRNNDescriptor_t,
                                           layer: ::libc::c_int,
                                           xDesc: cudnnTensorDescriptor_t,
                                           wDesc: cudnnFilterDescriptor_t,
                                           w: *const ::libc::c_void,
                                           linLayerID: ::libc::c_int,
                                           linLayerMatDesc:
                                               cudnnFilterDescriptor_t,
                                           linLayerMat:
                                               *mut *mut ::libc::c_void)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnGetRNNLinLayerBiasParams(handle: cudnnHandle_t,
                                         rnnDesc: cudnnRNNDescriptor_t,
                                         layer: ::libc::c_int,
                                         xDesc: cudnnTensorDescriptor_t,
                                         wDesc: cudnnFilterDescriptor_t,
                                         w: *const ::libc::c_void,
                                         linLayerID: ::libc::c_int,
                                         linLayerBiasDesc:
                                             cudnnFilterDescriptor_t,
                                         linLayerBias:
                                             *mut *mut ::libc::c_void)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnRNNForwardInference(handle: cudnnHandle_t,
                                    rnnDesc: cudnnRNNDescriptor_t,
                                    seqLength: ::libc::c_int,
                                    xDesc: *const cudnnTensorDescriptor_t,
                                    x: *const ::libc::c_void,
                                    hxDesc: cudnnTensorDescriptor_t,
                                    hx: *const ::libc::c_void,
                                    cxDesc: cudnnTensorDescriptor_t,
                                    cx: *const ::libc::c_void,
                                    wDesc: cudnnFilterDescriptor_t,
                                    w: *const ::libc::c_void,
                                    yDesc: *const cudnnTensorDescriptor_t,
                                    y: *mut ::libc::c_void,
                                    hyDesc: cudnnTensorDescriptor_t,
                                    hy: *mut ::libc::c_void,
                                    cyDesc: cudnnTensorDescriptor_t,
                                    cy: *mut ::libc::c_void,
                                    workspace: *mut ::libc::c_void,
                                    workSpaceSizeInBytes: usize)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnRNNForwardTraining(handle: cudnnHandle_t,
                                   rnnDesc: cudnnRNNDescriptor_t,
                                   seqLength: ::libc::c_int,
                                   xDesc: *const cudnnTensorDescriptor_t,
                                   x: *const ::libc::c_void,
                                   hxDesc: cudnnTensorDescriptor_t,
                                   hx: *const ::libc::c_void,
                                   cxDesc: cudnnTensorDescriptor_t,
                                   cx: *const ::libc::c_void,
                                   wDesc: cudnnFilterDescriptor_t,
                                   w: *const ::libc::c_void,
                                   yDesc: *const cudnnTensorDescriptor_t,
                                   y: *mut ::libc::c_void,
                                   hyDesc: cudnnTensorDescriptor_t,
                                   hy: *mut ::libc::c_void,
                                   cyDesc: cudnnTensorDescriptor_t,
                                   cy: *mut ::libc::c_void,
                                   workspace: *mut ::libc::c_void,
                                   workSpaceSizeInBytes: usize,
                                   reserveSpace: *mut ::libc::c_void,
                                   reserveSpaceSizeInBytes: usize)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnRNNBackwardData(handle: cudnnHandle_t,
                                rnnDesc: cudnnRNNDescriptor_t,
                                seqLength: ::libc::c_int,
                                yDesc: *const cudnnTensorDescriptor_t,
                                y: *const ::libc::c_void,
                                dyDesc: *const cudnnTensorDescriptor_t,
                                dy: *const ::libc::c_void,
                                dhyDesc: cudnnTensorDescriptor_t,
                                dhy: *const ::libc::c_void,
                                dcyDesc: cudnnTensorDescriptor_t,
                                dcy: *const ::libc::c_void,
                                wDesc: cudnnFilterDescriptor_t,
                                w: *const ::libc::c_void,
                                hxDesc: cudnnTensorDescriptor_t,
                                hx: *const ::libc::c_void,
                                cxDesc: cudnnTensorDescriptor_t,
                                cx: *const ::libc::c_void,
                                dxDesc: *const cudnnTensorDescriptor_t,
                                dx: *mut ::libc::c_void,
                                dhxDesc: cudnnTensorDescriptor_t,
                                dhx: *mut ::libc::c_void,
                                dcxDesc: cudnnTensorDescriptor_t,
                                dcx: *mut ::libc::c_void,
                                workspace: *mut ::libc::c_void,
                                workSpaceSizeInBytes: usize,
                                reserveSpace: *const ::libc::c_void,
                                reserveSpaceSizeInBytes: usize)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnRNNBackwardWeights(handle: cudnnHandle_t,
                                   rnnDesc: cudnnRNNDescriptor_t,
                                   seqLength: ::libc::c_int,
                                   xDesc: *const cudnnTensorDescriptor_t,
                                   x: *const ::libc::c_void,
                                   hxDesc: cudnnTensorDescriptor_t,
                                   hx: *const ::libc::c_void,
                                   yDesc: *const cudnnTensorDescriptor_t,
                                   y: *const ::libc::c_void,
                                   workspace: *const ::libc::c_void,
                                   workSpaceSizeInBytes: usize,
                                   dwDesc: cudnnFilterDescriptor_t,
                                   dw: *mut ::libc::c_void,
                                   reserveSpace:
                                       *const ::libc::c_void,
                                   reserveSpaceSizeInBytes: usize)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnSetFilter4dDescriptor_v3(filterDesc: cudnnFilterDescriptor_t,
                                         dataType: cudnnDataType_t,
                                         k: ::libc::c_int,
                                         c: ::libc::c_int,
                                         h: ::libc::c_int,
                                         w: ::libc::c_int)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnSetFilter4dDescriptor_v4(filterDesc: cudnnFilterDescriptor_t,
                                         dataType: cudnnDataType_t,
                                         format: cudnnTensorFormat_t,
                                         k: ::libc::c_int,
                                         c: ::libc::c_int,
                                         h: ::libc::c_int,
                                         w: ::libc::c_int)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnGetFilter4dDescriptor_v3(filterDesc: cudnnFilterDescriptor_t,
                                         dataType: *mut cudnnDataType_t,
                                         k: *mut ::libc::c_int,
                                         c: *mut ::libc::c_int,
                                         h: *mut ::libc::c_int,
                                         w: *mut ::libc::c_int)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnGetFilter4dDescriptor_v4(filterDesc: cudnnFilterDescriptor_t,
                                         dataType: *mut cudnnDataType_t,
                                         format: *mut cudnnTensorFormat_t,
                                         k: *mut ::libc::c_int,
                                         c: *mut ::libc::c_int,
                                         h: *mut ::libc::c_int,
                                         w: *mut ::libc::c_int)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnSetFilterNdDescriptor_v3(filterDesc: cudnnFilterDescriptor_t,
                                         dataType: cudnnDataType_t,
                                         nbDims: ::libc::c_int,
                                         filterDimA:
                                             *const ::libc::c_int)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnSetFilterNdDescriptor_v4(filterDesc: cudnnFilterDescriptor_t,
                                         dataType: cudnnDataType_t,
                                         format: cudnnTensorFormat_t,
                                         nbDims: ::libc::c_int,
                                         filterDimA:
                                             *const ::libc::c_int)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnGetFilterNdDescriptor_v3(filterDesc: cudnnFilterDescriptor_t,
                                         nbDimsRequested:
                                             ::libc::c_int,
                                         dataType: *mut cudnnDataType_t,
                                         nbDims: *mut ::libc::c_int,
                                         filterDimA:
                                             *mut ::libc::c_int)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnGetFilterNdDescriptor_v4(filterDesc: cudnnFilterDescriptor_t,
                                         nbDimsRequested:
                                             ::libc::c_int,
                                         dataType: *mut cudnnDataType_t,
                                         format: *mut cudnnTensorFormat_t,
                                         nbDims: *mut ::libc::c_int,
                                         filterDimA:
                                             *mut ::libc::c_int)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnSetPooling2dDescriptor_v3(poolingDesc:
                                              cudnnPoolingDescriptor_t,
                                          mode: cudnnPoolingMode_t,
                                          windowHeight: ::libc::c_int,
                                          windowWidth: ::libc::c_int,
                                          verticalPadding:
                                              ::libc::c_int,
                                          horizontalPadding:
                                              ::libc::c_int,
                                          verticalStride:
                                              ::libc::c_int,
                                          horizontalStride:
                                              ::libc::c_int)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnSetPooling2dDescriptor_v4(poolingDesc:
                                              cudnnPoolingDescriptor_t,
                                          mode: cudnnPoolingMode_t,
                                          maxpoolingNanOpt:
                                              cudnnNanPropagation_t,
                                          windowHeight: ::libc::c_int,
                                          windowWidth: ::libc::c_int,
                                          verticalPadding:
                                              ::libc::c_int,
                                          horizontalPadding:
                                              ::libc::c_int,
                                          verticalStride:
                                              ::libc::c_int,
                                          horizontalStride:
                                              ::libc::c_int)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnGetPooling2dDescriptor_v3(poolingDesc:
                                              cudnnPoolingDescriptor_t,
                                          mode: *mut cudnnPoolingMode_t,
                                          windowHeight:
                                              *mut ::libc::c_int,
                                          windowWidth:
                                              *mut ::libc::c_int,
                                          verticalPadding:
                                              *mut ::libc::c_int,
                                          horizontalPadding:
                                              *mut ::libc::c_int,
                                          verticalStride:
                                              *mut ::libc::c_int,
                                          horizontalStride:
                                              *mut ::libc::c_int)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnGetPooling2dDescriptor_v4(poolingDesc:
                                              cudnnPoolingDescriptor_t,
                                          mode: *mut cudnnPoolingMode_t,
                                          maxpoolingNanOpt:
                                              *mut cudnnNanPropagation_t,
                                          windowHeight:
                                              *mut ::libc::c_int,
                                          windowWidth:
                                              *mut ::libc::c_int,
                                          verticalPadding:
                                              *mut ::libc::c_int,
                                          horizontalPadding:
                                              *mut ::libc::c_int,
                                          verticalStride:
                                              *mut ::libc::c_int,
                                          horizontalStride:
                                              *mut ::libc::c_int)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnSetPoolingNdDescriptor_v3(poolingDesc:
                                              cudnnPoolingDescriptor_t,
                                          mode: cudnnPoolingMode_t,
                                          nbDims: ::libc::c_int,
                                          windowDimA:
                                              *const ::libc::c_int,
                                          paddingA:
                                              *const ::libc::c_int,
                                          strideA:
                                              *const ::libc::c_int)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnSetPoolingNdDescriptor_v4(poolingDesc:
                                              cudnnPoolingDescriptor_t,
                                          mode: cudnnPoolingMode_t,
                                          maxpoolingNanOpt:
                                              cudnnNanPropagation_t,
                                          nbDims: ::libc::c_int,
                                          windowDimA:
                                              *const ::libc::c_int,
                                          paddingA:
                                              *const ::libc::c_int,
                                          strideA:
                                              *const ::libc::c_int)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnGetPoolingNdDescriptor_v3(poolingDesc:
                                              cudnnPoolingDescriptor_t,
                                          nbDimsRequested:
                                              ::libc::c_int,
                                          mode: *mut cudnnPoolingMode_t,
                                          nbDims: *mut ::libc::c_int,
                                          windowDimA:
                                              *mut ::libc::c_int,
                                          paddingA:
                                              *mut ::libc::c_int,
                                          strideA: *mut ::libc::c_int)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnGetPoolingNdDescriptor_v4(poolingDesc:
                                              cudnnPoolingDescriptor_t,
                                          nbDimsRequested:
                                              ::libc::c_int,
                                          mode: *mut cudnnPoolingMode_t,
                                          maxpoolingNanOpt:
                                              *mut cudnnNanPropagation_t,
                                          nbDims: *mut ::libc::c_int,
                                          windowDimA:
                                              *mut ::libc::c_int,
                                          paddingA:
                                              *mut ::libc::c_int,
                                          strideA: *mut ::libc::c_int)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnActivationForward_v3(handle: cudnnHandle_t,
                                     mode: cudnnActivationMode_t,
                                     alpha: *const ::libc::c_void,
                                     xDesc: cudnnTensorDescriptor_t,
                                     x: *const ::libc::c_void,
                                     beta: *const ::libc::c_void,
                                     yDesc: cudnnTensorDescriptor_t,
                                     y: *mut ::libc::c_void)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnActivationForward_v4(handle: cudnnHandle_t,
                                     activationDesc:
                                         cudnnActivationDescriptor_t,
                                     alpha: *const ::libc::c_void,
                                     xDesc: cudnnTensorDescriptor_t,
                                     x: *const ::libc::c_void,
                                     beta: *const ::libc::c_void,
                                     yDesc: cudnnTensorDescriptor_t,
                                     y: *mut ::libc::c_void)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnActivationBackward_v3(handle: cudnnHandle_t,
                                      mode: cudnnActivationMode_t,
                                      alpha: *const ::libc::c_void,
                                      yDesc: cudnnTensorDescriptor_t,
                                      y: *const ::libc::c_void,
                                      dyDesc: cudnnTensorDescriptor_t,
                                      dy: *const ::libc::c_void,
                                      xDesc: cudnnTensorDescriptor_t,
                                      x: *const ::libc::c_void,
                                      beta: *const ::libc::c_void,
                                      dxDesc: cudnnTensorDescriptor_t,
                                      dx: *mut ::libc::c_void)
     -> cudnnStatus_t;
}
extern "C" {
    pub fn cudnnActivationBackward_v4(handle: cudnnHandle_t,
                                      activationDesc:
                                          cudnnActivationDescriptor_t,
                                      alpha: *const ::libc::c_void,
                                      yDesc: cudnnTensorDescriptor_t,
                                      y: *const ::libc::c_void,
                                      dyDesc: cudnnTensorDescriptor_t,
                                      dy: *const ::libc::c_void,
                                      xDesc: cudnnTensorDescriptor_t,
                                      x: *const ::libc::c_void,
                                      beta: *const ::libc::c_void,
                                      dxDesc: cudnnTensorDescriptor_t,
                                      dx: *mut ::libc::c_void)
     -> cudnnStatus_t;
}
